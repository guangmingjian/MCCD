Dataset: Mutagenicity,
Model Name: MCCD
MCCD(
  (MGL): MixGCNLayers(
    (gcn_layer): ModuleList(
      (0): GCNConv(14, 64)
      (1): ReLU()
      (2): Dropout(p=0.0, inplace=False)
      (3): GCNConv(64, 64)
      (4): ReLU()
      (5): Dropout(p=0.0, inplace=False)
      (6): GCNConv(64, 64)
      (7): ReLU()
      (8): Dropout(p=0.0, inplace=False)
    )
    (graph_conv_layer): ModuleList(
      (0): SAGEConv(14, 64)
      (1): ReLU()
      (2): Dropout(p=0.0, inplace=False)
      (3): SAGEConv(64, 64)
      (4): ReLU()
      (5): Dropout(p=0.0, inplace=False)
      (6): SAGEConv(64, 64)
      (7): ReLU()
      (8): Dropout(p=0.0, inplace=False)
    )
    (gat_layer): ModuleList(
      (0): GATConv(14, 8, heads=8)
      (1): ReLU()
      (2): Dropout(p=0.0, inplace=False)
      (3): GATConv(64, 64, heads=1)
      (4): ReLU()
      (5): Dropout(p=0.0, inplace=False)
      (6): GATConv(64, 64, heads=1)
      (7): ReLU()
      (8): Dropout(p=0.0, inplace=False)
    )
  )
  (EmTran): EmbeddingTransform(
    (k_fc): Linear(in_features=64, out_features=32, bias=True)
    (v_fc): Linear(in_features=64, out_features=32, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
  )
  (cnn_net): LeNet(
    (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))
    (conv2): Conv2d(16, 18, kernel_size=(5, 5), stride=(1, 1))
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (fc1): Linear(in_features=450, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (gfc): Linear(in_features=64, out_features=2, bias=True)
  (dropout): Dropout(p=0.30000000000000004, inplace=False)
)

fold [0/10] is start!!
[01/0001] | train_loss:0.7605 val_acc:60.739 val_loss:0.6815
model is saved at epoch 1!![01/0002] | train_loss:0.7542 val_acc:60.739 val_loss:0.6709
[01/0003] | train_loss:0.7405 val_acc:64.6651 val_loss:0.6533
model is saved at epoch 3!![01/0004] | train_loss:0.7195 val_acc:67.4365 val_loss:0.6228
model is saved at epoch 4!![01/0005] | train_loss:0.7024 val_acc:67.2055 val_loss:0.6267
[01/0006] | train_loss:0.688 val_acc:62.5866 val_loss:0.6529
[01/0007] | train_loss:0.6725 val_acc:71.5935 val_loss:0.5531
model is saved at epoch 7!![01/0008] | train_loss:0.6514 val_acc:72.9792 val_loss:0.5345
model is saved at epoch 8!![01/0009] | train_loss:0.6411 val_acc:74.3649 val_loss:0.5361
model is saved at epoch 9!![01/0010] | train_loss:0.633 val_acc:74.5958 val_loss:0.5454
model is saved at epoch 10!![01/0011] | train_loss:0.624 val_acc:74.8268 val_loss:0.5188
model is saved at epoch 11!![01/0012] | train_loss:0.6255 val_acc:75.0577 val_loss:0.5248
model is saved at epoch 12!![01/0013] | train_loss:0.6172 val_acc:75.2887 val_loss:0.527
model is saved at epoch 13!![01/0014] | train_loss:0.6141 val_acc:75.2887 val_loss:0.5285
[01/0015] | train_loss:0.6059 val_acc:74.8268 val_loss:0.5185
[01/0016] | train_loss:0.6005 val_acc:76.9053 val_loss:0.4957
model is saved at epoch 16!![01/0017] | train_loss:0.6012 val_acc:76.4434 val_loss:0.4861
[01/0018] | train_loss:0.6085 val_acc:76.2125 val_loss:0.4944
[01/0019] | train_loss:0.6341 val_acc:71.5935 val_loss:0.5774
[01/0020] | train_loss:0.6118 val_acc:76.4434 val_loss:0.4936
[01/0021] | train_loss:0.5915 val_acc:76.9053 val_loss:0.4882
[01/0022] | train_loss:0.5887 val_acc:77.3672 val_loss:0.4954
model is saved at epoch 22!![01/0023] | train_loss:0.5906 val_acc:77.1363 val_loss:0.4962
[01/0024] | train_loss:0.5827 val_acc:76.4434 val_loss:0.499
[01/0025] | train_loss:0.5858 val_acc:78.06 val_loss:0.481
model is saved at epoch 25!![01/0026] | train_loss:0.579 val_acc:78.06 val_loss:0.4822
[01/0027] | train_loss:0.5789 val_acc:78.291 val_loss:0.468
model is saved at epoch 27!![01/0028] | train_loss:0.5821 val_acc:78.5219 val_loss:0.4719
model is saved at epoch 28!![01/0029] | train_loss:0.5759 val_acc:77.8291 val_loss:0.4643
[01/0030] | train_loss:0.5675 val_acc:79.4457 val_loss:0.4632
model is saved at epoch 30!![01/0031] | train_loss:0.5591 val_acc:79.4457 val_loss:0.4635
[01/0032] | train_loss:0.5647 val_acc:78.5219 val_loss:0.4839
[01/0033] | train_loss:0.5789 val_acc:79.9076 val_loss:0.4717
model is saved at epoch 33!![01/0034] | train_loss:0.5732 val_acc:75.5196 val_loss:0.5096
[01/0035] | train_loss:0.5658 val_acc:80.1386 val_loss:0.47
model is saved at epoch 35!![01/0036] | train_loss:0.5521 val_acc:80.3695 val_loss:0.4471
model is saved at epoch 36!![01/0037] | train_loss:0.5474 val_acc:81.2933 val_loss:0.4449
model is saved at epoch 37!![01/0038] | train_loss:0.5395 val_acc:81.0624 val_loss:0.4429
[01/0039] | train_loss:0.543 val_acc:81.0624 val_loss:0.4544
[01/0040] | train_loss:0.5309 val_acc:79.6767 val_loss:0.446
[01/0041] | train_loss:0.533 val_acc:81.0624 val_loss:0.4401
[01/0042] | train_loss:0.5276 val_acc:81.0624 val_loss:0.449
[01/0043] | train_loss:0.5213 val_acc:81.2933 val_loss:0.4278
[01/0044] | train_loss:0.5237 val_acc:82.2171 val_loss:0.4275
model is saved at epoch 44!![01/0045] | train_loss:0.5234 val_acc:83.1409 val_loss:0.4232
model is saved at epoch 45!![01/0046] | train_loss:0.5305 val_acc:79.9076 val_loss:0.4455
[01/0047] | train_loss:0.544 val_acc:79.4457 val_loss:0.4564
[01/0048] | train_loss:0.5293 val_acc:79.2148 val_loss:0.4626
[01/0049] | train_loss:0.5289 val_acc:79.2148 val_loss:0.4607
[01/0050] | train_loss:0.5164 val_acc:80.3695 val_loss:0.4503
[01/0051] | train_loss:0.5099 val_acc:81.2933 val_loss:0.4279
[01/0052] | train_loss:0.5049 val_acc:83.3718 val_loss:0.4301
model is saved at epoch 52!![01/0053] | train_loss:0.5042 val_acc:81.7552 val_loss:0.432
[01/0054] | train_loss:0.5015 val_acc:83.1409 val_loss:0.4155
[01/0055] | train_loss:0.4956 val_acc:81.9861 val_loss:0.4226
[01/0056] | train_loss:0.4964 val_acc:82.679 val_loss:0.4172
[01/0057] | train_loss:0.4883 val_acc:83.1409 val_loss:0.4116
[01/0058] | train_loss:0.4843 val_acc:83.1409 val_loss:0.4184
[01/0059] | train_loss:0.4838 val_acc:83.6028 val_loss:0.409
model is saved at epoch 59!![01/0060] | train_loss:0.4938 val_acc:84.0647 val_loss:0.4109
model is saved at epoch 60!![01/0061] | train_loss:0.4751 val_acc:82.9099 val_loss:0.4327
[01/0062] | train_loss:0.478 val_acc:84.0647 val_loss:0.4158
[01/0063] | train_loss:0.4608 val_acc:83.6028 val_loss:0.4059
[01/0064] | train_loss:0.4583 val_acc:84.7575 val_loss:0.4024
model is saved at epoch 64!![01/0065] | train_loss:0.4559 val_acc:84.0647 val_loss:0.407
[01/0066] | train_loss:0.4609 val_acc:84.0647 val_loss:0.3996
[01/0067] | train_loss:0.4476 val_acc:83.6028 val_loss:0.4085
[01/0068] | train_loss:0.4433 val_acc:84.2956 val_loss:0.4117
[01/0069] | train_loss:0.4427 val_acc:84.0647 val_loss:0.3984
[01/0070] | train_loss:0.4392 val_acc:84.0647 val_loss:0.3981
[01/0071] | train_loss:0.4336 val_acc:84.2956 val_loss:0.3934
[01/0072] | train_loss:0.4308 val_acc:81.9861 val_loss:0.4404
[01/0073] | train_loss:0.4313 val_acc:83.8337 val_loss:0.4154
[01/0074] | train_loss:0.4479 val_acc:83.3718 val_loss:0.4009
[01/0075] | train_loss:0.4287 val_acc:82.2171 val_loss:0.4105
[01/0076] | train_loss:0.4393 val_acc:83.6028 val_loss:0.4017
[01/0077] | train_loss:0.4227 val_acc:84.9885 val_loss:0.3935
model is saved at epoch 77!![01/0078] | train_loss:0.4201 val_acc:83.3718 val_loss:0.4014
[01/0079] | train_loss:0.4114 val_acc:83.3718 val_loss:0.409
[01/0080] | train_loss:0.4066 val_acc:83.3718 val_loss:0.3986
[01/0081] | train_loss:0.4218 val_acc:80.3695 val_loss:0.4608
[01/0082] | train_loss:0.455 val_acc:81.9861 val_loss:0.4136
[01/0083] | train_loss:0.4176 val_acc:82.679 val_loss:0.4087
[01/0084] | train_loss:0.397 val_acc:83.3718 val_loss:0.401
[01/0085] | train_loss:0.3971 val_acc:84.2956 val_loss:0.3908
[01/0086] | train_loss:0.392 val_acc:81.7552 val_loss:0.4312
[01/0087] | train_loss:0.3887 val_acc:84.0647 val_loss:0.4093
[01/0088] | train_loss:0.3973 val_acc:83.1409 val_loss:0.4065
[01/0089] | train_loss:0.4231 val_acc:83.1409 val_loss:0.4098
[01/0090] | train_loss:0.401 val_acc:84.9885 val_loss:0.4072
[01/0091] | train_loss:0.3851 val_acc:81.7552 val_loss:0.4169
[01/0092] | train_loss:0.3922 val_acc:84.9885 val_loss:0.3908
[01/0093] | train_loss:0.388 val_acc:80.8314 val_loss:0.4502
[01/0094] | train_loss:0.3792 val_acc:84.0647 val_loss:0.4025
[01/0095] | train_loss:0.3607 val_acc:84.0647 val_loss:0.4077
[01/0096] | train_loss:0.3657 val_acc:83.1409 val_loss:0.4043
[01/0097] | train_loss:0.3816 val_acc:83.8337 val_loss:0.4059
[01/0098] | train_loss:0.3746 val_acc:83.6028 val_loss:0.3965
[01/0099] | train_loss:0.3745 val_acc:82.2171 val_loss:0.42
[01/0100] | train_loss:0.351 val_acc:84.7575 val_loss:0.4075
[01/0101] | train_loss:0.3664 val_acc:83.3718 val_loss:0.4134
[01/0102] | train_loss:0.3621 val_acc:80.1386 val_loss:0.443
[01/0103] | train_loss:0.3756 val_acc:82.679 val_loss:0.4058
[01/0104] | train_loss:0.3656 val_acc:82.9099 val_loss:0.4188
[01/0105] | train_loss:0.3725 val_acc:83.3718 val_loss:0.4342
[01/0106] | train_loss:0.3481 val_acc:83.8337 val_loss:0.4185
[01/0107] | train_loss:0.355 val_acc:82.448 val_loss:0.4272
[01/0108] | train_loss:0.3415 val_acc:82.2171 val_loss:0.4127
[01/0109] | train_loss:0.3253 val_acc:83.3718 val_loss:0.425
[01/0110] | train_loss:0.3325 val_acc:79.2148 val_loss:0.4552
[01/0111] | train_loss:0.3389 val_acc:85.4503 val_loss:0.3998
model is saved at epoch 111!![01/0112] | train_loss:0.3184 val_acc:81.9861 val_loss:0.436
[01/0113] | train_loss:0.3237 val_acc:84.5266 val_loss:0.4027
[01/0114] | train_loss:0.3406 val_acc:81.9861 val_loss:0.4396
[01/0115] | train_loss:0.3342 val_acc:83.8337 val_loss:0.4249
[01/0116] | train_loss:0.3267 val_acc:80.1386 val_loss:0.4392
[01/0117] | train_loss:0.3163 val_acc:83.8337 val_loss:0.4296
[01/0118] | train_loss:0.313 val_acc:83.6028 val_loss:0.4268
[01/0119] | train_loss:0.3318 val_acc:84.0647 val_loss:0.4263
[01/0120] | train_loss:0.3189 val_acc:81.5242 val_loss:0.4468
[01/0121] | train_loss:0.3381 val_acc:82.679 val_loss:0.4093
[01/0122] | train_loss:0.3477 val_acc:81.5242 val_loss:0.4568
[01/0123] | train_loss:0.3359 val_acc:81.9861 val_loss:0.4234
[01/0124] | train_loss:0.3226 val_acc:81.2933 val_loss:0.4394
[01/0125] | train_loss:0.312 val_acc:84.2956 val_loss:0.4368
[01/0126] | train_loss:0.289 val_acc:82.9099 val_loss:0.4389
[01/0127] | train_loss:0.2832 val_acc:83.3718 val_loss:0.4258
[01/0128] | train_loss:0.3076 val_acc:83.6028 val_loss:0.4241
[01/0129] | train_loss:0.2909 val_acc:83.6028 val_loss:0.4253
[01/0130] | train_loss:0.3077 val_acc:83.1409 val_loss:0.4241
[01/0131] | train_loss:0.2963 val_acc:80.8314 val_loss:0.4706
[01/0132] | train_loss:0.2824 val_acc:84.0647 val_loss:0.427
[01/0133] | train_loss:0.2739 val_acc:82.9099 val_loss:0.4279
[01/0134] | train_loss:0.2796 val_acc:83.8337 val_loss:0.4204
[01/0135] | train_loss:0.2659 val_acc:79.6767 val_loss:0.4875
[01/0136] | train_loss:0.2851 val_acc:84.2956 val_loss:0.4114
[01/0137] | train_loss:0.2764 val_acc:78.9838 val_loss:0.518
[01/0138] | train_loss:0.2903 val_acc:83.1409 val_loss:0.4615
[01/0139] | train_loss:0.2874 val_acc:84.5266 val_loss:0.434
[01/0140] | train_loss:0.2705 val_acc:83.3718 val_loss:0.4412
[01/0141] | train_loss:0.26 val_acc:83.8337 val_loss:0.4473
[01/0142] | train_loss:0.2518 val_acc:82.9099 val_loss:0.4706
[01/0143] | train_loss:0.2558 val_acc:81.2933 val_loss:0.4945
[01/0144] | train_loss:0.2786 val_acc:84.2956 val_loss:0.4778
[01/0145] | train_loss:0.2856 val_acc:84.5266 val_loss:0.4572
[01/0146] | train_loss:0.2877 val_acc:85.2194 val_loss:0.4288
[01/0147] | train_loss:0.2564 val_acc:81.9861 val_loss:0.4715
[01/0148] | train_loss:0.2489 val_acc:84.5266 val_loss:0.4562
[01/0149] | train_loss:0.2429 val_acc:83.3718 val_loss:0.4438
[01/0150] | train_loss:0.2357 val_acc:79.2148 val_loss:0.5228
[01/0151] | train_loss:0.2516 val_acc:84.5266 val_loss:0.4387
[01/0152] | train_loss:0.2613 val_acc:81.5242 val_loss:0.4863
[01/0153] | train_loss:0.2705 val_acc:83.3718 val_loss:0.4344
[01/0154] | train_loss:0.2425 val_acc:83.8337 val_loss:0.4699
[01/0155] | train_loss:0.2371 val_acc:82.448 val_loss:0.5119
[01/0156] | train_loss:0.2427 val_acc:83.8337 val_loss:0.4541
[01/0157] | train_loss:0.2332 val_acc:83.1409 val_loss:0.4511
[01/0158] | train_loss:0.2362 val_acc:84.2956 val_loss:0.5058
[01/0159] | train_loss:0.2404 val_acc:83.6028 val_loss:0.4606
[01/0160] | train_loss:0.2313 val_acc:83.3718 val_loss:0.5016
[01/0161] | train_loss:0.2457 val_acc:82.9099 val_loss:0.4551
[01/0162] | train_loss:0.2381 val_acc:83.8337 val_loss:0.4942
Fold: [1/10] Test is finish !! 
 Test Metrics are: test_acc:84.1014 test_loss:0.3843fold [1/10] is start!!
[02/0001] | train_loss:0.7564 val_acc:55.9908 val_loss:0.6803
model is saved at epoch 1!![02/0002] | train_loss:0.7474 val_acc:55.9908 val_loss:0.6619
[02/0003] | train_loss:0.7237 val_acc:65.2074 val_loss:0.6291
model is saved at epoch 3!![02/0004] | train_loss:0.6916 val_acc:67.0507 val_loss:0.6014
model is saved at epoch 4!![02/0005] | train_loss:0.6713 val_acc:69.8157 val_loss:0.5726
model is saved at epoch 5!![02/0006] | train_loss:0.6676 val_acc:67.9724 val_loss:0.5819
[02/0007] | train_loss:0.6494 val_acc:69.1244 val_loss:0.5704
[02/0008] | train_loss:0.6321 val_acc:72.5806 val_loss:0.5617
model is saved at epoch 8!![02/0009] | train_loss:0.6302 val_acc:72.5806 val_loss:0.5568
[02/0010] | train_loss:0.6288 val_acc:73.7327 val_loss:0.5547
model is saved at epoch 10!![02/0011] | train_loss:0.6182 val_acc:70.7373 val_loss:0.5483
[02/0012] | train_loss:0.6086 val_acc:74.6544 val_loss:0.534
model is saved at epoch 12!![02/0013] | train_loss:0.6009 val_acc:72.8111 val_loss:0.5344
[02/0014] | train_loss:0.6044 val_acc:73.9631 val_loss:0.5208
[02/0015] | train_loss:0.5923 val_acc:74.6544 val_loss:0.5174
[02/0016] | train_loss:0.5864 val_acc:76.2673 val_loss:0.5192
model is saved at epoch 16!![02/0017] | train_loss:0.5795 val_acc:76.0369 val_loss:0.5043
[02/0018] | train_loss:0.5851 val_acc:76.2673 val_loss:0.5162
[02/0019] | train_loss:0.5914 val_acc:74.424 val_loss:0.5305
[02/0020] | train_loss:0.5867 val_acc:77.8802 val_loss:0.5053
model is saved at epoch 20!![02/0021] | train_loss:0.5761 val_acc:77.1889 val_loss:0.5052
[02/0022] | train_loss:0.5755 val_acc:76.0369 val_loss:0.51
[02/0023] | train_loss:0.5665 val_acc:78.341 val_loss:0.4908
model is saved at epoch 23!![02/0024] | train_loss:0.5563 val_acc:78.8018 val_loss:0.4784
model is saved at epoch 24!![02/0025] | train_loss:0.5622 val_acc:77.4194 val_loss:0.4938
[02/0026] | train_loss:0.5637 val_acc:75.576 val_loss:0.5034
[02/0027] | train_loss:0.5578 val_acc:78.8018 val_loss:0.4697
[02/0028] | train_loss:0.5406 val_acc:79.2627 val_loss:0.4686
model is saved at epoch 28!![02/0029] | train_loss:0.5423 val_acc:79.4931 val_loss:0.4723
model is saved at epoch 29!![02/0030] | train_loss:0.5423 val_acc:79.4931 val_loss:0.4672
[02/0031] | train_loss:0.5529 val_acc:78.1106 val_loss:0.4877
[02/0032] | train_loss:0.5498 val_acc:79.4931 val_loss:0.4686
[02/0033] | train_loss:0.5347 val_acc:79.4931 val_loss:0.4542
[02/0034] | train_loss:0.5175 val_acc:80.4147 val_loss:0.4569
model is saved at epoch 34!![02/0035] | train_loss:0.5206 val_acc:79.9539 val_loss:0.4574
[02/0036] | train_loss:0.5176 val_acc:81.3364 val_loss:0.4501
model is saved at epoch 36!![02/0037] | train_loss:0.5184 val_acc:79.7235 val_loss:0.4489
[02/0038] | train_loss:0.5142 val_acc:80.8756 val_loss:0.4486
[02/0039] | train_loss:0.5149 val_acc:80.8756 val_loss:0.4525
[02/0040] | train_loss:0.5144 val_acc:80.1843 val_loss:0.4546
[02/0041] | train_loss:0.5157 val_acc:81.106 val_loss:0.4553
[02/0042] | train_loss:0.5094 val_acc:81.106 val_loss:0.4441
[02/0043] | train_loss:0.5033 val_acc:81.5668 val_loss:0.4354
model is saved at epoch 43!![02/0044] | train_loss:0.501 val_acc:80.6452 val_loss:0.4494
[02/0045] | train_loss:0.501 val_acc:80.1843 val_loss:0.4447
[02/0046] | train_loss:0.5155 val_acc:80.4147 val_loss:0.4525
[02/0047] | train_loss:0.5158 val_acc:79.9539 val_loss:0.4585
[02/0048] | train_loss:0.5174 val_acc:78.8018 val_loss:0.4804
[02/0049] | train_loss:0.5172 val_acc:78.341 val_loss:0.4674
[02/0050] | train_loss:0.5094 val_acc:80.8756 val_loss:0.4445
[02/0051] | train_loss:0.4982 val_acc:81.5668 val_loss:0.4305
[02/0052] | train_loss:0.496 val_acc:82.9493 val_loss:0.4232
model is saved at epoch 52!![02/0053] | train_loss:0.4845 val_acc:82.0276 val_loss:0.4256
[02/0054] | train_loss:0.4807 val_acc:79.4931 val_loss:0.4461
[02/0055] | train_loss:0.4794 val_acc:79.2627 val_loss:0.4384
[02/0056] | train_loss:0.4811 val_acc:80.6452 val_loss:0.4281
[02/0057] | train_loss:0.4779 val_acc:82.0276 val_loss:0.4194
[02/0058] | train_loss:0.4737 val_acc:81.5668 val_loss:0.4236
[02/0059] | train_loss:0.4738 val_acc:81.106 val_loss:0.4304
[02/0060] | train_loss:0.4688 val_acc:82.4885 val_loss:0.4132
[02/0061] | train_loss:0.47 val_acc:79.9539 val_loss:0.459
[02/0062] | train_loss:0.4913 val_acc:76.4977 val_loss:0.4781
[02/0063] | train_loss:0.4889 val_acc:78.1106 val_loss:0.4715
[02/0064] | train_loss:0.4912 val_acc:79.9539 val_loss:0.4473
[02/0065] | train_loss:0.486 val_acc:82.4885 val_loss:0.421
[02/0066] | train_loss:0.4651 val_acc:81.3364 val_loss:0.4218
[02/0067] | train_loss:0.4582 val_acc:81.106 val_loss:0.4233
[02/0068] | train_loss:0.4543 val_acc:81.3364 val_loss:0.4234
[02/0069] | train_loss:0.4695 val_acc:82.4885 val_loss:0.4083
[02/0070] | train_loss:0.4725 val_acc:80.8756 val_loss:0.4476
[02/0071] | train_loss:0.4734 val_acc:83.6406 val_loss:0.408
model is saved at epoch 71!![02/0072] | train_loss:0.472 val_acc:81.5668 val_loss:0.4239
[02/0073] | train_loss:0.4593 val_acc:81.7972 val_loss:0.4088
[02/0074] | train_loss:0.443 val_acc:81.5668 val_loss:0.4056
[02/0075] | train_loss:0.4429 val_acc:80.6452 val_loss:0.4338
[02/0076] | train_loss:0.4445 val_acc:81.5668 val_loss:0.4157
[02/0077] | train_loss:0.4287 val_acc:82.9493 val_loss:0.4066
[02/0078] | train_loss:0.4356 val_acc:82.7189 val_loss:0.4127
[02/0079] | train_loss:0.453 val_acc:81.7972 val_loss:0.4155
[02/0080] | train_loss:0.4285 val_acc:81.7972 val_loss:0.3975
[02/0081] | train_loss:0.4347 val_acc:82.2581 val_loss:0.4116
[02/0082] | train_loss:0.4376 val_acc:83.6406 val_loss:0.4118
[02/0083] | train_loss:0.4311 val_acc:81.3364 val_loss:0.4114
[02/0084] | train_loss:0.4282 val_acc:83.6406 val_loss:0.4133
[02/0085] | train_loss:0.4214 val_acc:82.9493 val_loss:0.4082
[02/0086] | train_loss:0.4145 val_acc:82.2581 val_loss:0.4249
[02/0087] | train_loss:0.4232 val_acc:84.1014 val_loss:0.4186
model is saved at epoch 87!![02/0088] | train_loss:0.4195 val_acc:81.7972 val_loss:0.4026
[02/0089] | train_loss:0.4134 val_acc:81.3364 val_loss:0.4289
[02/0090] | train_loss:0.4185 val_acc:82.7189 val_loss:0.4057
[02/0091] | train_loss:0.4236 val_acc:80.8756 val_loss:0.4336
[02/0092] | train_loss:0.4171 val_acc:83.871 val_loss:0.4074
[02/0093] | train_loss:0.4034 val_acc:83.6406 val_loss:0.3889
[02/0094] | train_loss:0.4057 val_acc:82.9493 val_loss:0.3832
[02/0095] | train_loss:0.4095 val_acc:82.0276 val_loss:0.404
[02/0096] | train_loss:0.3968 val_acc:83.4101 val_loss:0.3931
[02/0097] | train_loss:0.3915 val_acc:83.1797 val_loss:0.3976
[02/0098] | train_loss:0.3783 val_acc:83.4101 val_loss:0.4148
[02/0099] | train_loss:0.3896 val_acc:82.9493 val_loss:0.4088
[02/0100] | train_loss:0.3861 val_acc:83.871 val_loss:0.3948
[02/0101] | train_loss:0.3923 val_acc:81.106 val_loss:0.4169
[02/0102] | train_loss:0.3948 val_acc:80.4147 val_loss:0.4261
[02/0103] | train_loss:0.394 val_acc:83.4101 val_loss:0.3913
[02/0104] | train_loss:0.3866 val_acc:83.6406 val_loss:0.4014
[02/0105] | train_loss:0.3866 val_acc:83.4101 val_loss:0.4089
[02/0106] | train_loss:0.3731 val_acc:82.0276 val_loss:0.4218
[02/0107] | train_loss:0.3746 val_acc:85.023 val_loss:0.4081
model is saved at epoch 107!![02/0108] | train_loss:0.37 val_acc:81.106 val_loss:0.4235
[02/0109] | train_loss:0.3642 val_acc:82.2581 val_loss:0.4226
[02/0110] | train_loss:0.3824 val_acc:82.9493 val_loss:0.3995
[02/0111] | train_loss:0.4037 val_acc:81.5668 val_loss:0.4241
[02/0112] | train_loss:0.3878 val_acc:83.6406 val_loss:0.4288
[02/0113] | train_loss:0.387 val_acc:84.3318 val_loss:0.3979
[02/0114] | train_loss:0.3722 val_acc:84.1014 val_loss:0.397
[02/0115] | train_loss:0.3478 val_acc:84.5622 val_loss:0.4141
[02/0116] | train_loss:0.3406 val_acc:84.3318 val_loss:0.4269
[02/0117] | train_loss:0.3491 val_acc:83.871 val_loss:0.4041
[02/0118] | train_loss:0.3386 val_acc:83.6406 val_loss:0.4115
[02/0119] | train_loss:0.3303 val_acc:83.6406 val_loss:0.4121
[02/0120] | train_loss:0.333 val_acc:82.4885 val_loss:0.4197
[02/0121] | train_loss:0.3313 val_acc:83.6406 val_loss:0.4212
[02/0122] | train_loss:0.3314 val_acc:82.2581 val_loss:0.4295
[02/0123] | train_loss:0.3329 val_acc:82.2581 val_loss:0.4272
[02/0124] | train_loss:0.332 val_acc:83.6406 val_loss:0.4041
[02/0125] | train_loss:0.3332 val_acc:83.4101 val_loss:0.4308
[02/0126] | train_loss:0.3232 val_acc:82.4885 val_loss:0.444
[02/0127] | train_loss:0.3105 val_acc:83.4101 val_loss:0.4265
[02/0128] | train_loss:0.3069 val_acc:81.7972 val_loss:0.4555
[02/0129] | train_loss:0.3234 val_acc:83.1797 val_loss:0.4343
[02/0130] | train_loss:0.2996 val_acc:83.871 val_loss:0.4155
[02/0131] | train_loss:0.3149 val_acc:83.6406 val_loss:0.4301
[02/0132] | train_loss:0.3421 val_acc:84.1014 val_loss:0.4153
[02/0133] | train_loss:0.3172 val_acc:83.871 val_loss:0.4523
[02/0134] | train_loss:0.3136 val_acc:80.4147 val_loss:0.5506
[02/0135] | train_loss:0.3611 val_acc:83.6406 val_loss:0.4171
[02/0136] | train_loss:0.3322 val_acc:84.1014 val_loss:0.4055
[02/0137] | train_loss:0.3168 val_acc:83.1797 val_loss:0.4549
[02/0138] | train_loss:0.2985 val_acc:83.871 val_loss:0.4249
[02/0139] | train_loss:0.2934 val_acc:85.4839 val_loss:0.4451
model is saved at epoch 139!![02/0140] | train_loss:0.2756 val_acc:83.1797 val_loss:0.45
[02/0141] | train_loss:0.2942 val_acc:82.4885 val_loss:0.4691
[02/0142] | train_loss:0.3119 val_acc:82.0276 val_loss:0.4578
[02/0143] | train_loss:0.2922 val_acc:82.9493 val_loss:0.4391
[02/0144] | train_loss:0.2767 val_acc:82.7189 val_loss:0.4824
[02/0145] | train_loss:0.2887 val_acc:83.1797 val_loss:0.4691
[02/0146] | train_loss:0.3202 val_acc:81.7972 val_loss:0.4793
[02/0147] | train_loss:0.3136 val_acc:82.2581 val_loss:0.4351
[02/0148] | train_loss:0.2914 val_acc:81.5668 val_loss:0.4759
[02/0149] | train_loss:0.2838 val_acc:84.1014 val_loss:0.46
[02/0150] | train_loss:0.2646 val_acc:80.4147 val_loss:0.5149
[02/0151] | train_loss:0.2637 val_acc:82.9493 val_loss:0.4786
[02/0152] | train_loss:0.261 val_acc:82.7189 val_loss:0.4629
[02/0153] | train_loss:0.2507 val_acc:82.9493 val_loss:0.4858
[02/0154] | train_loss:0.249 val_acc:82.9493 val_loss:0.4683
[02/0155] | train_loss:0.2493 val_acc:82.4885 val_loss:0.5302
[02/0156] | train_loss:0.2516 val_acc:81.106 val_loss:0.5431
[02/0157] | train_loss:0.2509 val_acc:82.7189 val_loss:0.5047
[02/0158] | train_loss:0.2609 val_acc:82.4885 val_loss:0.4823
[02/0159] | train_loss:0.2561 val_acc:83.1797 val_loss:0.4618
[02/0160] | train_loss:0.2411 val_acc:81.7972 val_loss:0.501
[02/0161] | train_loss:0.2587 val_acc:82.9493 val_loss:0.4756
[02/0162] | train_loss:0.2648 val_acc:82.7189 val_loss:0.4789
[02/0163] | train_loss:0.2578 val_acc:83.1797 val_loss:0.4838
[02/0164] | train_loss:0.2461 val_acc:82.7189 val_loss:0.4678
[02/0165] | train_loss:0.2519 val_acc:83.4101 val_loss:0.5315
[02/0166] | train_loss:0.246 val_acc:82.9493 val_loss:0.4964
[02/0167] | train_loss:0.239 val_acc:81.5668 val_loss:0.5134
[02/0168] | train_loss:0.2315 val_acc:80.6452 val_loss:0.5265
[02/0169] | train_loss:0.2586 val_acc:83.1797 val_loss:0.5188
[02/0170] | train_loss:0.2759 val_acc:81.7972 val_loss:0.463
[02/0171] | train_loss:0.2443 val_acc:82.4885 val_loss:0.4925
[02/0172] | train_loss:0.229 val_acc:83.1797 val_loss:0.5167
[02/0173] | train_loss:0.2535 val_acc:82.2581 val_loss:0.5229
[02/0174] | train_loss:0.2408 val_acc:82.0276 val_loss:0.4683
[02/0175] | train_loss:0.2227 val_acc:79.9539 val_loss:0.5439
[02/0176] | train_loss:0.2427 val_acc:81.106 val_loss:0.5451
[02/0177] | train_loss:0.2317 val_acc:83.1797 val_loss:0.5386
[02/0178] | train_loss:0.2426 val_acc:83.4101 val_loss:0.5131
[02/0179] | train_loss:0.2436 val_acc:82.2581 val_loss:0.5092
[02/0180] | train_loss:0.243 val_acc:81.106 val_loss:0.6273
[02/0181] | train_loss:0.2756 val_acc:81.7972 val_loss:0.5377
[02/0182] | train_loss:0.2665 val_acc:82.2581 val_loss:0.4781
[02/0183] | train_loss:0.245 val_acc:80.1843 val_loss:0.5623
[02/0184] | train_loss:0.2396 val_acc:82.0276 val_loss:0.5366
[02/0185] | train_loss:0.2239 val_acc:82.2581 val_loss:0.5387
[02/0186] | train_loss:0.2201 val_acc:83.1797 val_loss:0.561
[02/0187] | train_loss:0.201 val_acc:82.0276 val_loss:0.5855
[02/0188] | train_loss:0.2 val_acc:83.1797 val_loss:0.5797
[02/0189] | train_loss:0.2078 val_acc:83.6406 val_loss:0.5312
[02/0190] | train_loss:0.1968 val_acc:83.4101 val_loss:0.5717
Fold: [2/10] Test is finish !! 
 Test Metrics are: test_acc:79.7235 test_loss:0.5508fold [2/10] is start!!
[03/0001] | train_loss:0.7579 val_acc:55.7604 val_loss:0.685
model is saved at epoch 1!![03/0002] | train_loss:0.7539 val_acc:55.7604 val_loss:0.6744
[03/0003] | train_loss:0.7388 val_acc:63.3641 val_loss:0.641
model is saved at epoch 3!![03/0004] | train_loss:0.7097 val_acc:65.6682 val_loss:0.6259
model is saved at epoch 4!![03/0005] | train_loss:0.6946 val_acc:67.5115 val_loss:0.6095
model is saved at epoch 5!![03/0006] | train_loss:0.6762 val_acc:68.894 val_loss:0.5907
model is saved at epoch 6!![03/0007] | train_loss:0.6662 val_acc:67.2811 val_loss:0.622
[03/0008] | train_loss:0.6613 val_acc:67.7419 val_loss:0.5746
[03/0009] | train_loss:0.6499 val_acc:69.1244 val_loss:0.5731
model is saved at epoch 9!![03/0010] | train_loss:0.6547 val_acc:68.4332 val_loss:0.5659
[03/0011] | train_loss:0.6518 val_acc:69.3548 val_loss:0.5667
model is saved at epoch 11!![03/0012] | train_loss:0.6262 val_acc:68.894 val_loss:0.6133
[03/0013] | train_loss:0.6322 val_acc:71.4286 val_loss:0.569
model is saved at epoch 13!![03/0014] | train_loss:0.624 val_acc:70.2765 val_loss:0.5644
[03/0015] | train_loss:0.6203 val_acc:70.2765 val_loss:0.5687
[03/0016] | train_loss:0.6149 val_acc:70.9677 val_loss:0.5558
[03/0017] | train_loss:0.6058 val_acc:72.8111 val_loss:0.5455
model is saved at epoch 17!![03/0018] | train_loss:0.6025 val_acc:73.2719 val_loss:0.5585
model is saved at epoch 18!![03/0019] | train_loss:0.6015 val_acc:73.9631 val_loss:0.5388
model is saved at epoch 19!![03/0020] | train_loss:0.5943 val_acc:74.1935 val_loss:0.5447
model is saved at epoch 20!![03/0021] | train_loss:0.5863 val_acc:73.2719 val_loss:0.5359
[03/0022] | train_loss:0.5863 val_acc:72.8111 val_loss:0.5373
[03/0023] | train_loss:0.5742 val_acc:72.8111 val_loss:0.5323
[03/0024] | train_loss:0.5773 val_acc:71.8894 val_loss:0.5403
[03/0025] | train_loss:0.5693 val_acc:74.6544 val_loss:0.5157
model is saved at epoch 25!![03/0026] | train_loss:0.5592 val_acc:73.2719 val_loss:0.5574
[03/0027] | train_loss:0.5655 val_acc:75.1152 val_loss:0.5119
model is saved at epoch 27!![03/0028] | train_loss:0.5578 val_acc:73.9631 val_loss:0.5124
[03/0029] | train_loss:0.5466 val_acc:75.3456 val_loss:0.51
model is saved at epoch 29!![03/0030] | train_loss:0.5359 val_acc:74.6544 val_loss:0.5226
[03/0031] | train_loss:0.5414 val_acc:75.576 val_loss:0.5019
model is saved at epoch 31!![03/0032] | train_loss:0.5346 val_acc:75.8065 val_loss:0.5035
model is saved at epoch 32!![03/0033] | train_loss:0.5316 val_acc:75.3456 val_loss:0.5063
[03/0034] | train_loss:0.5233 val_acc:76.7281 val_loss:0.4903
model is saved at epoch 34!![03/0035] | train_loss:0.5223 val_acc:74.1935 val_loss:0.513
[03/0036] | train_loss:0.5302 val_acc:75.1152 val_loss:0.4924
[03/0037] | train_loss:0.5153 val_acc:76.9585 val_loss:0.497
model is saved at epoch 37!![03/0038] | train_loss:0.5147 val_acc:75.576 val_loss:0.4943
[03/0039] | train_loss:0.5084 val_acc:78.1106 val_loss:0.4835
model is saved at epoch 39!![03/0040] | train_loss:0.5115 val_acc:76.4977 val_loss:0.4811
[03/0041] | train_loss:0.506 val_acc:74.6544 val_loss:0.4932
[03/0042] | train_loss:0.5058 val_acc:76.2673 val_loss:0.4904
[03/0043] | train_loss:0.5064 val_acc:75.1152 val_loss:0.5035
[03/0044] | train_loss:0.5087 val_acc:77.8802 val_loss:0.489
[03/0045] | train_loss:0.4969 val_acc:76.4977 val_loss:0.4935
[03/0046] | train_loss:0.4926 val_acc:76.4977 val_loss:0.4797
[03/0047] | train_loss:0.4883 val_acc:76.0369 val_loss:0.4853
[03/0048] | train_loss:0.4838 val_acc:77.1889 val_loss:0.4868
[03/0049] | train_loss:0.4924 val_acc:74.8848 val_loss:0.4959
[03/0050] | train_loss:0.48 val_acc:74.6544 val_loss:0.5166
[03/0051] | train_loss:0.4816 val_acc:76.4977 val_loss:0.4836
[03/0052] | train_loss:0.4956 val_acc:78.8018 val_loss:0.4642
model is saved at epoch 52!![03/0053] | train_loss:0.5053 val_acc:75.1152 val_loss:0.4835
[03/0054] | train_loss:0.4848 val_acc:75.3456 val_loss:0.4919
[03/0055] | train_loss:0.4732 val_acc:79.4931 val_loss:0.4627
model is saved at epoch 55!![03/0056] | train_loss:0.4701 val_acc:77.4194 val_loss:0.4876
[03/0057] | train_loss:0.4655 val_acc:78.1106 val_loss:0.4709
[03/0058] | train_loss:0.4716 val_acc:71.8894 val_loss:0.5396
[03/0059] | train_loss:0.4842 val_acc:74.8848 val_loss:0.5005
[03/0060] | train_loss:0.4855 val_acc:78.8018 val_loss:0.4657
[03/0061] | train_loss:0.4728 val_acc:76.4977 val_loss:0.4844
[03/0062] | train_loss:0.4638 val_acc:77.1889 val_loss:0.4895
[03/0063] | train_loss:0.4575 val_acc:76.2673 val_loss:0.481
[03/0064] | train_loss:0.4466 val_acc:75.3456 val_loss:0.51
[03/0065] | train_loss:0.4531 val_acc:76.0369 val_loss:0.4936
[03/0066] | train_loss:0.4491 val_acc:76.7281 val_loss:0.4935
[03/0067] | train_loss:0.4379 val_acc:75.576 val_loss:0.5059
[03/0068] | train_loss:0.4381 val_acc:76.0369 val_loss:0.5017
[03/0069] | train_loss:0.4361 val_acc:74.6544 val_loss:0.5116
[03/0070] | train_loss:0.4317 val_acc:75.8065 val_loss:0.5126
[03/0071] | train_loss:0.4387 val_acc:77.4194 val_loss:0.4909
[03/0072] | train_loss:0.4434 val_acc:77.8802 val_loss:0.4767
[03/0073] | train_loss:0.4373 val_acc:77.6498 val_loss:0.4939
[03/0074] | train_loss:0.43 val_acc:79.4931 val_loss:0.4572
[03/0075] | train_loss:0.4269 val_acc:76.0369 val_loss:0.4847
[03/0076] | train_loss:0.4157 val_acc:75.576 val_loss:0.5042
[03/0077] | train_loss:0.426 val_acc:78.341 val_loss:0.4669
[03/0078] | train_loss:0.4208 val_acc:79.9539 val_loss:0.4568
model is saved at epoch 78!![03/0079] | train_loss:0.4255 val_acc:77.6498 val_loss:0.4739
[03/0080] | train_loss:0.4092 val_acc:77.8802 val_loss:0.4623
[03/0081] | train_loss:0.4059 val_acc:75.576 val_loss:0.5247
[03/0082] | train_loss:0.4031 val_acc:78.5714 val_loss:0.4694
[03/0083] | train_loss:0.4008 val_acc:77.8802 val_loss:0.4877
[03/0084] | train_loss:0.4 val_acc:77.6498 val_loss:0.4895
[03/0085] | train_loss:0.3906 val_acc:77.8802 val_loss:0.4915
[03/0086] | train_loss:0.3917 val_acc:76.7281 val_loss:0.5032
[03/0087] | train_loss:0.3983 val_acc:77.6498 val_loss:0.4768
[03/0088] | train_loss:0.3866 val_acc:76.0369 val_loss:0.5167
[03/0089] | train_loss:0.4055 val_acc:75.8065 val_loss:0.5155
[03/0090] | train_loss:0.3886 val_acc:78.8018 val_loss:0.4833
[03/0091] | train_loss:0.3974 val_acc:80.8756 val_loss:0.4687
model is saved at epoch 91!![03/0092] | train_loss:0.3934 val_acc:80.1843 val_loss:0.4973
[03/0093] | train_loss:0.3956 val_acc:77.6498 val_loss:0.4794
[03/0094] | train_loss:0.3913 val_acc:76.4977 val_loss:0.5255
[03/0095] | train_loss:0.4152 val_acc:76.7281 val_loss:0.5005
[03/0096] | train_loss:0.388 val_acc:78.5714 val_loss:0.4809
[03/0097] | train_loss:0.3845 val_acc:79.2627 val_loss:0.5177
[03/0098] | train_loss:0.3798 val_acc:76.9585 val_loss:0.4969
[03/0099] | train_loss:0.3868 val_acc:76.7281 val_loss:0.5225
[03/0100] | train_loss:0.3712 val_acc:74.6544 val_loss:0.5961
[03/0101] | train_loss:0.3815 val_acc:78.8018 val_loss:0.482
[03/0102] | train_loss:0.3579 val_acc:77.6498 val_loss:0.5207
[03/0103] | train_loss:0.3532 val_acc:76.0369 val_loss:0.5281
[03/0104] | train_loss:0.3515 val_acc:79.7235 val_loss:0.4997
[03/0105] | train_loss:0.363 val_acc:77.1889 val_loss:0.5271
[03/0106] | train_loss:0.3483 val_acc:78.8018 val_loss:0.5091
[03/0107] | train_loss:0.3408 val_acc:78.8018 val_loss:0.5276
[03/0108] | train_loss:0.3607 val_acc:80.6452 val_loss:0.4758
[03/0109] | train_loss:0.3589 val_acc:78.341 val_loss:0.5298
[03/0110] | train_loss:0.3503 val_acc:77.6498 val_loss:0.5085
[03/0111] | train_loss:0.349 val_acc:79.2627 val_loss:0.5182
[03/0112] | train_loss:0.3195 val_acc:79.4931 val_loss:0.5345
[03/0113] | train_loss:0.3262 val_acc:78.8018 val_loss:0.5219
[03/0114] | train_loss:0.323 val_acc:78.1106 val_loss:0.5318
[03/0115] | train_loss:0.3299 val_acc:79.7235 val_loss:0.4956
[03/0116] | train_loss:0.3205 val_acc:76.7281 val_loss:0.5703
[03/0117] | train_loss:0.3294 val_acc:76.7281 val_loss:0.5904
[03/0118] | train_loss:0.3199 val_acc:79.9539 val_loss:0.5106
[03/0119] | train_loss:0.3106 val_acc:79.4931 val_loss:0.5145
[03/0120] | train_loss:0.3109 val_acc:79.7235 val_loss:0.5066
[03/0121] | train_loss:0.3151 val_acc:75.8065 val_loss:0.5628
[03/0122] | train_loss:0.3188 val_acc:78.341 val_loss:0.5131
[03/0123] | train_loss:0.3054 val_acc:76.9585 val_loss:0.613
[03/0124] | train_loss:0.2991 val_acc:80.8756 val_loss:0.5407
[03/0125] | train_loss:0.2927 val_acc:78.1106 val_loss:0.5711
[03/0126] | train_loss:0.2993 val_acc:80.4147 val_loss:0.55
[03/0127] | train_loss:0.2892 val_acc:79.0323 val_loss:0.5524
[03/0128] | train_loss:0.2894 val_acc:79.4931 val_loss:0.5568
[03/0129] | train_loss:0.3062 val_acc:79.4931 val_loss:0.5377
[03/0130] | train_loss:0.3002 val_acc:78.5714 val_loss:0.6068
[03/0131] | train_loss:0.287 val_acc:76.9585 val_loss:0.5894
[03/0132] | train_loss:0.3234 val_acc:81.5668 val_loss:0.513
model is saved at epoch 132!![03/0133] | train_loss:0.3247 val_acc:78.341 val_loss:0.5359
[03/0134] | train_loss:0.3123 val_acc:75.3456 val_loss:0.6639
[03/0135] | train_loss:0.3206 val_acc:78.5714 val_loss:0.5931
[03/0136] | train_loss:0.3067 val_acc:78.5714 val_loss:0.5738
[03/0137] | train_loss:0.3026 val_acc:78.8018 val_loss:0.583
[03/0138] | train_loss:0.2916 val_acc:76.7281 val_loss:0.6458
[03/0139] | train_loss:0.2834 val_acc:80.4147 val_loss:0.5406
[03/0140] | train_loss:0.2761 val_acc:76.4977 val_loss:0.623
[03/0141] | train_loss:0.26 val_acc:80.1843 val_loss:0.5722
[03/0142] | train_loss:0.2667 val_acc:77.1889 val_loss:0.611
[03/0143] | train_loss:0.2843 val_acc:79.4931 val_loss:0.5674
[03/0144] | train_loss:0.2698 val_acc:78.341 val_loss:0.6509
[03/0145] | train_loss:0.2564 val_acc:79.9539 val_loss:0.5803
[03/0146] | train_loss:0.2613 val_acc:78.341 val_loss:0.5496
[03/0147] | train_loss:0.2597 val_acc:78.341 val_loss:0.6305
[03/0148] | train_loss:0.2523 val_acc:78.1106 val_loss:0.6842
[03/0149] | train_loss:0.2714 val_acc:79.0323 val_loss:0.6114
[03/0150] | train_loss:0.2492 val_acc:78.8018 val_loss:0.6219
[03/0151] | train_loss:0.237 val_acc:79.2627 val_loss:0.6392
[03/0152] | train_loss:0.2421 val_acc:79.0323 val_loss:0.5966
[03/0153] | train_loss:0.2517 val_acc:79.7235 val_loss:0.6354
[03/0154] | train_loss:0.2467 val_acc:80.6452 val_loss:0.5928
[03/0155] | train_loss:0.2673 val_acc:78.1106 val_loss:0.6543
[03/0156] | train_loss:0.2501 val_acc:78.5714 val_loss:0.5839
[03/0157] | train_loss:0.2466 val_acc:79.0323 val_loss:0.677
[03/0158] | train_loss:0.2415 val_acc:79.0323 val_loss:0.6366
[03/0159] | train_loss:0.235 val_acc:79.4931 val_loss:0.6046
[03/0160] | train_loss:0.2305 val_acc:78.1106 val_loss:0.686
[03/0161] | train_loss:0.227 val_acc:80.8756 val_loss:0.6258
[03/0162] | train_loss:0.2251 val_acc:76.7281 val_loss:0.7365
[03/0163] | train_loss:0.2218 val_acc:78.5714 val_loss:0.665
[03/0164] | train_loss:0.2098 val_acc:76.0369 val_loss:0.7602
[03/0165] | train_loss:0.2268 val_acc:78.341 val_loss:0.7045
[03/0166] | train_loss:0.2164 val_acc:78.1106 val_loss:0.685
[03/0167] | train_loss:0.2407 val_acc:78.8018 val_loss:0.6839
[03/0168] | train_loss:0.2182 val_acc:78.5714 val_loss:0.6918
[03/0169] | train_loss:0.2093 val_acc:77.4194 val_loss:0.717
[03/0170] | train_loss:0.2345 val_acc:79.0323 val_loss:0.6665
[03/0171] | train_loss:0.227 val_acc:78.341 val_loss:0.6949
[03/0172] | train_loss:0.2049 val_acc:77.8802 val_loss:0.7566
[03/0173] | train_loss:0.2099 val_acc:77.6498 val_loss:0.7711
[03/0174] | train_loss:0.2333 val_acc:80.4147 val_loss:0.6657
[03/0175] | train_loss:0.2231 val_acc:78.5714 val_loss:0.6688
[03/0176] | train_loss:0.2181 val_acc:79.2627 val_loss:0.7028
[03/0177] | train_loss:0.2069 val_acc:78.1106 val_loss:0.6797
[03/0178] | train_loss:0.2207 val_acc:77.4194 val_loss:0.7325
[03/0179] | train_loss:0.2055 val_acc:80.4147 val_loss:0.7089
[03/0180] | train_loss:0.195 val_acc:78.1106 val_loss:0.7454
[03/0181] | train_loss:0.1987 val_acc:76.7281 val_loss:0.7943
[03/0182] | train_loss:0.2241 val_acc:80.8756 val_loss:0.7421
[03/0183] | train_loss:0.2109 val_acc:79.0323 val_loss:0.6793
Fold: [3/10] Test is finish !! 
 Test Metrics are: test_acc:81.5668 test_loss:0.4184fold [3/10] is start!!
[04/0001] | train_loss:0.7579 val_acc:54.8387 val_loss:0.6852
model is saved at epoch 1!![04/0002] | train_loss:0.7448 val_acc:54.8387 val_loss:0.6716
[04/0003] | train_loss:0.7207 val_acc:66.5899 val_loss:0.6532
model is saved at epoch 3!![04/0004] | train_loss:0.6876 val_acc:67.0507 val_loss:0.6114
model is saved at epoch 4!![04/0005] | train_loss:0.6519 val_acc:69.5853 val_loss:0.5763
model is saved at epoch 5!![04/0006] | train_loss:0.6451 val_acc:69.1244 val_loss:0.5824
[04/0007] | train_loss:0.6245 val_acc:72.1198 val_loss:0.5574
model is saved at epoch 7!![04/0008] | train_loss:0.6291 val_acc:70.5069 val_loss:0.5606
[04/0009] | train_loss:0.6404 val_acc:70.5069 val_loss:0.5636
[04/0010] | train_loss:0.6193 val_acc:69.3548 val_loss:0.5683
[04/0011] | train_loss:0.6059 val_acc:70.5069 val_loss:0.5522
[04/0012] | train_loss:0.6006 val_acc:75.576 val_loss:0.5235
model is saved at epoch 12!![04/0013] | train_loss:0.5981 val_acc:75.3456 val_loss:0.5331
[04/0014] | train_loss:0.5961 val_acc:72.5806 val_loss:0.522
[04/0015] | train_loss:0.5887 val_acc:72.5806 val_loss:0.5268
[04/0016] | train_loss:0.5932 val_acc:70.5069 val_loss:0.5417
[04/0017] | train_loss:0.6006 val_acc:74.6544 val_loss:0.5213
[04/0018] | train_loss:0.5878 val_acc:73.9631 val_loss:0.5197
[04/0019] | train_loss:0.5782 val_acc:75.8065 val_loss:0.5091
model is saved at epoch 19!![04/0020] | train_loss:0.5637 val_acc:75.3456 val_loss:0.4996
[04/0021] | train_loss:0.5597 val_acc:76.9585 val_loss:0.4917
model is saved at epoch 21!![04/0022] | train_loss:0.5515 val_acc:76.4977 val_loss:0.49
[04/0023] | train_loss:0.548 val_acc:75.576 val_loss:0.4972
[04/0024] | train_loss:0.5467 val_acc:76.7281 val_loss:0.4909
[04/0025] | train_loss:0.5428 val_acc:76.7281 val_loss:0.499
[04/0026] | train_loss:0.5413 val_acc:75.3456 val_loss:0.4746
[04/0027] | train_loss:0.5232 val_acc:76.7281 val_loss:0.4715
[04/0028] | train_loss:0.5305 val_acc:76.2673 val_loss:0.4908
[04/0029] | train_loss:0.5241 val_acc:76.4977 val_loss:0.4651
[04/0030] | train_loss:0.5178 val_acc:77.6498 val_loss:0.4651
model is saved at epoch 30!![04/0031] | train_loss:0.5143 val_acc:78.8018 val_loss:0.4615
model is saved at epoch 31!![04/0032] | train_loss:0.515 val_acc:70.7373 val_loss:0.5385
[04/0033] | train_loss:0.5478 val_acc:76.0369 val_loss:0.492
[04/0034] | train_loss:0.542 val_acc:76.7281 val_loss:0.4912
[04/0035] | train_loss:0.5178 val_acc:73.5023 val_loss:0.5331
[04/0036] | train_loss:0.525 val_acc:75.3456 val_loss:0.4931
[04/0037] | train_loss:0.5219 val_acc:77.4194 val_loss:0.4712
[04/0038] | train_loss:0.5125 val_acc:77.6498 val_loss:0.4648
[04/0039] | train_loss:0.5082 val_acc:77.6498 val_loss:0.461
[04/0040] | train_loss:0.5162 val_acc:77.4194 val_loss:0.4722
[04/0041] | train_loss:0.5068 val_acc:78.1106 val_loss:0.4656
[04/0042] | train_loss:0.5083 val_acc:77.4194 val_loss:0.4669
[04/0043] | train_loss:0.4961 val_acc:79.7235 val_loss:0.4594
model is saved at epoch 43!![04/0044] | train_loss:0.492 val_acc:77.6498 val_loss:0.4655
[04/0045] | train_loss:0.4912 val_acc:78.341 val_loss:0.4538
[04/0046] | train_loss:0.4927 val_acc:77.8802 val_loss:0.4689
[04/0047] | train_loss:0.4985 val_acc:77.6498 val_loss:0.4539
[04/0048] | train_loss:0.4891 val_acc:78.1106 val_loss:0.4706
[04/0049] | train_loss:0.4835 val_acc:78.1106 val_loss:0.4531
[04/0050] | train_loss:0.4851 val_acc:78.341 val_loss:0.4607
[04/0051] | train_loss:0.4901 val_acc:76.9585 val_loss:0.4793
[04/0052] | train_loss:0.4876 val_acc:77.4194 val_loss:0.4558
[04/0053] | train_loss:0.4771 val_acc:77.8802 val_loss:0.4596
[04/0054] | train_loss:0.4729 val_acc:79.9539 val_loss:0.44
model is saved at epoch 54!![04/0055] | train_loss:0.473 val_acc:79.2627 val_loss:0.4545
[04/0056] | train_loss:0.4689 val_acc:78.341 val_loss:0.4539
[04/0057] | train_loss:0.4632 val_acc:77.6498 val_loss:0.458
[04/0058] | train_loss:0.4648 val_acc:79.7235 val_loss:0.4367
[04/0059] | train_loss:0.4562 val_acc:79.4931 val_loss:0.4403
[04/0060] | train_loss:0.4551 val_acc:78.1106 val_loss:0.4516
[04/0061] | train_loss:0.4505 val_acc:79.7235 val_loss:0.4407
[04/0062] | train_loss:0.4519 val_acc:78.8018 val_loss:0.4394
[04/0063] | train_loss:0.4512 val_acc:79.7235 val_loss:0.4328
[04/0064] | train_loss:0.4433 val_acc:78.5714 val_loss:0.4378
[04/0065] | train_loss:0.4437 val_acc:79.2627 val_loss:0.4345
[04/0066] | train_loss:0.4469 val_acc:79.4931 val_loss:0.4435
[04/0067] | train_loss:0.4515 val_acc:79.4931 val_loss:0.4522
[04/0068] | train_loss:0.4441 val_acc:78.8018 val_loss:0.4541
[04/0069] | train_loss:0.443 val_acc:79.4931 val_loss:0.4549
[04/0070] | train_loss:0.4438 val_acc:78.8018 val_loss:0.4515
[04/0071] | train_loss:0.4445 val_acc:78.8018 val_loss:0.4412
[04/0072] | train_loss:0.4469 val_acc:77.6498 val_loss:0.4724
[04/0073] | train_loss:0.4508 val_acc:77.4194 val_loss:0.4707
[04/0074] | train_loss:0.448 val_acc:79.2627 val_loss:0.4416
[04/0075] | train_loss:0.4555 val_acc:79.9539 val_loss:0.4588
[04/0076] | train_loss:0.4504 val_acc:80.6452 val_loss:0.4343
model is saved at epoch 76!![04/0077] | train_loss:0.437 val_acc:79.2627 val_loss:0.4413
[04/0078] | train_loss:0.4278 val_acc:80.4147 val_loss:0.4328
[04/0079] | train_loss:0.4172 val_acc:80.8756 val_loss:0.4314
model is saved at epoch 79!![04/0080] | train_loss:0.4207 val_acc:80.8756 val_loss:0.4269
[04/0081] | train_loss:0.4161 val_acc:80.4147 val_loss:0.4428
[04/0082] | train_loss:0.4115 val_acc:81.5668 val_loss:0.4251
model is saved at epoch 82!![04/0083] | train_loss:0.4131 val_acc:80.4147 val_loss:0.4272
[04/0084] | train_loss:0.4067 val_acc:82.4885 val_loss:0.4182
model is saved at epoch 84!![04/0085] | train_loss:0.4124 val_acc:82.0276 val_loss:0.4261
[04/0086] | train_loss:0.4144 val_acc:79.0323 val_loss:0.4462
[04/0087] | train_loss:0.3995 val_acc:79.9539 val_loss:0.4283
[04/0088] | train_loss:0.3904 val_acc:81.3364 val_loss:0.4213
[04/0089] | train_loss:0.3944 val_acc:81.5668 val_loss:0.4187
[04/0090] | train_loss:0.3834 val_acc:81.7972 val_loss:0.4277
[04/0091] | train_loss:0.3852 val_acc:81.5668 val_loss:0.4175
[04/0092] | train_loss:0.3843 val_acc:79.0323 val_loss:0.4574
[04/0093] | train_loss:0.4096 val_acc:82.7189 val_loss:0.4101
model is saved at epoch 93!![04/0094] | train_loss:0.3946 val_acc:82.2581 val_loss:0.4397
[04/0095] | train_loss:0.3919 val_acc:80.6452 val_loss:0.4374
[04/0096] | train_loss:0.3889 val_acc:82.0276 val_loss:0.4146
[04/0097] | train_loss:0.374 val_acc:80.8756 val_loss:0.4417
[04/0098] | train_loss:0.3843 val_acc:79.9539 val_loss:0.438
[04/0099] | train_loss:0.3748 val_acc:81.3364 val_loss:0.4156
[04/0100] | train_loss:0.3772 val_acc:82.4885 val_loss:0.434
[04/0101] | train_loss:0.3857 val_acc:80.6452 val_loss:0.4349
[04/0102] | train_loss:0.3788 val_acc:82.4885 val_loss:0.4189
[04/0103] | train_loss:0.3658 val_acc:79.9539 val_loss:0.4347
[04/0104] | train_loss:0.3586 val_acc:82.0276 val_loss:0.4263
[04/0105] | train_loss:0.3525 val_acc:81.5668 val_loss:0.4219
[04/0106] | train_loss:0.3504 val_acc:82.7189 val_loss:0.4135
[04/0107] | train_loss:0.3476 val_acc:82.2581 val_loss:0.4226
[04/0108] | train_loss:0.3525 val_acc:80.4147 val_loss:0.4257
[04/0109] | train_loss:0.3514 val_acc:83.871 val_loss:0.4071
model is saved at epoch 109!![04/0110] | train_loss:0.349 val_acc:82.4885 val_loss:0.4201
[04/0111] | train_loss:0.3394 val_acc:82.2581 val_loss:0.4154
[04/0112] | train_loss:0.3373 val_acc:80.1843 val_loss:0.4244
[04/0113] | train_loss:0.3358 val_acc:81.106 val_loss:0.4305
[04/0114] | train_loss:0.3324 val_acc:80.8756 val_loss:0.4376
[04/0115] | train_loss:0.3385 val_acc:81.5668 val_loss:0.4209
[04/0116] | train_loss:0.3272 val_acc:82.4885 val_loss:0.4243
[04/0117] | train_loss:0.3111 val_acc:82.7189 val_loss:0.4239
[04/0118] | train_loss:0.3286 val_acc:83.4101 val_loss:0.4209
[04/0119] | train_loss:0.3524 val_acc:80.4147 val_loss:0.4407
[04/0120] | train_loss:0.3578 val_acc:80.8756 val_loss:0.4348
[04/0121] | train_loss:0.3333 val_acc:79.9539 val_loss:0.4611
[04/0122] | train_loss:0.3286 val_acc:83.871 val_loss:0.4099
[04/0123] | train_loss:0.318 val_acc:82.4885 val_loss:0.4015
[04/0124] | train_loss:0.3121 val_acc:80.8756 val_loss:0.4267
[04/0125] | train_loss:0.3131 val_acc:82.9493 val_loss:0.4131
[04/0126] | train_loss:0.3287 val_acc:81.3364 val_loss:0.438
[04/0127] | train_loss:0.3407 val_acc:82.9493 val_loss:0.4229
[04/0128] | train_loss:0.3253 val_acc:81.7972 val_loss:0.4371
[04/0129] | train_loss:0.3247 val_acc:83.871 val_loss:0.4147
[04/0130] | train_loss:0.306 val_acc:82.9493 val_loss:0.4261
[04/0131] | train_loss:0.3075 val_acc:81.106 val_loss:0.4145
[04/0132] | train_loss:0.2934 val_acc:81.7972 val_loss:0.4331
[04/0133] | train_loss:0.305 val_acc:81.3364 val_loss:0.4425
[04/0134] | train_loss:0.2973 val_acc:80.8756 val_loss:0.4674
[04/0135] | train_loss:0.2976 val_acc:81.5668 val_loss:0.4415
[04/0136] | train_loss:0.2879 val_acc:83.871 val_loss:0.4328
[04/0137] | train_loss:0.297 val_acc:80.8756 val_loss:0.4563
[04/0138] | train_loss:0.2811 val_acc:84.1014 val_loss:0.4348
model is saved at epoch 138!![04/0139] | train_loss:0.2735 val_acc:82.7189 val_loss:0.4442
[04/0140] | train_loss:0.2799 val_acc:81.3364 val_loss:0.4446
[04/0141] | train_loss:0.2813 val_acc:80.8756 val_loss:0.4624
[04/0142] | train_loss:0.2897 val_acc:84.1014 val_loss:0.4448
[04/0143] | train_loss:0.2811 val_acc:78.8018 val_loss:0.5046
[04/0144] | train_loss:0.2958 val_acc:80.6452 val_loss:0.4307
[04/0145] | train_loss:0.287 val_acc:82.0276 val_loss:0.4427
[04/0146] | train_loss:0.3301 val_acc:79.4931 val_loss:0.4695
[04/0147] | train_loss:0.3597 val_acc:81.3364 val_loss:0.4394
[04/0148] | train_loss:0.3361 val_acc:82.9493 val_loss:0.4422
[04/0149] | train_loss:0.2941 val_acc:83.6406 val_loss:0.461
[04/0150] | train_loss:0.2736 val_acc:79.0323 val_loss:0.513
[04/0151] | train_loss:0.2755 val_acc:82.2581 val_loss:0.4576
[04/0152] | train_loss:0.2532 val_acc:82.0276 val_loss:0.4815
[04/0153] | train_loss:0.2562 val_acc:82.7189 val_loss:0.466
[04/0154] | train_loss:0.2565 val_acc:81.7972 val_loss:0.4719
[04/0155] | train_loss:0.2554 val_acc:83.1797 val_loss:0.4533
[04/0156] | train_loss:0.2429 val_acc:82.9493 val_loss:0.4565
[04/0157] | train_loss:0.257 val_acc:81.7972 val_loss:0.4732
[04/0158] | train_loss:0.2601 val_acc:82.0276 val_loss:0.4549
[04/0159] | train_loss:0.2421 val_acc:81.7972 val_loss:0.4847
[04/0160] | train_loss:0.2343 val_acc:83.6406 val_loss:0.4739
[04/0161] | train_loss:0.2389 val_acc:81.3364 val_loss:0.5064
[04/0162] | train_loss:0.2424 val_acc:79.4931 val_loss:0.5176
[04/0163] | train_loss:0.2533 val_acc:82.4885 val_loss:0.4881
[04/0164] | train_loss:0.2463 val_acc:80.4147 val_loss:0.4759
[04/0165] | train_loss:0.2304 val_acc:81.106 val_loss:0.5197
[04/0166] | train_loss:0.2174 val_acc:82.9493 val_loss:0.5021
[04/0167] | train_loss:0.233 val_acc:82.4885 val_loss:0.4856
[04/0168] | train_loss:0.229 val_acc:80.8756 val_loss:0.5003
[04/0169] | train_loss:0.2212 val_acc:80.6452 val_loss:0.5271
[04/0170] | train_loss:0.2252 val_acc:81.5668 val_loss:0.4993
[04/0171] | train_loss:0.2132 val_acc:82.4885 val_loss:0.5019
[04/0172] | train_loss:0.2241 val_acc:82.7189 val_loss:0.5228
[04/0173] | train_loss:0.2243 val_acc:79.9539 val_loss:0.5249
[04/0174] | train_loss:0.2148 val_acc:81.3364 val_loss:0.5187
[04/0175] | train_loss:0.2084 val_acc:82.2581 val_loss:0.5263
[04/0176] | train_loss:0.2302 val_acc:80.6452 val_loss:0.4992
[04/0177] | train_loss:0.2127 val_acc:82.0276 val_loss:0.5351
[04/0178] | train_loss:0.2152 val_acc:79.9539 val_loss:0.5615
[04/0179] | train_loss:0.2092 val_acc:81.106 val_loss:0.5534
[04/0180] | train_loss:0.2411 val_acc:81.7972 val_loss:0.5208
[04/0181] | train_loss:0.2277 val_acc:80.8756 val_loss:0.5556
[04/0182] | train_loss:0.2156 val_acc:82.7189 val_loss:0.5334
[04/0183] | train_loss:0.2254 val_acc:80.1843 val_loss:0.5445
[04/0184] | train_loss:0.204 val_acc:80.8756 val_loss:0.5532
[04/0185] | train_loss:0.1999 val_acc:81.5668 val_loss:0.551
[04/0186] | train_loss:0.2 val_acc:82.0276 val_loss:0.5204
[04/0187] | train_loss:0.1896 val_acc:81.5668 val_loss:0.5639
[04/0188] | train_loss:0.1944 val_acc:82.0276 val_loss:0.5439
[04/0189] | train_loss:0.1953 val_acc:80.4147 val_loss:0.6023
Fold: [4/10] Test is finish !! 
 Test Metrics are: test_acc:80.4147 test_loss:0.5609fold [4/10] is start!!
[05/0001] | train_loss:0.756 val_acc:55.9908 val_loss:0.6816
model is saved at epoch 1!![05/0002] | train_loss:0.7463 val_acc:55.9908 val_loss:0.6713
[05/0003] | train_loss:0.7345 val_acc:55.7604 val_loss:0.6537
[05/0004] | train_loss:0.7167 val_acc:67.7419 val_loss:0.6338
model is saved at epoch 4!![05/0005] | train_loss:0.7017 val_acc:65.6682 val_loss:0.6129
[05/0006] | train_loss:0.6844 val_acc:67.9724 val_loss:0.5996
model is saved at epoch 6!![05/0007] | train_loss:0.6825 val_acc:72.1198 val_loss:0.5825
model is saved at epoch 7!![05/0008] | train_loss:0.6544 val_acc:71.659 val_loss:0.5705
[05/0009] | train_loss:0.6373 val_acc:72.5806 val_loss:0.5526
model is saved at epoch 9!![05/0010] | train_loss:0.6341 val_acc:72.5806 val_loss:0.5367
[05/0011] | train_loss:0.6238 val_acc:73.7327 val_loss:0.537
model is saved at epoch 11!![05/0012] | train_loss:0.6121 val_acc:73.7327 val_loss:0.5329
[05/0013] | train_loss:0.6129 val_acc:71.659 val_loss:0.5673
[05/0014] | train_loss:0.6079 val_acc:73.0415 val_loss:0.5302
[05/0015] | train_loss:0.6048 val_acc:73.5023 val_loss:0.5237
[05/0016] | train_loss:0.5997 val_acc:75.1152 val_loss:0.5226
model is saved at epoch 16!![05/0017] | train_loss:0.5871 val_acc:73.2719 val_loss:0.5377
[05/0018] | train_loss:0.5873 val_acc:73.9631 val_loss:0.5189
[05/0019] | train_loss:0.5815 val_acc:74.6544 val_loss:0.5173
[05/0020] | train_loss:0.5804 val_acc:74.8848 val_loss:0.5176
[05/0021] | train_loss:0.584 val_acc:74.1935 val_loss:0.525
[05/0022] | train_loss:0.5918 val_acc:74.6544 val_loss:0.5203
[05/0023] | train_loss:0.5932 val_acc:72.1198 val_loss:0.5601
[05/0024] | train_loss:0.6078 val_acc:74.8848 val_loss:0.5267
[05/0025] | train_loss:0.5835 val_acc:75.576 val_loss:0.5226
model is saved at epoch 25!![05/0026] | train_loss:0.5823 val_acc:74.6544 val_loss:0.5184
[05/0027] | train_loss:0.5812 val_acc:74.424 val_loss:0.5162
[05/0028] | train_loss:0.5837 val_acc:73.7327 val_loss:0.5414
[05/0029] | train_loss:0.5803 val_acc:75.3456 val_loss:0.5129
[05/0030] | train_loss:0.5673 val_acc:74.6544 val_loss:0.5078
[05/0031] | train_loss:0.5576 val_acc:75.1152 val_loss:0.5111
[05/0032] | train_loss:0.5565 val_acc:76.2673 val_loss:0.508
model is saved at epoch 32!![05/0033] | train_loss:0.5496 val_acc:76.9585 val_loss:0.5014
model is saved at epoch 33!![05/0034] | train_loss:0.5424 val_acc:76.2673 val_loss:0.4987
[05/0035] | train_loss:0.5457 val_acc:76.9585 val_loss:0.5013
[05/0036] | train_loss:0.5314 val_acc:77.4194 val_loss:0.4935
model is saved at epoch 36!![05/0037] | train_loss:0.5254 val_acc:75.8065 val_loss:0.5021
[05/0038] | train_loss:0.5286 val_acc:75.1152 val_loss:0.5117
[05/0039] | train_loss:0.5331 val_acc:76.2673 val_loss:0.5063
[05/0040] | train_loss:0.5265 val_acc:75.8065 val_loss:0.5116
[05/0041] | train_loss:0.5241 val_acc:76.2673 val_loss:0.504
[05/0042] | train_loss:0.5231 val_acc:77.1889 val_loss:0.4818
[05/0043] | train_loss:0.5092 val_acc:76.7281 val_loss:0.5008
[05/0044] | train_loss:0.5078 val_acc:77.1889 val_loss:0.4939
[05/0045] | train_loss:0.5103 val_acc:76.0369 val_loss:0.5015
[05/0046] | train_loss:0.5035 val_acc:75.576 val_loss:0.5132
[05/0047] | train_loss:0.4968 val_acc:77.1889 val_loss:0.5082
[05/0048] | train_loss:0.5007 val_acc:75.576 val_loss:0.4964
[05/0049] | train_loss:0.507 val_acc:75.576 val_loss:0.5027
[05/0050] | train_loss:0.4972 val_acc:77.6498 val_loss:0.4887
model is saved at epoch 50!![05/0051] | train_loss:0.4904 val_acc:76.2673 val_loss:0.4951
[05/0052] | train_loss:0.4928 val_acc:76.0369 val_loss:0.494
[05/0053] | train_loss:0.4815 val_acc:77.1889 val_loss:0.497
[05/0054] | train_loss:0.4873 val_acc:78.341 val_loss:0.4855
model is saved at epoch 54!![05/0055] | train_loss:0.4839 val_acc:76.9585 val_loss:0.5025
[05/0056] | train_loss:0.4683 val_acc:76.7281 val_loss:0.484
[05/0057] | train_loss:0.4708 val_acc:76.0369 val_loss:0.4865
[05/0058] | train_loss:0.4659 val_acc:77.4194 val_loss:0.5027
[05/0059] | train_loss:0.4701 val_acc:77.8802 val_loss:0.4773
[05/0060] | train_loss:0.4623 val_acc:77.8802 val_loss:0.4903
[05/0061] | train_loss:0.4538 val_acc:78.1106 val_loss:0.4742
[05/0062] | train_loss:0.4576 val_acc:77.4194 val_loss:0.4919
[05/0063] | train_loss:0.4626 val_acc:76.9585 val_loss:0.513
[05/0064] | train_loss:0.4647 val_acc:79.4931 val_loss:0.4776
model is saved at epoch 64!![05/0065] | train_loss:0.4623 val_acc:79.4931 val_loss:0.4866
[05/0066] | train_loss:0.4554 val_acc:78.341 val_loss:0.478
[05/0067] | train_loss:0.452 val_acc:76.7281 val_loss:0.5083
[05/0068] | train_loss:0.4433 val_acc:78.5714 val_loss:0.4837
[05/0069] | train_loss:0.4363 val_acc:77.6498 val_loss:0.4991
[05/0070] | train_loss:0.4369 val_acc:78.5714 val_loss:0.4738
[05/0071] | train_loss:0.4313 val_acc:77.1889 val_loss:0.514
[05/0072] | train_loss:0.4342 val_acc:79.7235 val_loss:0.4983
model is saved at epoch 72!![05/0073] | train_loss:0.4422 val_acc:76.7281 val_loss:0.4959
[05/0074] | train_loss:0.4238 val_acc:78.8018 val_loss:0.4804
[05/0075] | train_loss:0.4325 val_acc:77.6498 val_loss:0.5015
[05/0076] | train_loss:0.4247 val_acc:77.6498 val_loss:0.4911
[05/0077] | train_loss:0.4243 val_acc:79.2627 val_loss:0.4851
[05/0078] | train_loss:0.4154 val_acc:79.2627 val_loss:0.498
[05/0079] | train_loss:0.4169 val_acc:79.0323 val_loss:0.4813
[05/0080] | train_loss:0.4177 val_acc:80.4147 val_loss:0.4795
model is saved at epoch 80!![05/0081] | train_loss:0.4076 val_acc:79.9539 val_loss:0.4823
[05/0082] | train_loss:0.4075 val_acc:79.7235 val_loss:0.484
[05/0083] | train_loss:0.4083 val_acc:80.8756 val_loss:0.4782
model is saved at epoch 83!![05/0084] | train_loss:0.399 val_acc:80.8756 val_loss:0.4871
[05/0085] | train_loss:0.3978 val_acc:79.2627 val_loss:0.4736
[05/0086] | train_loss:0.3881 val_acc:77.4194 val_loss:0.5082
[05/0087] | train_loss:0.3986 val_acc:81.106 val_loss:0.4929
model is saved at epoch 87!![05/0088] | train_loss:0.4037 val_acc:81.3364 val_loss:0.4799
model is saved at epoch 88!![05/0089] | train_loss:0.4122 val_acc:78.8018 val_loss:0.4857
[05/0090] | train_loss:0.402 val_acc:80.1843 val_loss:0.4814
[05/0091] | train_loss:0.3779 val_acc:80.4147 val_loss:0.484
[05/0092] | train_loss:0.3894 val_acc:79.0323 val_loss:0.4981
[05/0093] | train_loss:0.3818 val_acc:79.9539 val_loss:0.4888
[05/0094] | train_loss:0.3749 val_acc:80.6452 val_loss:0.511
[05/0095] | train_loss:0.3855 val_acc:78.8018 val_loss:0.4856
[05/0096] | train_loss:0.3797 val_acc:78.8018 val_loss:0.4984
[05/0097] | train_loss:0.3757 val_acc:79.7235 val_loss:0.5033
[05/0098] | train_loss:0.38 val_acc:79.9539 val_loss:0.4885
[05/0099] | train_loss:0.3595 val_acc:81.3364 val_loss:0.4976
[05/0100] | train_loss:0.356 val_acc:79.7235 val_loss:0.4734
[05/0101] | train_loss:0.3566 val_acc:77.6498 val_loss:0.5828
[05/0102] | train_loss:0.3997 val_acc:79.4931 val_loss:0.4744
[05/0103] | train_loss:0.3758 val_acc:79.4931 val_loss:0.5135
[05/0104] | train_loss:0.3771 val_acc:80.6452 val_loss:0.505
[05/0105] | train_loss:0.3692 val_acc:79.2627 val_loss:0.5054
[05/0106] | train_loss:0.3486 val_acc:77.8802 val_loss:0.5189
[05/0107] | train_loss:0.3558 val_acc:80.4147 val_loss:0.5003
[05/0108] | train_loss:0.3713 val_acc:78.8018 val_loss:0.5032
[05/0109] | train_loss:0.3581 val_acc:80.1843 val_loss:0.5088
[05/0110] | train_loss:0.338 val_acc:79.0323 val_loss:0.5369
[05/0111] | train_loss:0.3373 val_acc:80.8756 val_loss:0.4952
[05/0112] | train_loss:0.3445 val_acc:80.4147 val_loss:0.5044
[05/0113] | train_loss:0.3376 val_acc:80.4147 val_loss:0.4762
[05/0114] | train_loss:0.3161 val_acc:79.4931 val_loss:0.5394
[05/0115] | train_loss:0.3401 val_acc:80.6452 val_loss:0.514
[05/0116] | train_loss:0.3329 val_acc:76.4977 val_loss:0.5837
[05/0117] | train_loss:0.3527 val_acc:79.4931 val_loss:0.5078
[05/0118] | train_loss:0.3231 val_acc:78.8018 val_loss:0.5379
[05/0119] | train_loss:0.3181 val_acc:79.4931 val_loss:0.5164
[05/0120] | train_loss:0.3166 val_acc:77.6498 val_loss:0.613
[05/0121] | train_loss:0.3479 val_acc:80.1843 val_loss:0.5051
[05/0122] | train_loss:0.3822 val_acc:79.0323 val_loss:0.5096
[05/0123] | train_loss:0.3669 val_acc:79.9539 val_loss:0.5107
[05/0124] | train_loss:0.3383 val_acc:79.9539 val_loss:0.4973
[05/0125] | train_loss:0.3175 val_acc:79.0323 val_loss:0.5232
[05/0126] | train_loss:0.3019 val_acc:78.8018 val_loss:0.542
[05/0127] | train_loss:0.311 val_acc:79.9539 val_loss:0.5424
[05/0128] | train_loss:0.3058 val_acc:79.4931 val_loss:0.5394
[05/0129] | train_loss:0.2909 val_acc:78.341 val_loss:0.5843
[05/0130] | train_loss:0.3139 val_acc:79.9539 val_loss:0.516
[05/0131] | train_loss:0.2976 val_acc:79.2627 val_loss:0.5783
[05/0132] | train_loss:0.2821 val_acc:79.0323 val_loss:0.5614
[05/0133] | train_loss:0.2998 val_acc:80.8756 val_loss:0.532
[05/0134] | train_loss:0.3104 val_acc:80.1843 val_loss:0.562
[05/0135] | train_loss:0.2849 val_acc:80.6452 val_loss:0.5613
[05/0136] | train_loss:0.2671 val_acc:81.5668 val_loss:0.5649
model is saved at epoch 136!![05/0137] | train_loss:0.2829 val_acc:79.9539 val_loss:0.5871
[05/0138] | train_loss:0.285 val_acc:79.2627 val_loss:0.5687
[05/0139] | train_loss:0.2649 val_acc:78.8018 val_loss:0.5878
[05/0140] | train_loss:0.2819 val_acc:74.1935 val_loss:0.5952
[05/0141] | train_loss:0.279 val_acc:78.341 val_loss:0.5288
[05/0142] | train_loss:0.2867 val_acc:79.0323 val_loss:0.6471
[05/0143] | train_loss:0.2745 val_acc:77.8802 val_loss:0.5876
[05/0144] | train_loss:0.2634 val_acc:79.0323 val_loss:0.5832
[05/0145] | train_loss:0.276 val_acc:78.341 val_loss:0.5945
[05/0146] | train_loss:0.2818 val_acc:80.1843 val_loss:0.5637
[05/0147] | train_loss:0.2806 val_acc:79.4931 val_loss:0.5759
[05/0148] | train_loss:0.2963 val_acc:79.4931 val_loss:0.6014
[05/0149] | train_loss:0.2993 val_acc:78.341 val_loss:0.566
[05/0150] | train_loss:0.2873 val_acc:79.2627 val_loss:0.5466
[05/0151] | train_loss:0.2849 val_acc:79.9539 val_loss:0.5469
[05/0152] | train_loss:0.2572 val_acc:78.5714 val_loss:0.6214
[05/0153] | train_loss:0.2694 val_acc:79.7235 val_loss:0.6589
[05/0154] | train_loss:0.2751 val_acc:78.5714 val_loss:0.5334
[05/0155] | train_loss:0.2616 val_acc:78.5714 val_loss:0.6526
[05/0156] | train_loss:0.2644 val_acc:79.4931 val_loss:0.5786
[05/0157] | train_loss:0.2452 val_acc:79.7235 val_loss:0.6078
[05/0158] | train_loss:0.2465 val_acc:79.9539 val_loss:0.6527
[05/0159] | train_loss:0.2317 val_acc:79.4931 val_loss:0.652
[05/0160] | train_loss:0.2527 val_acc:79.7235 val_loss:0.63
[05/0161] | train_loss:0.2372 val_acc:80.1843 val_loss:0.6207
[05/0162] | train_loss:0.2376 val_acc:79.7235 val_loss:0.6829
[05/0163] | train_loss:0.2367 val_acc:81.3364 val_loss:0.6011
[05/0164] | train_loss:0.2375 val_acc:79.2627 val_loss:0.6832
[05/0165] | train_loss:0.2296 val_acc:80.1843 val_loss:0.6371
[05/0166] | train_loss:0.215 val_acc:78.1106 val_loss:0.6604
[05/0167] | train_loss:0.2142 val_acc:78.5714 val_loss:0.662
[05/0168] | train_loss:0.2212 val_acc:79.2627 val_loss:0.671
[05/0169] | train_loss:0.2429 val_acc:80.8756 val_loss:0.7011
[05/0170] | train_loss:0.2439 val_acc:80.6452 val_loss:0.6326
[05/0171] | train_loss:0.2282 val_acc:79.7235 val_loss:0.6906
[05/0172] | train_loss:0.2378 val_acc:79.7235 val_loss:0.6681
[05/0173] | train_loss:0.2215 val_acc:79.4931 val_loss:0.6963
[05/0174] | train_loss:0.2317 val_acc:79.0323 val_loss:0.6745
[05/0175] | train_loss:0.2093 val_acc:77.6498 val_loss:0.7426
[05/0176] | train_loss:0.2296 val_acc:77.8802 val_loss:0.6863
[05/0177] | train_loss:0.212 val_acc:78.5714 val_loss:0.7301
[05/0178] | train_loss:0.2416 val_acc:79.7235 val_loss:0.6362
[05/0179] | train_loss:0.2193 val_acc:80.4147 val_loss:0.735
[05/0180] | train_loss:0.2306 val_acc:79.4931 val_loss:0.7127
[05/0181] | train_loss:0.2221 val_acc:79.2627 val_loss:0.6568
[05/0182] | train_loss:0.1993 val_acc:79.7235 val_loss:0.7154
[05/0183] | train_loss:0.1975 val_acc:79.9539 val_loss:0.7384
[05/0184] | train_loss:0.1977 val_acc:79.0323 val_loss:0.7639
[05/0185] | train_loss:0.1978 val_acc:77.4194 val_loss:0.7395
[05/0186] | train_loss:0.1957 val_acc:79.2627 val_loss:0.7498
[05/0187] | train_loss:0.1916 val_acc:77.8802 val_loss:0.756
Fold: [5/10] Test is finish !! 
 Test Metrics are: test_acc:79.2627 test_loss:0.518fold [5/10] is start!!
[06/0001] | train_loss:0.7592 val_acc:48.8479 val_loss:0.6968
model is saved at epoch 1!![06/0002] | train_loss:0.751 val_acc:48.8479 val_loss:0.6968
[06/0003] | train_loss:0.7431 val_acc:48.8479 val_loss:0.6904
[06/0004] | train_loss:0.7315 val_acc:48.8479 val_loss:0.6731
[06/0005] | train_loss:0.7184 val_acc:58.0645 val_loss:0.6579
model is saved at epoch 5!![06/0006] | train_loss:0.6958 val_acc:67.5115 val_loss:0.627
model is saved at epoch 6!![06/0007] | train_loss:0.6688 val_acc:64.7465 val_loss:0.6107
[06/0008] | train_loss:0.6618 val_acc:76.0369 val_loss:0.5788
model is saved at epoch 8!![06/0009] | train_loss:0.6434 val_acc:76.4977 val_loss:0.5833
model is saved at epoch 9!![06/0010] | train_loss:0.6291 val_acc:77.8802 val_loss:0.5661
model is saved at epoch 10!![06/0011] | train_loss:0.6355 val_acc:73.0415 val_loss:0.5611
[06/0012] | train_loss:0.6487 val_acc:78.341 val_loss:0.569
model is saved at epoch 12!![06/0013] | train_loss:0.6169 val_acc:77.1889 val_loss:0.5455
[06/0014] | train_loss:0.6088 val_acc:77.1889 val_loss:0.5324
[06/0015] | train_loss:0.6021 val_acc:76.9585 val_loss:0.5276
[06/0016] | train_loss:0.596 val_acc:77.4194 val_loss:0.5309
[06/0017] | train_loss:0.5899 val_acc:77.1889 val_loss:0.5268
[06/0018] | train_loss:0.587 val_acc:76.0369 val_loss:0.5198
[06/0019] | train_loss:0.5816 val_acc:76.4977 val_loss:0.5201
[06/0020] | train_loss:0.5857 val_acc:77.6498 val_loss:0.5344
[06/0021] | train_loss:0.58 val_acc:73.2719 val_loss:0.558
[06/0022] | train_loss:0.5864 val_acc:77.4194 val_loss:0.5132
[06/0023] | train_loss:0.5692 val_acc:77.8802 val_loss:0.519
[06/0024] | train_loss:0.5662 val_acc:76.9585 val_loss:0.4929
[06/0025] | train_loss:0.5727 val_acc:76.0369 val_loss:0.5357
[06/0026] | train_loss:0.5605 val_acc:76.7281 val_loss:0.5078
[06/0027] | train_loss:0.5568 val_acc:77.6498 val_loss:0.506
[06/0028] | train_loss:0.5501 val_acc:79.2627 val_loss:0.497
model is saved at epoch 28!![06/0029] | train_loss:0.5358 val_acc:78.1106 val_loss:0.4895
[06/0030] | train_loss:0.5322 val_acc:78.8018 val_loss:0.4864
[06/0031] | train_loss:0.538 val_acc:79.2627 val_loss:0.494
[06/0032] | train_loss:0.5271 val_acc:77.8802 val_loss:0.4876
[06/0033] | train_loss:0.5247 val_acc:79.0323 val_loss:0.4848
[06/0034] | train_loss:0.5187 val_acc:78.8018 val_loss:0.4939
[06/0035] | train_loss:0.5172 val_acc:78.341 val_loss:0.4962
[06/0036] | train_loss:0.5163 val_acc:79.0323 val_loss:0.4841
[06/0037] | train_loss:0.5049 val_acc:77.8802 val_loss:0.4705
[06/0038] | train_loss:0.5055 val_acc:79.2627 val_loss:0.4779
[06/0039] | train_loss:0.5014 val_acc:79.4931 val_loss:0.467
model is saved at epoch 39!![06/0040] | train_loss:0.4967 val_acc:79.2627 val_loss:0.4765
[06/0041] | train_loss:0.5205 val_acc:79.0323 val_loss:0.4664
[06/0042] | train_loss:0.4956 val_acc:78.8018 val_loss:0.4661
[06/0043] | train_loss:0.4951 val_acc:79.4931 val_loss:0.4656
[06/0044] | train_loss:0.4821 val_acc:78.1106 val_loss:0.4901
[06/0045] | train_loss:0.4805 val_acc:78.5714 val_loss:0.4887
[06/0046] | train_loss:0.483 val_acc:78.341 val_loss:0.4811
[06/0047] | train_loss:0.4751 val_acc:80.1843 val_loss:0.4705
model is saved at epoch 47!![06/0048] | train_loss:0.4964 val_acc:78.8018 val_loss:0.4738
[06/0049] | train_loss:0.4906 val_acc:79.9539 val_loss:0.4758
[06/0050] | train_loss:0.4843 val_acc:79.2627 val_loss:0.4609
[06/0051] | train_loss:0.4667 val_acc:78.8018 val_loss:0.4613
[06/0052] | train_loss:0.4772 val_acc:78.8018 val_loss:0.4667
[06/0053] | train_loss:0.4773 val_acc:77.4194 val_loss:0.4899
[06/0054] | train_loss:0.4613 val_acc:76.7281 val_loss:0.4947
[06/0055] | train_loss:0.4778 val_acc:77.6498 val_loss:0.4835
[06/0056] | train_loss:0.4786 val_acc:79.2627 val_loss:0.4746
[06/0057] | train_loss:0.4689 val_acc:78.8018 val_loss:0.4664
[06/0058] | train_loss:0.4663 val_acc:79.9539 val_loss:0.4627
[06/0059] | train_loss:0.4464 val_acc:79.7235 val_loss:0.4556
[06/0060] | train_loss:0.4586 val_acc:79.4931 val_loss:0.461
[06/0061] | train_loss:0.4481 val_acc:77.4194 val_loss:0.478
[06/0062] | train_loss:0.4414 val_acc:79.7235 val_loss:0.4687
[06/0063] | train_loss:0.4415 val_acc:77.1889 val_loss:0.4768
[06/0064] | train_loss:0.4499 val_acc:79.7235 val_loss:0.4626
[06/0065] | train_loss:0.4374 val_acc:79.4931 val_loss:0.4526
[06/0066] | train_loss:0.4371 val_acc:79.2627 val_loss:0.4637
[06/0067] | train_loss:0.4252 val_acc:78.341 val_loss:0.4738
[06/0068] | train_loss:0.4318 val_acc:78.5714 val_loss:0.4767
[06/0069] | train_loss:0.4302 val_acc:79.2627 val_loss:0.4566
[06/0070] | train_loss:0.4442 val_acc:79.9539 val_loss:0.4487
[06/0071] | train_loss:0.4293 val_acc:81.106 val_loss:0.448
model is saved at epoch 71!![06/0072] | train_loss:0.415 val_acc:81.5668 val_loss:0.4443
model is saved at epoch 72!![06/0073] | train_loss:0.4226 val_acc:81.5668 val_loss:0.4486
[06/0074] | train_loss:0.4193 val_acc:81.106 val_loss:0.4482
[06/0075] | train_loss:0.4182 val_acc:79.9539 val_loss:0.4579
[06/0076] | train_loss:0.4185 val_acc:81.106 val_loss:0.4551
[06/0077] | train_loss:0.4128 val_acc:79.2627 val_loss:0.4812
[06/0078] | train_loss:0.4015 val_acc:79.7235 val_loss:0.4575
[06/0079] | train_loss:0.4049 val_acc:80.4147 val_loss:0.4534
[06/0080] | train_loss:0.3944 val_acc:76.9585 val_loss:0.4908
[06/0081] | train_loss:0.3972 val_acc:79.4931 val_loss:0.4679
[06/0082] | train_loss:0.3937 val_acc:80.4147 val_loss:0.4538
[06/0083] | train_loss:0.3993 val_acc:80.4147 val_loss:0.4486
[06/0084] | train_loss:0.3847 val_acc:80.4147 val_loss:0.4747
[06/0085] | train_loss:0.3841 val_acc:81.5668 val_loss:0.4513
[06/0086] | train_loss:0.3801 val_acc:81.7972 val_loss:0.4529
model is saved at epoch 86!![06/0087] | train_loss:0.3746 val_acc:79.9539 val_loss:0.4756
[06/0088] | train_loss:0.3827 val_acc:79.4931 val_loss:0.4681
[06/0089] | train_loss:0.3655 val_acc:80.8756 val_loss:0.4601
[06/0090] | train_loss:0.3707 val_acc:81.7972 val_loss:0.4318
[06/0091] | train_loss:0.3733 val_acc:80.6452 val_loss:0.478
[06/0092] | train_loss:0.3736 val_acc:80.8756 val_loss:0.469
[06/0093] | train_loss:0.3621 val_acc:81.106 val_loss:0.4544
[06/0094] | train_loss:0.3567 val_acc:78.8018 val_loss:0.4746
[06/0095] | train_loss:0.3691 val_acc:80.8756 val_loss:0.4538
[06/0096] | train_loss:0.3745 val_acc:80.6452 val_loss:0.468
[06/0097] | train_loss:0.3698 val_acc:80.1843 val_loss:0.4876
[06/0098] | train_loss:0.3668 val_acc:78.8018 val_loss:0.481
[06/0099] | train_loss:0.3578 val_acc:80.8756 val_loss:0.4556
[06/0100] | train_loss:0.3551 val_acc:80.6452 val_loss:0.4938
[06/0101] | train_loss:0.3494 val_acc:79.7235 val_loss:0.4883
[06/0102] | train_loss:0.3333 val_acc:81.106 val_loss:0.4662
[06/0103] | train_loss:0.3471 val_acc:80.6452 val_loss:0.4701
[06/0104] | train_loss:0.3296 val_acc:78.341 val_loss:0.5094
[06/0105] | train_loss:0.3315 val_acc:78.8018 val_loss:0.5023
[06/0106] | train_loss:0.3319 val_acc:80.1843 val_loss:0.5251
[06/0107] | train_loss:0.3307 val_acc:80.6452 val_loss:0.475
[06/0108] | train_loss:0.3307 val_acc:79.0323 val_loss:0.4982
[06/0109] | train_loss:0.3447 val_acc:83.1797 val_loss:0.4641
model is saved at epoch 109!![06/0110] | train_loss:0.3239 val_acc:78.5714 val_loss:0.5005
[06/0111] | train_loss:0.3248 val_acc:80.8756 val_loss:0.4602
[06/0112] | train_loss:0.337 val_acc:81.106 val_loss:0.4866
[06/0113] | train_loss:0.3218 val_acc:80.8756 val_loss:0.4642
[06/0114] | train_loss:0.3125 val_acc:82.0276 val_loss:0.5016
[06/0115] | train_loss:0.3161 val_acc:81.3364 val_loss:0.4727
[06/0116] | train_loss:0.3285 val_acc:82.4885 val_loss:0.4557
[06/0117] | train_loss:0.3337 val_acc:82.0276 val_loss:0.4675
[06/0118] | train_loss:0.3293 val_acc:79.0323 val_loss:0.5149
[06/0119] | train_loss:0.2917 val_acc:79.2627 val_loss:0.5515
[06/0120] | train_loss:0.2993 val_acc:81.3364 val_loss:0.4629
[06/0121] | train_loss:0.2867 val_acc:81.106 val_loss:0.5283
[06/0122] | train_loss:0.2859 val_acc:77.4194 val_loss:0.5462
[06/0123] | train_loss:0.2909 val_acc:81.106 val_loss:0.5053
[06/0124] | train_loss:0.2836 val_acc:79.2627 val_loss:0.4899
[06/0125] | train_loss:0.2817 val_acc:81.106 val_loss:0.5044
[06/0126] | train_loss:0.2855 val_acc:80.8756 val_loss:0.5147
[06/0127] | train_loss:0.3032 val_acc:80.1843 val_loss:0.547
[06/0128] | train_loss:0.2932 val_acc:80.6452 val_loss:0.4872
[06/0129] | train_loss:0.2658 val_acc:79.9539 val_loss:0.5372
[06/0130] | train_loss:0.2722 val_acc:82.4885 val_loss:0.4808
[06/0131] | train_loss:0.2644 val_acc:81.106 val_loss:0.5285
[06/0132] | train_loss:0.268 val_acc:79.4931 val_loss:0.5739
[06/0133] | train_loss:0.2903 val_acc:82.0276 val_loss:0.4818
[06/0134] | train_loss:0.2776 val_acc:79.7235 val_loss:0.5047
[06/0135] | train_loss:0.2823 val_acc:76.2673 val_loss:0.5866
[06/0136] | train_loss:0.2777 val_acc:80.1843 val_loss:0.5089
[06/0137] | train_loss:0.2594 val_acc:81.3364 val_loss:0.5617
[06/0138] | train_loss:0.2645 val_acc:81.5668 val_loss:0.5256
[06/0139] | train_loss:0.2509 val_acc:81.106 val_loss:0.5319
[06/0140] | train_loss:0.2522 val_acc:78.341 val_loss:0.5673
[06/0141] | train_loss:0.2541 val_acc:79.7235 val_loss:0.5543
[06/0142] | train_loss:0.2567 val_acc:81.106 val_loss:0.5535
[06/0143] | train_loss:0.2524 val_acc:79.7235 val_loss:0.546
[06/0144] | train_loss:0.2546 val_acc:79.7235 val_loss:0.5687
[06/0145] | train_loss:0.2336 val_acc:81.5668 val_loss:0.5092
[06/0146] | train_loss:0.2426 val_acc:81.3364 val_loss:0.5419
[06/0147] | train_loss:0.2396 val_acc:81.106 val_loss:0.521
[06/0148] | train_loss:0.2482 val_acc:79.0323 val_loss:0.5792
[06/0149] | train_loss:0.2487 val_acc:79.7235 val_loss:0.5885
[06/0150] | train_loss:0.2421 val_acc:79.2627 val_loss:0.5419
[06/0151] | train_loss:0.2493 val_acc:81.5668 val_loss:0.5787
[06/0152] | train_loss:0.2328 val_acc:81.7972 val_loss:0.5721
[06/0153] | train_loss:0.2287 val_acc:80.6452 val_loss:0.5355
[06/0154] | train_loss:0.2313 val_acc:81.7972 val_loss:0.5633
[06/0155] | train_loss:0.2186 val_acc:80.8756 val_loss:0.5821
[06/0156] | train_loss:0.2121 val_acc:82.2581 val_loss:0.5446
[06/0157] | train_loss:0.2085 val_acc:79.7235 val_loss:0.5848
[06/0158] | train_loss:0.2188 val_acc:82.7189 val_loss:0.544
[06/0159] | train_loss:0.2126 val_acc:80.1843 val_loss:0.6016
[06/0160] | train_loss:0.2175 val_acc:83.6406 val_loss:0.5202
model is saved at epoch 160!![06/0161] | train_loss:0.2176 val_acc:82.2581 val_loss:0.6077
[06/0162] | train_loss:0.2049 val_acc:81.106 val_loss:0.5964
[06/0163] | train_loss:0.204 val_acc:81.106 val_loss:0.574
[06/0164] | train_loss:0.1998 val_acc:81.7972 val_loss:0.548
[06/0165] | train_loss:0.1985 val_acc:81.3364 val_loss:0.6345
[06/0166] | train_loss:0.2127 val_acc:79.9539 val_loss:0.5669
[06/0167] | train_loss:0.2548 val_acc:82.4885 val_loss:0.5206
[06/0168] | train_loss:0.243 val_acc:81.106 val_loss:0.5955
[06/0169] | train_loss:0.2174 val_acc:81.3364 val_loss:0.6097
[06/0170] | train_loss:0.2016 val_acc:80.1843 val_loss:0.5887
[06/0171] | train_loss:0.1988 val_acc:80.4147 val_loss:0.598
[06/0172] | train_loss:0.2137 val_acc:82.7189 val_loss:0.5711
[06/0173] | train_loss:0.227 val_acc:79.9539 val_loss:0.63
[06/0174] | train_loss:0.2322 val_acc:79.9539 val_loss:0.6219
[06/0175] | train_loss:0.2294 val_acc:80.1843 val_loss:0.5987
[06/0176] | train_loss:0.2061 val_acc:80.4147 val_loss:0.6441
[06/0177] | train_loss:0.1985 val_acc:81.3364 val_loss:0.5911
[06/0178] | train_loss:0.196 val_acc:81.7972 val_loss:0.642
[06/0179] | train_loss:0.1863 val_acc:80.1843 val_loss:0.6312
[06/0180] | train_loss:0.191 val_acc:79.7235 val_loss:0.5871
[06/0181] | train_loss:0.2157 val_acc:81.106 val_loss:0.5744
[06/0182] | train_loss:0.1977 val_acc:82.2581 val_loss:0.593
[06/0183] | train_loss:0.1881 val_acc:81.5668 val_loss:0.5882
[06/0184] | train_loss:0.1969 val_acc:79.4931 val_loss:0.6854
[06/0185] | train_loss:0.2139 val_acc:80.1843 val_loss:0.599
[06/0186] | train_loss:0.188 val_acc:81.7972 val_loss:0.6135
[06/0187] | train_loss:0.1806 val_acc:80.6452 val_loss:0.6533
[06/0188] | train_loss:0.1903 val_acc:79.4931 val_loss:0.6538
[06/0189] | train_loss:0.2003 val_acc:78.8018 val_loss:0.6863
[06/0190] | train_loss:0.1955 val_acc:81.3364 val_loss:0.6136
[06/0191] | train_loss:0.1699 val_acc:82.4885 val_loss:0.6411
[06/0192] | train_loss:0.1844 val_acc:80.1843 val_loss:0.74
[06/0193] | train_loss:0.1956 val_acc:79.4931 val_loss:0.7049
[06/0194] | train_loss:0.2063 val_acc:79.7235 val_loss:0.6906
[06/0195] | train_loss:0.2038 val_acc:80.4147 val_loss:0.6024
[06/0196] | train_loss:0.2201 val_acc:82.0276 val_loss:0.5584
[06/0197] | train_loss:0.2168 val_acc:80.6452 val_loss:0.6652
[06/0198] | train_loss:0.1893 val_acc:80.4147 val_loss:0.6234
[06/0199] | train_loss:0.179 val_acc:82.0276 val_loss:0.6175
[06/0200] | train_loss:0.1687 val_acc:79.0323 val_loss:0.6384
[06/0201] | train_loss:0.1682 val_acc:82.4885 val_loss:0.6717
[06/0202] | train_loss:0.1675 val_acc:81.5668 val_loss:0.637
[06/0203] | train_loss:0.1677 val_acc:80.1843 val_loss:0.7184
[06/0204] | train_loss:0.1756 val_acc:79.9539 val_loss:0.6803
[06/0205] | train_loss:0.174 val_acc:80.1843 val_loss:0.6668
[06/0206] | train_loss:0.1645 val_acc:80.8756 val_loss:0.7127
[06/0207] | train_loss:0.1873 val_acc:81.5668 val_loss:0.6426
[06/0208] | train_loss:0.1742 val_acc:82.2581 val_loss:0.7434
[06/0209] | train_loss:0.1699 val_acc:80.4147 val_loss:0.6915
[06/0210] | train_loss:0.1858 val_acc:81.5668 val_loss:0.6222
[06/0211] | train_loss:0.1842 val_acc:81.7972 val_loss:0.6554
Fold: [6/10] Test is finish !! 
 Test Metrics are: test_acc:82.9493 test_loss:0.5539fold [6/10] is start!!
[07/0001] | train_loss:0.7572 val_acc:54.1475 val_loss:0.6874
model is saved at epoch 1!![07/0002] | train_loss:0.7568 val_acc:54.1475 val_loss:0.6876
[07/0003] | train_loss:0.75 val_acc:54.1475 val_loss:0.6783
[07/0004] | train_loss:0.7347 val_acc:61.0599 val_loss:0.6606
model is saved at epoch 4!![07/0005] | train_loss:0.709 val_acc:66.8203 val_loss:0.6241
model is saved at epoch 5!![07/0006] | train_loss:0.6854 val_acc:68.2028 val_loss:0.5969
model is saved at epoch 6!![07/0007] | train_loss:0.6626 val_acc:68.6636 val_loss:0.583
model is saved at epoch 7!![07/0008] | train_loss:0.6484 val_acc:69.5853 val_loss:0.5821
model is saved at epoch 8!![07/0009] | train_loss:0.6635 val_acc:70.5069 val_loss:0.5725
model is saved at epoch 9!![07/0010] | train_loss:0.6462 val_acc:68.6636 val_loss:0.5773
[07/0011] | train_loss:0.6292 val_acc:70.2765 val_loss:0.5926
[07/0012] | train_loss:0.6399 val_acc:70.7373 val_loss:0.5812
model is saved at epoch 12!![07/0013] | train_loss:0.6248 val_acc:72.1198 val_loss:0.5494
model is saved at epoch 13!![07/0014] | train_loss:0.6112 val_acc:70.9677 val_loss:0.5494
[07/0015] | train_loss:0.6011 val_acc:73.9631 val_loss:0.5391
model is saved at epoch 15!![07/0016] | train_loss:0.5976 val_acc:73.0415 val_loss:0.5499
[07/0017] | train_loss:0.5894 val_acc:74.6544 val_loss:0.5369
model is saved at epoch 17!![07/0018] | train_loss:0.588 val_acc:71.8894 val_loss:0.5557
[07/0019] | train_loss:0.599 val_acc:73.5023 val_loss:0.5322
[07/0020] | train_loss:0.6225 val_acc:70.9677 val_loss:0.5815
[07/0021] | train_loss:0.6179 val_acc:74.8848 val_loss:0.538
model is saved at epoch 21!![07/0022] | train_loss:0.5916 val_acc:72.1198 val_loss:0.5611
[07/0023] | train_loss:0.5938 val_acc:73.7327 val_loss:0.5372
[07/0024] | train_loss:0.5807 val_acc:75.576 val_loss:0.521
model is saved at epoch 24!![07/0025] | train_loss:0.5722 val_acc:73.9631 val_loss:0.5273
[07/0026] | train_loss:0.5602 val_acc:74.8848 val_loss:0.5199
[07/0027] | train_loss:0.5546 val_acc:75.576 val_loss:0.513
[07/0028] | train_loss:0.5456 val_acc:75.576 val_loss:0.5083
[07/0029] | train_loss:0.5476 val_acc:78.1106 val_loss:0.4931
model is saved at epoch 29!![07/0030] | train_loss:0.5345 val_acc:76.7281 val_loss:0.4842
[07/0031] | train_loss:0.5266 val_acc:76.0369 val_loss:0.4942
[07/0032] | train_loss:0.5249 val_acc:77.4194 val_loss:0.4749
[07/0033] | train_loss:0.5215 val_acc:78.341 val_loss:0.4723
model is saved at epoch 33!![07/0034] | train_loss:0.5196 val_acc:77.8802 val_loss:0.4823
[07/0035] | train_loss:0.5206 val_acc:78.1106 val_loss:0.4819
[07/0036] | train_loss:0.5234 val_acc:75.1152 val_loss:0.5097
[07/0037] | train_loss:0.5252 val_acc:78.5714 val_loss:0.4775
model is saved at epoch 37!![07/0038] | train_loss:0.5219 val_acc:80.8756 val_loss:0.4624
model is saved at epoch 38!![07/0039] | train_loss:0.5063 val_acc:80.8756 val_loss:0.4604
[07/0040] | train_loss:0.5047 val_acc:78.5714 val_loss:0.4778
[07/0041] | train_loss:0.4959 val_acc:79.7235 val_loss:0.4621
[07/0042] | train_loss:0.4929 val_acc:79.2627 val_loss:0.4677
[07/0043] | train_loss:0.4966 val_acc:79.0323 val_loss:0.4531
[07/0044] | train_loss:0.4843 val_acc:79.9539 val_loss:0.4529
[07/0045] | train_loss:0.4811 val_acc:79.9539 val_loss:0.4486
[07/0046] | train_loss:0.4806 val_acc:79.0323 val_loss:0.4723
[07/0047] | train_loss:0.4921 val_acc:80.1843 val_loss:0.4602
[07/0048] | train_loss:0.4897 val_acc:79.7235 val_loss:0.4594
[07/0049] | train_loss:0.4835 val_acc:79.4931 val_loss:0.46
[07/0050] | train_loss:0.4844 val_acc:81.5668 val_loss:0.4443
model is saved at epoch 50!![07/0051] | train_loss:0.485 val_acc:79.9539 val_loss:0.4557
[07/0052] | train_loss:0.4743 val_acc:80.1843 val_loss:0.4594
[07/0053] | train_loss:0.4718 val_acc:79.4931 val_loss:0.4562
[07/0054] | train_loss:0.4671 val_acc:80.8756 val_loss:0.4561
[07/0055] | train_loss:0.4628 val_acc:80.4147 val_loss:0.4428
[07/0056] | train_loss:0.4606 val_acc:80.4147 val_loss:0.4395
[07/0057] | train_loss:0.4624 val_acc:80.1843 val_loss:0.4691
[07/0058] | train_loss:0.469 val_acc:77.8802 val_loss:0.4584
[07/0059] | train_loss:0.4618 val_acc:80.4147 val_loss:0.4673
[07/0060] | train_loss:0.4528 val_acc:79.4931 val_loss:0.4575
[07/0061] | train_loss:0.4482 val_acc:80.8756 val_loss:0.4455
[07/0062] | train_loss:0.4409 val_acc:82.0276 val_loss:0.4429
model is saved at epoch 62!![07/0063] | train_loss:0.4394 val_acc:79.7235 val_loss:0.4503
[07/0064] | train_loss:0.4388 val_acc:79.0323 val_loss:0.4595
[07/0065] | train_loss:0.4482 val_acc:81.3364 val_loss:0.4482
[07/0066] | train_loss:0.4335 val_acc:81.5668 val_loss:0.4356
[07/0067] | train_loss:0.4328 val_acc:80.8756 val_loss:0.4582
[07/0068] | train_loss:0.4442 val_acc:82.4885 val_loss:0.442
model is saved at epoch 68!![07/0069] | train_loss:0.4338 val_acc:81.106 val_loss:0.4464
[07/0070] | train_loss:0.426 val_acc:80.8756 val_loss:0.4549
[07/0071] | train_loss:0.4346 val_acc:80.1843 val_loss:0.4583
[07/0072] | train_loss:0.4276 val_acc:81.3364 val_loss:0.4393
[07/0073] | train_loss:0.4306 val_acc:80.8756 val_loss:0.4302
[07/0074] | train_loss:0.4145 val_acc:81.3364 val_loss:0.454
[07/0075] | train_loss:0.4193 val_acc:81.106 val_loss:0.4368
[07/0076] | train_loss:0.4196 val_acc:81.106 val_loss:0.4299
[07/0077] | train_loss:0.4001 val_acc:82.4885 val_loss:0.4418
[07/0078] | train_loss:0.4089 val_acc:79.2627 val_loss:0.4632
[07/0079] | train_loss:0.4026 val_acc:83.1797 val_loss:0.4209
model is saved at epoch 79!![07/0080] | train_loss:0.4068 val_acc:81.7972 val_loss:0.4289
[07/0081] | train_loss:0.4093 val_acc:82.0276 val_loss:0.434
[07/0082] | train_loss:0.4101 val_acc:82.4885 val_loss:0.461
[07/0083] | train_loss:0.4094 val_acc:82.0276 val_loss:0.4245
[07/0084] | train_loss:0.3975 val_acc:82.0276 val_loss:0.4485
[07/0085] | train_loss:0.4033 val_acc:81.5668 val_loss:0.4362
[07/0086] | train_loss:0.4009 val_acc:76.2673 val_loss:0.4926
[07/0087] | train_loss:0.4055 val_acc:77.8802 val_loss:0.4972
[07/0088] | train_loss:0.4059 val_acc:82.0276 val_loss:0.4299
[07/0089] | train_loss:0.3786 val_acc:82.2581 val_loss:0.4376
[07/0090] | train_loss:0.3816 val_acc:81.7972 val_loss:0.4324
[07/0091] | train_loss:0.3831 val_acc:80.6452 val_loss:0.449
[07/0092] | train_loss:0.3689 val_acc:79.9539 val_loss:0.4607
[07/0093] | train_loss:0.3725 val_acc:81.5668 val_loss:0.4427
[07/0094] | train_loss:0.3655 val_acc:81.7972 val_loss:0.4362
[07/0095] | train_loss:0.3672 val_acc:81.3364 val_loss:0.4537
[07/0096] | train_loss:0.3646 val_acc:82.2581 val_loss:0.4606
[07/0097] | train_loss:0.3689 val_acc:80.1843 val_loss:0.4383
[07/0098] | train_loss:0.3646 val_acc:82.0276 val_loss:0.4382
[07/0099] | train_loss:0.3522 val_acc:80.6452 val_loss:0.4545
[07/0100] | train_loss:0.3606 val_acc:82.0276 val_loss:0.4402
[07/0101] | train_loss:0.357 val_acc:81.7972 val_loss:0.4561
[07/0102] | train_loss:0.3466 val_acc:81.106 val_loss:0.4761
[07/0103] | train_loss:0.3517 val_acc:81.7972 val_loss:0.4637
[07/0104] | train_loss:0.3561 val_acc:82.7189 val_loss:0.4474
[07/0105] | train_loss:0.3547 val_acc:82.2581 val_loss:0.4395
[07/0106] | train_loss:0.3503 val_acc:80.6452 val_loss:0.4624
[07/0107] | train_loss:0.3313 val_acc:81.7972 val_loss:0.4449
[07/0108] | train_loss:0.3284 val_acc:80.8756 val_loss:0.4673
[07/0109] | train_loss:0.3316 val_acc:80.6452 val_loss:0.4949
[07/0110] | train_loss:0.3312 val_acc:81.7972 val_loss:0.44
[07/0111] | train_loss:0.332 val_acc:82.0276 val_loss:0.4761
[07/0112] | train_loss:0.3287 val_acc:79.9539 val_loss:0.5003
[07/0113] | train_loss:0.3139 val_acc:83.1797 val_loss:0.4597
[07/0114] | train_loss:0.3045 val_acc:81.106 val_loss:0.5139
[07/0115] | train_loss:0.3174 val_acc:79.0323 val_loss:0.4823
[07/0116] | train_loss:0.3379 val_acc:80.8756 val_loss:0.5063
[07/0117] | train_loss:0.3375 val_acc:82.2581 val_loss:0.4535
[07/0118] | train_loss:0.3292 val_acc:82.2581 val_loss:0.468
[07/0119] | train_loss:0.346 val_acc:80.4147 val_loss:0.4908
[07/0120] | train_loss:0.3273 val_acc:78.5714 val_loss:0.4951
[07/0121] | train_loss:0.3201 val_acc:81.106 val_loss:0.4684
[07/0122] | train_loss:0.315 val_acc:81.5668 val_loss:0.488
[07/0123] | train_loss:0.2983 val_acc:81.5668 val_loss:0.5034
[07/0124] | train_loss:0.2975 val_acc:82.0276 val_loss:0.5356
[07/0125] | train_loss:0.3007 val_acc:80.6452 val_loss:0.5122
[07/0126] | train_loss:0.2965 val_acc:82.2581 val_loss:0.4772
[07/0127] | train_loss:0.2797 val_acc:80.4147 val_loss:0.5486
[07/0128] | train_loss:0.2832 val_acc:83.1797 val_loss:0.4809
[07/0129] | train_loss:0.2712 val_acc:81.7972 val_loss:0.5072
[07/0130] | train_loss:0.274 val_acc:79.7235 val_loss:0.5555
Fold: [7/10] Test is finish !! 
 Test Metrics are: test_acc:81.3364 test_loss:0.494fold [7/10] is start!!
[08/0001] | train_loss:0.7603 val_acc:56.9124 val_loss:0.6844
model is saved at epoch 1!![08/0002] | train_loss:0.7556 val_acc:56.9124 val_loss:0.6802
[08/0003] | train_loss:0.7449 val_acc:56.9124 val_loss:0.67
[08/0004] | train_loss:0.7333 val_acc:58.2949 val_loss:0.6633
model is saved at epoch 4!![08/0005] | train_loss:0.7086 val_acc:64.2857 val_loss:0.6366
model is saved at epoch 5!![08/0006] | train_loss:0.6782 val_acc:64.977 val_loss:0.6131
model is saved at epoch 6!![08/0007] | train_loss:0.6552 val_acc:68.4332 val_loss:0.5932
model is saved at epoch 7!![08/0008] | train_loss:0.6551 val_acc:68.6636 val_loss:0.5936
model is saved at epoch 8!![08/0009] | train_loss:0.6412 val_acc:71.4286 val_loss:0.5852
model is saved at epoch 9!![08/0010] | train_loss:0.6352 val_acc:68.894 val_loss:0.589
[08/0011] | train_loss:0.6378 val_acc:71.1982 val_loss:0.5641
[08/0012] | train_loss:0.631 val_acc:65.8986 val_loss:0.608
[08/0013] | train_loss:0.642 val_acc:69.3548 val_loss:0.5821
[08/0014] | train_loss:0.6086 val_acc:71.8894 val_loss:0.5776
model is saved at epoch 14!![08/0015] | train_loss:0.6122 val_acc:72.3502 val_loss:0.5582
model is saved at epoch 15!![08/0016] | train_loss:0.6128 val_acc:71.4286 val_loss:0.5554
[08/0017] | train_loss:0.6149 val_acc:73.2719 val_loss:0.5567
model is saved at epoch 17!![08/0018] | train_loss:0.6002 val_acc:71.1982 val_loss:0.5643
[08/0019] | train_loss:0.5959 val_acc:73.7327 val_loss:0.5363
model is saved at epoch 19!![08/0020] | train_loss:0.5907 val_acc:72.8111 val_loss:0.54
[08/0021] | train_loss:0.5837 val_acc:73.5023 val_loss:0.5355
[08/0022] | train_loss:0.5747 val_acc:74.1935 val_loss:0.5313
model is saved at epoch 22!![08/0023] | train_loss:0.5708 val_acc:73.9631 val_loss:0.5325
[08/0024] | train_loss:0.5645 val_acc:74.1935 val_loss:0.5252
[08/0025] | train_loss:0.5716 val_acc:73.5023 val_loss:0.5303
[08/0026] | train_loss:0.5643 val_acc:76.7281 val_loss:0.5214
model is saved at epoch 26!![08/0027] | train_loss:0.5609 val_acc:73.9631 val_loss:0.5148
[08/0028] | train_loss:0.5597 val_acc:74.1935 val_loss:0.5148
[08/0029] | train_loss:0.5506 val_acc:76.7281 val_loss:0.5162
[08/0030] | train_loss:0.5535 val_acc:74.1935 val_loss:0.55
[08/0031] | train_loss:0.5652 val_acc:75.8065 val_loss:0.551
[08/0032] | train_loss:0.563 val_acc:76.4977 val_loss:0.5088
[08/0033] | train_loss:0.5533 val_acc:76.7281 val_loss:0.5234
[08/0034] | train_loss:0.5481 val_acc:76.9585 val_loss:0.517
model is saved at epoch 34!![08/0035] | train_loss:0.5402 val_acc:77.6498 val_loss:0.5
model is saved at epoch 35!![08/0036] | train_loss:0.5287 val_acc:74.1935 val_loss:0.5191
[08/0037] | train_loss:0.5381 val_acc:76.2673 val_loss:0.5015
[08/0038] | train_loss:0.5216 val_acc:77.4194 val_loss:0.503
[08/0039] | train_loss:0.525 val_acc:78.1106 val_loss:0.4922
model is saved at epoch 39!![08/0040] | train_loss:0.5231 val_acc:77.8802 val_loss:0.4992
[08/0041] | train_loss:0.5196 val_acc:76.0369 val_loss:0.5258
[08/0042] | train_loss:0.5308 val_acc:79.0323 val_loss:0.5052
model is saved at epoch 42!![08/0043] | train_loss:0.5136 val_acc:78.1106 val_loss:0.5109
[08/0044] | train_loss:0.5176 val_acc:75.8065 val_loss:0.5052
[08/0045] | train_loss:0.5151 val_acc:77.6498 val_loss:0.4926
[08/0046] | train_loss:0.5247 val_acc:79.2627 val_loss:0.4869
model is saved at epoch 46!![08/0047] | train_loss:0.5031 val_acc:77.8802 val_loss:0.5075
[08/0048] | train_loss:0.5047 val_acc:78.8018 val_loss:0.5141
[08/0049] | train_loss:0.4963 val_acc:79.0323 val_loss:0.502
[08/0050] | train_loss:0.4918 val_acc:78.5714 val_loss:0.492
[08/0051] | train_loss:0.4924 val_acc:79.2627 val_loss:0.4912
[08/0052] | train_loss:0.4786 val_acc:78.5714 val_loss:0.494
[08/0053] | train_loss:0.4826 val_acc:77.4194 val_loss:0.5042
[08/0054] | train_loss:0.4746 val_acc:78.8018 val_loss:0.4976
[08/0055] | train_loss:0.4838 val_acc:78.5714 val_loss:0.4912
[08/0056] | train_loss:0.4711 val_acc:79.2627 val_loss:0.4954
[08/0057] | train_loss:0.4677 val_acc:78.1106 val_loss:0.4984
[08/0058] | train_loss:0.4623 val_acc:79.0323 val_loss:0.4958
[08/0059] | train_loss:0.4747 val_acc:78.1106 val_loss:0.4883
[08/0060] | train_loss:0.4713 val_acc:79.2627 val_loss:0.4958
[08/0061] | train_loss:0.4543 val_acc:79.2627 val_loss:0.5021
[08/0062] | train_loss:0.4533 val_acc:79.0323 val_loss:0.503
[08/0063] | train_loss:0.456 val_acc:79.0323 val_loss:0.5143
[08/0064] | train_loss:0.4538 val_acc:78.1106 val_loss:0.4979
[08/0065] | train_loss:0.4599 val_acc:77.8802 val_loss:0.522
[08/0066] | train_loss:0.4581 val_acc:76.9585 val_loss:0.56
[08/0067] | train_loss:0.4664 val_acc:77.1889 val_loss:0.528
[08/0068] | train_loss:0.4702 val_acc:78.5714 val_loss:0.5223
[08/0069] | train_loss:0.4636 val_acc:76.4977 val_loss:0.5226
[08/0070] | train_loss:0.462 val_acc:78.341 val_loss:0.5027
[08/0071] | train_loss:0.4478 val_acc:79.2627 val_loss:0.4954
[08/0072] | train_loss:0.4421 val_acc:78.8018 val_loss:0.5149
[08/0073] | train_loss:0.4332 val_acc:77.8802 val_loss:0.5255
[08/0074] | train_loss:0.4527 val_acc:78.8018 val_loss:0.5014
[08/0075] | train_loss:0.4382 val_acc:79.0323 val_loss:0.5044
[08/0076] | train_loss:0.4315 val_acc:78.1106 val_loss:0.5436
[08/0077] | train_loss:0.4339 val_acc:77.1889 val_loss:0.5034
[08/0078] | train_loss:0.4232 val_acc:78.5714 val_loss:0.5128
[08/0079] | train_loss:0.4185 val_acc:78.5714 val_loss:0.5026
[08/0080] | train_loss:0.4203 val_acc:78.341 val_loss:0.5199
[08/0081] | train_loss:0.4274 val_acc:79.2627 val_loss:0.4976
[08/0082] | train_loss:0.4232 val_acc:77.1889 val_loss:0.5135
[08/0083] | train_loss:0.4068 val_acc:78.341 val_loss:0.5179
[08/0084] | train_loss:0.3976 val_acc:77.6498 val_loss:0.5218
[08/0085] | train_loss:0.4013 val_acc:77.8802 val_loss:0.537
[08/0086] | train_loss:0.4209 val_acc:79.7235 val_loss:0.5212
model is saved at epoch 86!![08/0087] | train_loss:0.3974 val_acc:79.2627 val_loss:0.5241
[08/0088] | train_loss:0.4053 val_acc:77.6498 val_loss:0.5164
[08/0089] | train_loss:0.4011 val_acc:78.5714 val_loss:0.5086
[08/0090] | train_loss:0.3874 val_acc:76.9585 val_loss:0.5286
[08/0091] | train_loss:0.3936 val_acc:78.341 val_loss:0.5072
[08/0092] | train_loss:0.382 val_acc:79.4931 val_loss:0.533
[08/0093] | train_loss:0.3826 val_acc:77.6498 val_loss:0.5001
[08/0094] | train_loss:0.3772 val_acc:78.341 val_loss:0.5336
[08/0095] | train_loss:0.3743 val_acc:79.2627 val_loss:0.5124
[08/0096] | train_loss:0.3699 val_acc:79.4931 val_loss:0.5188
[08/0097] | train_loss:0.3719 val_acc:76.9585 val_loss:0.526
[08/0098] | train_loss:0.3765 val_acc:78.8018 val_loss:0.5384
[08/0099] | train_loss:0.3752 val_acc:79.0323 val_loss:0.5163
[08/0100] | train_loss:0.3666 val_acc:77.1889 val_loss:0.5267
[08/0101] | train_loss:0.3684 val_acc:77.4194 val_loss:0.5343
[08/0102] | train_loss:0.3724 val_acc:77.8802 val_loss:0.5308
[08/0103] | train_loss:0.3688 val_acc:79.0323 val_loss:0.5573
[08/0104] | train_loss:0.3663 val_acc:76.4977 val_loss:0.537
[08/0105] | train_loss:0.372 val_acc:79.2627 val_loss:0.5274
[08/0106] | train_loss:0.3761 val_acc:74.424 val_loss:0.5751
[08/0107] | train_loss:0.3804 val_acc:76.7281 val_loss:0.5422
[08/0108] | train_loss:0.3575 val_acc:78.5714 val_loss:0.5403
[08/0109] | train_loss:0.3688 val_acc:74.8848 val_loss:0.5688
[08/0110] | train_loss:0.3628 val_acc:79.0323 val_loss:0.5427
[08/0111] | train_loss:0.3484 val_acc:79.4931 val_loss:0.574
[08/0112] | train_loss:0.3452 val_acc:79.0323 val_loss:0.5578
[08/0113] | train_loss:0.3752 val_acc:79.0323 val_loss:0.5496
[08/0114] | train_loss:0.3761 val_acc:77.4194 val_loss:0.5551
[08/0115] | train_loss:0.3446 val_acc:77.8802 val_loss:0.5541
[08/0116] | train_loss:0.3329 val_acc:78.341 val_loss:0.6167
[08/0117] | train_loss:0.3516 val_acc:78.8018 val_loss:0.5595
[08/0118] | train_loss:0.3492 val_acc:77.8802 val_loss:0.5823
[08/0119] | train_loss:0.3456 val_acc:79.2627 val_loss:0.5438
[08/0120] | train_loss:0.3336 val_acc:78.5714 val_loss:0.6
[08/0121] | train_loss:0.3311 val_acc:79.2627 val_loss:0.558
[08/0122] | train_loss:0.3155 val_acc:76.2673 val_loss:0.5932
[08/0123] | train_loss:0.3172 val_acc:77.1889 val_loss:0.5877
[08/0124] | train_loss:0.334 val_acc:78.341 val_loss:0.6128
[08/0125] | train_loss:0.3209 val_acc:78.341 val_loss:0.5799
[08/0126] | train_loss:0.3043 val_acc:78.341 val_loss:0.5696
[08/0127] | train_loss:0.3055 val_acc:79.9539 val_loss:0.5861
model is saved at epoch 127!![08/0128] | train_loss:0.307 val_acc:78.341 val_loss:0.6321
[08/0129] | train_loss:0.3063 val_acc:78.5714 val_loss:0.5848
[08/0130] | train_loss:0.2948 val_acc:79.2627 val_loss:0.5727
[08/0131] | train_loss:0.2913 val_acc:78.5714 val_loss:0.5852
[08/0132] | train_loss:0.2943 val_acc:77.1889 val_loss:0.6185
[08/0133] | train_loss:0.3141 val_acc:74.1935 val_loss:0.6798
[08/0134] | train_loss:0.3282 val_acc:75.8065 val_loss:0.6102
[08/0135] | train_loss:0.3071 val_acc:76.9585 val_loss:0.5963
[08/0136] | train_loss:0.3041 val_acc:77.6498 val_loss:0.6713
[08/0137] | train_loss:0.322 val_acc:77.8802 val_loss:0.6067
[08/0138] | train_loss:0.3172 val_acc:79.9539 val_loss:0.6039
[08/0139] | train_loss:0.2946 val_acc:78.341 val_loss:0.6682
[08/0140] | train_loss:0.3092 val_acc:78.8018 val_loss:0.6311
[08/0141] | train_loss:0.2918 val_acc:77.6498 val_loss:0.6137
[08/0142] | train_loss:0.2866 val_acc:78.5714 val_loss:0.6501
[08/0143] | train_loss:0.289 val_acc:77.4194 val_loss:0.6313
[08/0144] | train_loss:0.2841 val_acc:79.4931 val_loss:0.6273
[08/0145] | train_loss:0.2789 val_acc:76.9585 val_loss:0.6299
[08/0146] | train_loss:0.2834 val_acc:74.424 val_loss:0.7179
[08/0147] | train_loss:0.2949 val_acc:78.5714 val_loss:0.6578
[08/0148] | train_loss:0.2973 val_acc:77.4194 val_loss:0.6356
[08/0149] | train_loss:0.2863 val_acc:80.1843 val_loss:0.6522
model is saved at epoch 149!![08/0150] | train_loss:0.2587 val_acc:78.8018 val_loss:0.6385
[08/0151] | train_loss:0.2445 val_acc:77.6498 val_loss:0.7057
[08/0152] | train_loss:0.2577 val_acc:77.6498 val_loss:0.6575
[08/0153] | train_loss:0.2562 val_acc:78.1106 val_loss:0.6939
[08/0154] | train_loss:0.26 val_acc:79.0323 val_loss:0.6913
[08/0155] | train_loss:0.2651 val_acc:79.9539 val_loss:0.6706
[08/0156] | train_loss:0.2777 val_acc:79.4931 val_loss:0.679
[08/0157] | train_loss:0.2567 val_acc:79.0323 val_loss:0.6842
[08/0158] | train_loss:0.2537 val_acc:78.8018 val_loss:0.6636
[08/0159] | train_loss:0.234 val_acc:77.8802 val_loss:0.7574
[08/0160] | train_loss:0.2481 val_acc:79.7235 val_loss:0.7106
[08/0161] | train_loss:0.2629 val_acc:79.7235 val_loss:0.6849
[08/0162] | train_loss:0.2556 val_acc:80.4147 val_loss:0.6923
model is saved at epoch 162!![08/0163] | train_loss:0.2333 val_acc:79.0323 val_loss:0.6984
[08/0164] | train_loss:0.2261 val_acc:77.1889 val_loss:0.712
[08/0165] | train_loss:0.2482 val_acc:78.341 val_loss:0.7183
[08/0166] | train_loss:0.2394 val_acc:77.4194 val_loss:0.7003
[08/0167] | train_loss:0.2753 val_acc:78.5714 val_loss:0.6916
[08/0168] | train_loss:0.2618 val_acc:77.6498 val_loss:0.699
[08/0169] | train_loss:0.246 val_acc:78.5714 val_loss:0.721
[08/0170] | train_loss:0.2465 val_acc:78.8018 val_loss:0.7035
[08/0171] | train_loss:0.2279 val_acc:81.106 val_loss:0.7189
model is saved at epoch 171!![08/0172] | train_loss:0.2206 val_acc:77.8802 val_loss:0.8054
[08/0173] | train_loss:0.224 val_acc:79.2627 val_loss:0.759
[08/0174] | train_loss:0.2152 val_acc:76.0369 val_loss:0.804
[08/0175] | train_loss:0.22 val_acc:79.4931 val_loss:0.7453
[08/0176] | train_loss:0.2065 val_acc:77.4194 val_loss:0.767
[08/0177] | train_loss:0.2235 val_acc:79.4931 val_loss:0.7903
[08/0178] | train_loss:0.2162 val_acc:79.4931 val_loss:0.7651
[08/0179] | train_loss:0.2176 val_acc:77.6498 val_loss:0.7512
[08/0180] | train_loss:0.209 val_acc:76.9585 val_loss:0.8324
[08/0181] | train_loss:0.2205 val_acc:78.8018 val_loss:0.7711
[08/0182] | train_loss:0.2037 val_acc:78.8018 val_loss:0.7466
[08/0183] | train_loss:0.2003 val_acc:79.2627 val_loss:0.8107
[08/0184] | train_loss:0.1996 val_acc:77.8802 val_loss:0.7496
[08/0185] | train_loss:0.2352 val_acc:78.1106 val_loss:0.7428
[08/0186] | train_loss:0.2297 val_acc:79.4931 val_loss:0.7664
[08/0187] | train_loss:0.2065 val_acc:76.0369 val_loss:0.815
[08/0188] | train_loss:0.2222 val_acc:75.576 val_loss:0.8372
[08/0189] | train_loss:0.2158 val_acc:78.341 val_loss:0.7407
[08/0190] | train_loss:0.2163 val_acc:78.341 val_loss:0.8008
[08/0191] | train_loss:0.2123 val_acc:78.1106 val_loss:0.7742
[08/0192] | train_loss:0.2198 val_acc:76.9585 val_loss:0.8241
[08/0193] | train_loss:0.2332 val_acc:79.7235 val_loss:0.8717
[08/0194] | train_loss:0.2133 val_acc:78.341 val_loss:0.8613
[08/0195] | train_loss:0.2227 val_acc:77.4194 val_loss:0.7955
[08/0196] | train_loss:0.2085 val_acc:78.8018 val_loss:0.8528
[08/0197] | train_loss:0.2148 val_acc:77.8802 val_loss:0.866
[08/0198] | train_loss:0.2087 val_acc:80.4147 val_loss:0.7755
[08/0199] | train_loss:0.2018 val_acc:79.7235 val_loss:0.8336
[08/0200] | train_loss:0.19 val_acc:79.9539 val_loss:0.8522
[08/0201] | train_loss:0.1772 val_acc:79.0323 val_loss:0.82
[08/0202] | train_loss:0.1788 val_acc:79.2627 val_loss:0.8395
[08/0203] | train_loss:0.1805 val_acc:78.5714 val_loss:0.8469
[08/0204] | train_loss:0.1816 val_acc:78.1106 val_loss:0.8533
[08/0205] | train_loss:0.1961 val_acc:78.341 val_loss:0.8212
[08/0206] | train_loss:0.1912 val_acc:80.4147 val_loss:0.9021
[08/0207] | train_loss:0.1948 val_acc:80.8756 val_loss:0.8753
[08/0208] | train_loss:0.1938 val_acc:79.9539 val_loss:0.89
[08/0209] | train_loss:0.1787 val_acc:80.6452 val_loss:0.8458
[08/0210] | train_loss:0.1782 val_acc:79.4931 val_loss:0.9585
[08/0211] | train_loss:0.1777 val_acc:79.0323 val_loss:0.9137
[08/0212] | train_loss:0.2081 val_acc:79.0323 val_loss:0.8967
[08/0213] | train_loss:0.2228 val_acc:78.8018 val_loss:0.9037
[08/0214] | train_loss:0.231 val_acc:79.4931 val_loss:0.8176
[08/0215] | train_loss:0.2163 val_acc:79.7235 val_loss:0.8512
[08/0216] | train_loss:0.1925 val_acc:78.1106 val_loss:0.8427
[08/0217] | train_loss:0.1733 val_acc:78.8018 val_loss:0.8611
[08/0218] | train_loss:0.1723 val_acc:80.6452 val_loss:0.812
[08/0219] | train_loss:0.1881 val_acc:79.0323 val_loss:0.9615
[08/0220] | train_loss:0.1825 val_acc:78.341 val_loss:0.9195
[08/0221] | train_loss:0.1744 val_acc:78.341 val_loss:0.9288
[08/0222] | train_loss:0.1833 val_acc:79.4931 val_loss:0.9506
Fold: [8/10] Test is finish !! 
 Test Metrics are: test_acc:83.8337 test_loss:0.4936fold [8/10] is start!!
[09/0001] | train_loss:0.7577 val_acc:57.7367 val_loss:0.6775
model is saved at epoch 1!![09/0002] | train_loss:0.749 val_acc:57.7367 val_loss:0.6671
[09/0003] | train_loss:0.7358 val_acc:57.7367 val_loss:0.6523
[09/0004] | train_loss:0.7264 val_acc:57.7367 val_loss:0.6701
[09/0005] | train_loss:0.7347 val_acc:67.2055 val_loss:0.6497
model is saved at epoch 5!![09/0006] | train_loss:0.7141 val_acc:66.9746 val_loss:0.6224
[09/0007] | train_loss:0.6907 val_acc:68.5912 val_loss:0.6152
model is saved at epoch 7!![09/0008] | train_loss:0.6923 val_acc:69.9769 val_loss:0.5871
model is saved at epoch 8!![09/0009] | train_loss:0.6647 val_acc:70.4388 val_loss:0.5894
model is saved at epoch 9!![09/0010] | train_loss:0.6512 val_acc:71.1316 val_loss:0.5672
model is saved at epoch 10!![09/0011] | train_loss:0.6485 val_acc:71.3626 val_loss:0.5683
model is saved at epoch 11!![09/0012] | train_loss:0.6581 val_acc:71.5935 val_loss:0.5896
model is saved at epoch 12!![09/0013] | train_loss:0.6488 val_acc:71.5935 val_loss:0.5543
[09/0014] | train_loss:0.6266 val_acc:72.5173 val_loss:0.5527
model is saved at epoch 14!![09/0015] | train_loss:0.6167 val_acc:73.2102 val_loss:0.5444
model is saved at epoch 15!![09/0016] | train_loss:0.6107 val_acc:72.9792 val_loss:0.5323
[09/0017] | train_loss:0.6133 val_acc:72.7483 val_loss:0.5319
[09/0018] | train_loss:0.6028 val_acc:70.6697 val_loss:0.5653
[09/0019] | train_loss:0.6083 val_acc:74.5958 val_loss:0.5265
model is saved at epoch 19!![09/0020] | train_loss:0.5987 val_acc:73.2102 val_loss:0.5248
[09/0021] | train_loss:0.6152 val_acc:70.9007 val_loss:0.5588
[09/0022] | train_loss:0.5987 val_acc:72.7483 val_loss:0.5294
[09/0023] | train_loss:0.6076 val_acc:74.5958 val_loss:0.5182
[09/0024] | train_loss:0.5992 val_acc:73.6721 val_loss:0.5256
[09/0025] | train_loss:0.5885 val_acc:73.6721 val_loss:0.5155
[09/0026] | train_loss:0.5803 val_acc:75.2887 val_loss:0.5034
model is saved at epoch 26!![09/0027] | train_loss:0.5797 val_acc:74.134 val_loss:0.5126
[09/0028] | train_loss:0.5738 val_acc:74.8268 val_loss:0.4997
[09/0029] | train_loss:0.5823 val_acc:76.4434 val_loss:0.4947
model is saved at epoch 29!![09/0030] | train_loss:0.5686 val_acc:75.0577 val_loss:0.4925
[09/0031] | train_loss:0.5627 val_acc:75.7506 val_loss:0.5015
[09/0032] | train_loss:0.567 val_acc:77.1363 val_loss:0.4806
model is saved at epoch 32!![09/0033] | train_loss:0.5556 val_acc:76.4434 val_loss:0.4915
[09/0034] | train_loss:0.5598 val_acc:77.8291 val_loss:0.4789
model is saved at epoch 34!![09/0035] | train_loss:0.55 val_acc:76.6744 val_loss:0.4749
[09/0036] | train_loss:0.5407 val_acc:78.9838 val_loss:0.4734
model is saved at epoch 36!![09/0037] | train_loss:0.5263 val_acc:78.9838 val_loss:0.4738
[09/0038] | train_loss:0.5325 val_acc:77.8291 val_loss:0.4662
[09/0039] | train_loss:0.5184 val_acc:77.5982 val_loss:0.4643
[09/0040] | train_loss:0.5261 val_acc:78.06 val_loss:0.4575
[09/0041] | train_loss:0.5287 val_acc:78.06 val_loss:0.4651
[09/0042] | train_loss:0.5344 val_acc:77.1363 val_loss:0.4691
[09/0043] | train_loss:0.5207 val_acc:78.5219 val_loss:0.4596
[09/0044] | train_loss:0.5082 val_acc:79.4457 val_loss:0.4591
model is saved at epoch 44!![09/0045] | train_loss:0.5185 val_acc:79.2148 val_loss:0.4675
[09/0046] | train_loss:0.5085 val_acc:78.5219 val_loss:0.4592
[09/0047] | train_loss:0.503 val_acc:78.06 val_loss:0.4644
[09/0048] | train_loss:0.4992 val_acc:77.1363 val_loss:0.4611
[09/0049] | train_loss:0.5049 val_acc:75.9815 val_loss:0.475
[09/0050] | train_loss:0.4973 val_acc:78.7529 val_loss:0.4639
[09/0051] | train_loss:0.488 val_acc:80.1386 val_loss:0.456
model is saved at epoch 51!![09/0052] | train_loss:0.4795 val_acc:78.291 val_loss:0.4467
[09/0053] | train_loss:0.4807 val_acc:78.291 val_loss:0.4526
[09/0054] | train_loss:0.4777 val_acc:77.5982 val_loss:0.4554
[09/0055] | train_loss:0.4989 val_acc:77.5982 val_loss:0.4627
[09/0056] | train_loss:0.4888 val_acc:76.6744 val_loss:0.4774
[09/0057] | train_loss:0.4848 val_acc:78.7529 val_loss:0.4647
[09/0058] | train_loss:0.4816 val_acc:78.06 val_loss:0.4664
[09/0059] | train_loss:0.4719 val_acc:79.9076 val_loss:0.4471
[09/0060] | train_loss:0.4675 val_acc:79.2148 val_loss:0.4484
[09/0061] | train_loss:0.4746 val_acc:76.9053 val_loss:0.4591
[09/0062] | train_loss:0.466 val_acc:75.5196 val_loss:0.4982
[09/0063] | train_loss:0.4842 val_acc:77.1363 val_loss:0.4681
[09/0064] | train_loss:0.4811 val_acc:77.3672 val_loss:0.4479
[09/0065] | train_loss:0.4696 val_acc:78.7529 val_loss:0.4727
[09/0066] | train_loss:0.4736 val_acc:79.2148 val_loss:0.4455
[09/0067] | train_loss:0.4664 val_acc:76.9053 val_loss:0.4635
[09/0068] | train_loss:0.4636 val_acc:78.291 val_loss:0.4561
[09/0069] | train_loss:0.4554 val_acc:79.6767 val_loss:0.4355
[09/0070] | train_loss:0.4455 val_acc:80.1386 val_loss:0.4394
[09/0071] | train_loss:0.4461 val_acc:78.291 val_loss:0.4757
[09/0072] | train_loss:0.4573 val_acc:78.5219 val_loss:0.4651
[09/0073] | train_loss:0.448 val_acc:78.9838 val_loss:0.4599
[09/0074] | train_loss:0.4458 val_acc:81.2933 val_loss:0.4296
model is saved at epoch 74!![09/0075] | train_loss:0.4324 val_acc:80.1386 val_loss:0.4309
[09/0076] | train_loss:0.4349 val_acc:81.2933 val_loss:0.4355
[09/0077] | train_loss:0.4305 val_acc:78.9838 val_loss:0.4344
[09/0078] | train_loss:0.4206 val_acc:79.6767 val_loss:0.4245
[09/0079] | train_loss:0.4302 val_acc:79.2148 val_loss:0.4413
[09/0080] | train_loss:0.4209 val_acc:80.8314 val_loss:0.4333
[09/0081] | train_loss:0.4302 val_acc:81.5242 val_loss:0.435
model is saved at epoch 81!![09/0082] | train_loss:0.4172 val_acc:81.5242 val_loss:0.4315
[09/0083] | train_loss:0.4198 val_acc:81.0624 val_loss:0.4376
[09/0084] | train_loss:0.4324 val_acc:80.1386 val_loss:0.438
[09/0085] | train_loss:0.4287 val_acc:79.6767 val_loss:0.4315
[09/0086] | train_loss:0.4215 val_acc:79.9076 val_loss:0.446
[09/0087] | train_loss:0.4086 val_acc:80.8314 val_loss:0.4421
[09/0088] | train_loss:0.4099 val_acc:80.3695 val_loss:0.4438
[09/0089] | train_loss:0.4022 val_acc:80.8314 val_loss:0.4401
[09/0090] | train_loss:0.401 val_acc:81.7552 val_loss:0.4408
model is saved at epoch 90!![09/0091] | train_loss:0.4054 val_acc:81.0624 val_loss:0.4355
[09/0092] | train_loss:0.4025 val_acc:81.7552 val_loss:0.4563
[09/0093] | train_loss:0.4186 val_acc:80.8314 val_loss:0.4404
[09/0094] | train_loss:0.4014 val_acc:82.2171 val_loss:0.4391
model is saved at epoch 94!![09/0095] | train_loss:0.4113 val_acc:80.1386 val_loss:0.4502
[09/0096] | train_loss:0.4094 val_acc:81.7552 val_loss:0.4306
[09/0097] | train_loss:0.3858 val_acc:80.6005 val_loss:0.4339
[09/0098] | train_loss:0.3796 val_acc:81.9861 val_loss:0.4284
[09/0099] | train_loss:0.378 val_acc:80.8314 val_loss:0.4415
[09/0100] | train_loss:0.3741 val_acc:80.6005 val_loss:0.4387
[09/0101] | train_loss:0.3797 val_acc:81.2933 val_loss:0.4404
[09/0102] | train_loss:0.3806 val_acc:81.9861 val_loss:0.4372
[09/0103] | train_loss:0.3815 val_acc:80.6005 val_loss:0.471
[09/0104] | train_loss:0.3841 val_acc:81.9861 val_loss:0.4252
[09/0105] | train_loss:0.3893 val_acc:82.2171 val_loss:0.4317
[09/0106] | train_loss:0.3912 val_acc:80.3695 val_loss:0.4326
[09/0107] | train_loss:0.3839 val_acc:79.4457 val_loss:0.4317
[09/0108] | train_loss:0.3776 val_acc:81.7552 val_loss:0.4402
[09/0109] | train_loss:0.36 val_acc:81.0624 val_loss:0.4483
[09/0110] | train_loss:0.3699 val_acc:80.6005 val_loss:0.4367
[09/0111] | train_loss:0.3565 val_acc:81.5242 val_loss:0.4474
[09/0112] | train_loss:0.3458 val_acc:81.7552 val_loss:0.4324
[09/0113] | train_loss:0.3487 val_acc:80.8314 val_loss:0.4385
[09/0114] | train_loss:0.3638 val_acc:81.0624 val_loss:0.4404
[09/0115] | train_loss:0.3543 val_acc:81.9861 val_loss:0.4446
[09/0116] | train_loss:0.3411 val_acc:81.7552 val_loss:0.447
[09/0117] | train_loss:0.3363 val_acc:81.2933 val_loss:0.4373
[09/0118] | train_loss:0.3397 val_acc:81.7552 val_loss:0.4281
[09/0119] | train_loss:0.3305 val_acc:82.679 val_loss:0.449
model is saved at epoch 119!![09/0120] | train_loss:0.3412 val_acc:81.9861 val_loss:0.4373
[09/0121] | train_loss:0.3295 val_acc:82.448 val_loss:0.4381
[09/0122] | train_loss:0.3254 val_acc:82.2171 val_loss:0.4504
[09/0123] | train_loss:0.3201 val_acc:81.5242 val_loss:0.4801
[09/0124] | train_loss:0.3267 val_acc:82.448 val_loss:0.4214
[09/0125] | train_loss:0.3136 val_acc:81.7552 val_loss:0.4659
[09/0126] | train_loss:0.3264 val_acc:82.679 val_loss:0.4392
[09/0127] | train_loss:0.3126 val_acc:81.5242 val_loss:0.4485
[09/0128] | train_loss:0.3094 val_acc:81.0624 val_loss:0.462
[09/0129] | train_loss:0.3033 val_acc:83.1409 val_loss:0.4682
model is saved at epoch 129!![09/0130] | train_loss:0.3243 val_acc:83.3718 val_loss:0.4594
model is saved at epoch 130!![09/0131] | train_loss:0.3209 val_acc:84.5266 val_loss:0.468
model is saved at epoch 131!![09/0132] | train_loss:0.306 val_acc:83.3718 val_loss:0.4746
[09/0133] | train_loss:0.2952 val_acc:82.448 val_loss:0.4573
[09/0134] | train_loss:0.3077 val_acc:82.9099 val_loss:0.4542
[09/0135] | train_loss:0.3048 val_acc:81.0624 val_loss:0.505
[09/0136] | train_loss:0.3167 val_acc:80.3695 val_loss:0.4889
[09/0137] | train_loss:0.317 val_acc:83.6028 val_loss:0.4627
[09/0138] | train_loss:0.318 val_acc:82.9099 val_loss:0.4523
[09/0139] | train_loss:0.2957 val_acc:81.2933 val_loss:0.4976
[09/0140] | train_loss:0.2991 val_acc:83.3718 val_loss:0.4663
[09/0141] | train_loss:0.2902 val_acc:81.2933 val_loss:0.5011
[09/0142] | train_loss:0.3 val_acc:81.5242 val_loss:0.4723
[09/0143] | train_loss:0.2804 val_acc:82.9099 val_loss:0.4841
[09/0144] | train_loss:0.2852 val_acc:82.679 val_loss:0.4899
[09/0145] | train_loss:0.2762 val_acc:82.679 val_loss:0.4973
[09/0146] | train_loss:0.2747 val_acc:81.9861 val_loss:0.5116
[09/0147] | train_loss:0.2704 val_acc:81.5242 val_loss:0.4936
[09/0148] | train_loss:0.2737 val_acc:82.679 val_loss:0.501
[09/0149] | train_loss:0.2626 val_acc:81.9861 val_loss:0.4743
[09/0150] | train_loss:0.2597 val_acc:81.9861 val_loss:0.5048
[09/0151] | train_loss:0.2613 val_acc:82.9099 val_loss:0.5028
[09/0152] | train_loss:0.2732 val_acc:81.2933 val_loss:0.5232
[09/0153] | train_loss:0.2659 val_acc:82.448 val_loss:0.5272
[09/0154] | train_loss:0.264 val_acc:82.2171 val_loss:0.5292
[09/0155] | train_loss:0.2551 val_acc:83.1409 val_loss:0.5231
[09/0156] | train_loss:0.2412 val_acc:83.1409 val_loss:0.5095
[09/0157] | train_loss:0.2481 val_acc:83.1409 val_loss:0.5554
[09/0158] | train_loss:0.243 val_acc:82.679 val_loss:0.5295
[09/0159] | train_loss:0.2362 val_acc:83.3718 val_loss:0.5629
[09/0160] | train_loss:0.2315 val_acc:82.448 val_loss:0.5556
[09/0161] | train_loss:0.2574 val_acc:81.0624 val_loss:0.537
[09/0162] | train_loss:0.2431 val_acc:81.5242 val_loss:0.5545
[09/0163] | train_loss:0.2902 val_acc:83.8337 val_loss:0.478
[09/0164] | train_loss:0.2598 val_acc:83.6028 val_loss:0.5246
[09/0165] | train_loss:0.2596 val_acc:82.448 val_loss:0.5378
[09/0166] | train_loss:0.255 val_acc:81.2933 val_loss:0.5407
[09/0167] | train_loss:0.2413 val_acc:82.2171 val_loss:0.5542
[09/0168] | train_loss:0.2526 val_acc:79.9076 val_loss:0.5426
[09/0169] | train_loss:0.2466 val_acc:82.679 val_loss:0.5598
[09/0170] | train_loss:0.2333 val_acc:81.5242 val_loss:0.552
[09/0171] | train_loss:0.2472 val_acc:79.4457 val_loss:0.6004
[09/0172] | train_loss:0.2469 val_acc:82.2171 val_loss:0.5383
[09/0173] | train_loss:0.254 val_acc:80.8314 val_loss:0.5725
[09/0174] | train_loss:0.2431 val_acc:81.2933 val_loss:0.551
[09/0175] | train_loss:0.238 val_acc:81.0624 val_loss:0.5944
[09/0176] | train_loss:0.252 val_acc:81.7552 val_loss:0.5541
[09/0177] | train_loss:0.2352 val_acc:80.8314 val_loss:0.5627
[09/0178] | train_loss:0.2168 val_acc:83.3718 val_loss:0.5888
[09/0179] | train_loss:0.2189 val_acc:82.2171 val_loss:0.6065
[09/0180] | train_loss:0.2353 val_acc:82.2171 val_loss:0.5406
[09/0181] | train_loss:0.2047 val_acc:81.7552 val_loss:0.6231
[09/0182] | train_loss:0.1954 val_acc:82.9099 val_loss:0.5796
Fold: [9/10] Test is finish !! 
 Test Metrics are: test_acc:79.6767 test_loss:0.4769fold [9/10] is start!!
[10/0001] | train_loss:0.7599 val_acc:52.6559 val_loss:0.6893
model is saved at epoch 1!![10/0002] | train_loss:0.7544 val_acc:52.6559 val_loss:0.6838
[10/0003] | train_loss:0.7409 val_acc:59.8152 val_loss:0.6591
model is saved at epoch 3!![10/0004] | train_loss:0.7128 val_acc:65.8199 val_loss:0.6158
model is saved at epoch 4!![10/0005] | train_loss:0.6817 val_acc:69.746 val_loss:0.5908
model is saved at epoch 5!![10/0006] | train_loss:0.6624 val_acc:66.5127 val_loss:0.6076
[10/0007] | train_loss:0.6578 val_acc:65.5889 val_loss:0.6355
[10/0008] | train_loss:0.6549 val_acc:72.5173 val_loss:0.577
model is saved at epoch 8!![10/0009] | train_loss:0.6285 val_acc:73.4411 val_loss:0.552
model is saved at epoch 9!![10/0010] | train_loss:0.6268 val_acc:73.6721 val_loss:0.5558
model is saved at epoch 10!![10/0011] | train_loss:0.6188 val_acc:74.3649 val_loss:0.5443
model is saved at epoch 11!![10/0012] | train_loss:0.6098 val_acc:75.0577 val_loss:0.5383
model is saved at epoch 12!![10/0013] | train_loss:0.6132 val_acc:73.903 val_loss:0.5571
[10/0014] | train_loss:0.603 val_acc:75.7506 val_loss:0.5435
model is saved at epoch 14!![10/0015] | train_loss:0.5937 val_acc:75.2887 val_loss:0.5416
[10/0016] | train_loss:0.5904 val_acc:74.134 val_loss:0.5459
[10/0017] | train_loss:0.5892 val_acc:76.4434 val_loss:0.5369
model is saved at epoch 17!![10/0018] | train_loss:0.5828 val_acc:75.2887 val_loss:0.5398
[10/0019] | train_loss:0.5825 val_acc:74.8268 val_loss:0.5375
[10/0020] | train_loss:0.5764 val_acc:72.2864 val_loss:0.5516
[10/0021] | train_loss:0.5813 val_acc:75.9815 val_loss:0.53
[10/0022] | train_loss:0.5725 val_acc:75.7506 val_loss:0.5273
[10/0023] | train_loss:0.5643 val_acc:76.6744 val_loss:0.5271
model is saved at epoch 23!![10/0024] | train_loss:0.558 val_acc:77.3672 val_loss:0.523
model is saved at epoch 24!![10/0025] | train_loss:0.5482 val_acc:76.9053 val_loss:0.5307
[10/0026] | train_loss:0.5636 val_acc:72.2864 val_loss:0.5484
[10/0027] | train_loss:0.5629 val_acc:77.5982 val_loss:0.5161
model is saved at epoch 27!![10/0028] | train_loss:0.5493 val_acc:77.8291 val_loss:0.5122
model is saved at epoch 28!![10/0029] | train_loss:0.5366 val_acc:75.9815 val_loss:0.515
[10/0030] | train_loss:0.5535 val_acc:76.2125 val_loss:0.5079
[10/0031] | train_loss:0.5436 val_acc:78.5219 val_loss:0.4979
model is saved at epoch 31!![10/0032] | train_loss:0.5387 val_acc:78.9838 val_loss:0.5162
model is saved at epoch 32!![10/0033] | train_loss:0.5257 val_acc:77.8291 val_loss:0.5269
[10/0034] | train_loss:0.5359 val_acc:75.7506 val_loss:0.5294
[10/0035] | train_loss:0.5385 val_acc:75.5196 val_loss:0.5461
[10/0036] | train_loss:0.5325 val_acc:79.2148 val_loss:0.5183
model is saved at epoch 36!![10/0037] | train_loss:0.5341 val_acc:78.5219 val_loss:0.5089
[10/0038] | train_loss:0.5219 val_acc:79.4457 val_loss:0.506
model is saved at epoch 38!![10/0039] | train_loss:0.5087 val_acc:79.4457 val_loss:0.4972
[10/0040] | train_loss:0.5039 val_acc:79.6767 val_loss:0.5102
model is saved at epoch 40!![10/0041] | train_loss:0.4996 val_acc:76.4434 val_loss:0.5131
[10/0042] | train_loss:0.4982 val_acc:78.7529 val_loss:0.5194
[10/0043] | train_loss:0.499 val_acc:79.4457 val_loss:0.488
[10/0044] | train_loss:0.4928 val_acc:78.291 val_loss:0.4877
[10/0045] | train_loss:0.5001 val_acc:79.4457 val_loss:0.4872
[10/0046] | train_loss:0.4837 val_acc:78.9838 val_loss:0.4879
[10/0047] | train_loss:0.4947 val_acc:77.3672 val_loss:0.4894
[10/0048] | train_loss:0.4899 val_acc:79.2148 val_loss:0.4918
[10/0049] | train_loss:0.4983 val_acc:79.9076 val_loss:0.4764
model is saved at epoch 49!![10/0050] | train_loss:0.4776 val_acc:78.9838 val_loss:0.4873
[10/0051] | train_loss:0.4859 val_acc:78.9838 val_loss:0.4815
[10/0052] | train_loss:0.4747 val_acc:79.2148 val_loss:0.5021
[10/0053] | train_loss:0.472 val_acc:78.7529 val_loss:0.4911
[10/0054] | train_loss:0.4665 val_acc:78.7529 val_loss:0.4841
[10/0055] | train_loss:0.4544 val_acc:78.5219 val_loss:0.4877
[10/0056] | train_loss:0.4696 val_acc:77.5982 val_loss:0.4921
[10/0057] | train_loss:0.4578 val_acc:79.4457 val_loss:0.4809
[10/0058] | train_loss:0.456 val_acc:79.2148 val_loss:0.484
[10/0059] | train_loss:0.4504 val_acc:78.7529 val_loss:0.4795
[10/0060] | train_loss:0.4433 val_acc:78.5219 val_loss:0.489
[10/0061] | train_loss:0.4605 val_acc:78.7529 val_loss:0.4849
[10/0062] | train_loss:0.4569 val_acc:78.5219 val_loss:0.4689
[10/0063] | train_loss:0.4439 val_acc:79.6767 val_loss:0.4738
[10/0064] | train_loss:0.4413 val_acc:79.4457 val_loss:0.4691
[10/0065] | train_loss:0.4545 val_acc:80.3695 val_loss:0.4868
model is saved at epoch 65!![10/0066] | train_loss:0.4578 val_acc:77.8291 val_loss:0.4792
[10/0067] | train_loss:0.4446 val_acc:79.9076 val_loss:0.4684
[10/0068] | train_loss:0.4314 val_acc:80.3695 val_loss:0.4867
[10/0069] | train_loss:0.4345 val_acc:79.6767 val_loss:0.4734
[10/0070] | train_loss:0.437 val_acc:75.7506 val_loss:0.4946
[10/0071] | train_loss:0.4379 val_acc:80.3695 val_loss:0.4887
[10/0072] | train_loss:0.4147 val_acc:79.4457 val_loss:0.4588
[10/0073] | train_loss:0.4178 val_acc:80.1386 val_loss:0.4707
[10/0074] | train_loss:0.4154 val_acc:79.4457 val_loss:0.47
[10/0075] | train_loss:0.4121 val_acc:81.5242 val_loss:0.458
model is saved at epoch 75!![10/0076] | train_loss:0.4031 val_acc:80.8314 val_loss:0.478
[10/0077] | train_loss:0.4001 val_acc:80.6005 val_loss:0.4645
[10/0078] | train_loss:0.401 val_acc:81.7552 val_loss:0.4892
model is saved at epoch 78!![10/0079] | train_loss:0.4337 val_acc:82.2171 val_loss:0.4647
model is saved at epoch 79!![10/0080] | train_loss:0.4352 val_acc:80.8314 val_loss:0.4595
[10/0081] | train_loss:0.4347 val_acc:80.3695 val_loss:0.4472
[10/0082] | train_loss:0.4145 val_acc:80.1386 val_loss:0.4881
[10/0083] | train_loss:0.405 val_acc:80.6005 val_loss:0.4753
[10/0084] | train_loss:0.402 val_acc:81.2933 val_loss:0.4704
[10/0085] | train_loss:0.3931 val_acc:81.2933 val_loss:0.4668
[10/0086] | train_loss:0.3912 val_acc:80.1386 val_loss:0.4656
[10/0087] | train_loss:0.3982 val_acc:81.0624 val_loss:0.479
[10/0088] | train_loss:0.3934 val_acc:81.2933 val_loss:0.4562
[10/0089] | train_loss:0.3902 val_acc:82.448 val_loss:0.4633
model is saved at epoch 89!![10/0090] | train_loss:0.3784 val_acc:79.6767 val_loss:0.4865
[10/0091] | train_loss:0.3857 val_acc:81.7552 val_loss:0.4723
[10/0092] | train_loss:0.3659 val_acc:81.0624 val_loss:0.4673
[10/0093] | train_loss:0.381 val_acc:79.9076 val_loss:0.4749
[10/0094] | train_loss:0.3703 val_acc:81.0624 val_loss:0.4967
[10/0095] | train_loss:0.3929 val_acc:78.06 val_loss:0.4843
[10/0096] | train_loss:0.3753 val_acc:82.2171 val_loss:0.4733
[10/0097] | train_loss:0.3653 val_acc:78.9838 val_loss:0.5324
[10/0098] | train_loss:0.3874 val_acc:80.1386 val_loss:0.4619
[10/0099] | train_loss:0.3819 val_acc:80.3695 val_loss:0.4942
[10/0100] | train_loss:0.3673 val_acc:81.2933 val_loss:0.4826
[10/0101] | train_loss:0.3564 val_acc:79.4457 val_loss:0.5146
[10/0102] | train_loss:0.353 val_acc:83.3718 val_loss:0.4928
model is saved at epoch 102!![10/0103] | train_loss:0.3557 val_acc:81.0624 val_loss:0.4795
[10/0104] | train_loss:0.3603 val_acc:79.9076 val_loss:0.4976
[10/0105] | train_loss:0.3476 val_acc:81.5242 val_loss:0.4941
[10/0106] | train_loss:0.3607 val_acc:81.9861 val_loss:0.4983
[10/0107] | train_loss:0.3375 val_acc:80.3695 val_loss:0.5039
[10/0108] | train_loss:0.3393 val_acc:81.9861 val_loss:0.5041
[10/0109] | train_loss:0.3523 val_acc:82.448 val_loss:0.4965
[10/0110] | train_loss:0.3475 val_acc:81.0624 val_loss:0.5078
[10/0111] | train_loss:0.3283 val_acc:80.1386 val_loss:0.5091
[10/0112] | train_loss:0.3321 val_acc:80.3695 val_loss:0.5094
[10/0113] | train_loss:0.3316 val_acc:78.291 val_loss:0.5285
[10/0114] | train_loss:0.3385 val_acc:81.2933 val_loss:0.515
[10/0115] | train_loss:0.3275 val_acc:80.1386 val_loss:0.5231
[10/0116] | train_loss:0.3127 val_acc:81.7552 val_loss:0.5129
[10/0117] | train_loss:0.3351 val_acc:80.8314 val_loss:0.4975
[10/0118] | train_loss:0.3258 val_acc:81.5242 val_loss:0.5061
[10/0119] | train_loss:0.3305 val_acc:79.9076 val_loss:0.5126
[10/0120] | train_loss:0.3398 val_acc:80.8314 val_loss:0.5183
[10/0121] | train_loss:0.3232 val_acc:81.2933 val_loss:0.4978
[10/0122] | train_loss:0.3235 val_acc:79.4457 val_loss:0.5141
[10/0123] | train_loss:0.3097 val_acc:80.3695 val_loss:0.5505
[10/0124] | train_loss:0.3034 val_acc:79.4457 val_loss:0.4976
[10/0125] | train_loss:0.2935 val_acc:78.7529 val_loss:0.5303
[10/0126] | train_loss:0.3011 val_acc:81.2933 val_loss:0.5176
[10/0127] | train_loss:0.2833 val_acc:80.3695 val_loss:0.5456
[10/0128] | train_loss:0.2994 val_acc:77.3672 val_loss:0.583
[10/0129] | train_loss:0.3321 val_acc:82.448 val_loss:0.4827
[10/0130] | train_loss:0.3456 val_acc:77.3672 val_loss:0.5795
[10/0131] | train_loss:0.3252 val_acc:79.9076 val_loss:0.5253
[10/0132] | train_loss:0.3204 val_acc:81.5242 val_loss:0.5166
[10/0133] | train_loss:0.3223 val_acc:78.5219 val_loss:0.5802
[10/0134] | train_loss:0.3353 val_acc:79.6767 val_loss:0.5247
[10/0135] | train_loss:0.3229 val_acc:81.2933 val_loss:0.5161
[10/0136] | train_loss:0.3238 val_acc:81.2933 val_loss:0.5544
[10/0137] | train_loss:0.2927 val_acc:79.2148 val_loss:0.5302
[10/0138] | train_loss:0.2896 val_acc:82.2171 val_loss:0.5371
[10/0139] | train_loss:0.2686 val_acc:80.8314 val_loss:0.5771
[10/0140] | train_loss:0.2673 val_acc:81.7552 val_loss:0.5504
[10/0141] | train_loss:0.2593 val_acc:80.8314 val_loss:0.5611
[10/0142] | train_loss:0.2573 val_acc:80.6005 val_loss:0.5444
[10/0143] | train_loss:0.2763 val_acc:79.9076 val_loss:0.5938
[10/0144] | train_loss:0.2857 val_acc:82.2171 val_loss:0.5381
[10/0145] | train_loss:0.2704 val_acc:80.3695 val_loss:0.5808
[10/0146] | train_loss:0.2838 val_acc:79.4457 val_loss:0.562
[10/0147] | train_loss:0.273 val_acc:79.6767 val_loss:0.5881
[10/0148] | train_loss:0.2896 val_acc:81.0624 val_loss:0.5913
[10/0149] | train_loss:0.2826 val_acc:81.0624 val_loss:0.5247
[10/0150] | train_loss:0.2609 val_acc:79.6767 val_loss:0.5932
[10/0151] | train_loss:0.2556 val_acc:82.2171 val_loss:0.549
[10/0152] | train_loss:0.246 val_acc:80.1386 val_loss:0.5996
[10/0153] | train_loss:0.2586 val_acc:80.6005 val_loss:0.5735
Fold: [10/10] Test is finish !! 
 Test Metrics are: test_acc:84.5266 test_loss:0.4277
all fold acc is: 
[84.10138487815857, 79.72350120544434, 81.56682252883911, 80.4147481918335, 79.26267385482788, 82.94931054115295, 81.3364028930664, 83.83371829986572, 79.67667579650879, 84.52655673027039] 
Test is finish !! 
 Test Metrics are: acc_mean:81.7392 acc_std:1.8867