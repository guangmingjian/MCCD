Dataset: NCI109,
Model Name: MCCD
MCCD(
  (MGL): MixGCNLayers(
    (gcn_layer): ModuleList(
      (0): GCNConv(38, 64)
      (1): GraphSizeNorm()
      (2): BatchNorm(64)
      (3): ReLU()
      (4): Dropout(p=0.0, inplace=False)
      (5): GCNConv(64, 64)
      (6): GraphSizeNorm()
      (7): BatchNorm(64)
      (8): ReLU()
      (9): Dropout(p=0.0, inplace=False)
      (10): GCNConv(64, 64)
      (11): GraphSizeNorm()
      (12): BatchNorm(64)
      (13): ReLU()
      (14): Dropout(p=0.0, inplace=False)
    )
    (graph_conv_layer): ModuleList(
      (0): SAGEConv(38, 64)
      (1): GraphSizeNorm()
      (2): BatchNorm(64)
      (3): ReLU()
      (4): Dropout(p=0.0, inplace=False)
      (5): SAGEConv(64, 64)
      (6): GraphSizeNorm()
      (7): BatchNorm(64)
      (8): ReLU()
      (9): Dropout(p=0.0, inplace=False)
      (10): SAGEConv(64, 64)
      (11): GraphSizeNorm()
      (12): BatchNorm(64)
      (13): ReLU()
      (14): Dropout(p=0.0, inplace=False)
    )
    (gat_layer): ModuleList(
      (0): GATConv(38, 8, heads=8)
      (1): GraphSizeNorm()
      (2): BatchNorm(64)
      (3): ReLU()
      (4): Dropout(p=0.0, inplace=False)
      (5): GATConv(64, 64, heads=1)
      (6): GraphSizeNorm()
      (7): BatchNorm(64)
      (8): ReLU()
      (9): Dropout(p=0.0, inplace=False)
      (10): GATConv(64, 64, heads=1)
      (11): GraphSizeNorm()
      (12): BatchNorm(64)
      (13): ReLU()
      (14): Dropout(p=0.0, inplace=False)
    )
  )
  (EmTran): EmbeddingTransform(
    (k_fc): Linear(in_features=64, out_features=32, bias=True)
    (v_fc): Linear(in_features=64, out_features=32, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
  )
  (cnn_net): LeNet(
    (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))
    (conv2): Conv2d(16, 18, kernel_size=(5, 5), stride=(1, 1))
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (fc1): Linear(in_features=450, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (gfc): Linear(in_features=64, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)

fold [0/10] is start!!
[01/0001] | train_loss:0.672 val_acc:49.2718 val_loss:0.6894
model is saved at epoch 1!![01/0002] | train_loss:0.6304 val_acc:49.2718 val_loss:0.6865
[01/0003] | train_loss:0.6074 val_acc:49.5146 val_loss:0.6976
model is saved at epoch 3!![01/0004] | train_loss:0.5844 val_acc:50.0 val_loss:0.7049
model is saved at epoch 4!![01/0005] | train_loss:0.5543 val_acc:53.3981 val_loss:0.6964
model is saved at epoch 5!![01/0006] | train_loss:0.5368 val_acc:66.0194 val_loss:0.5748
model is saved at epoch 6!![01/0007] | train_loss:0.5194 val_acc:71.3592 val_loss:0.5327
model is saved at epoch 7!![01/0008] | train_loss:0.5025 val_acc:75.4854 val_loss:0.5175
model is saved at epoch 8!![01/0009] | train_loss:0.491 val_acc:73.7864 val_loss:0.5117
[01/0010] | train_loss:0.4783 val_acc:74.2718 val_loss:0.5112
[01/0011] | train_loss:0.4682 val_acc:73.5437 val_loss:0.5047
[01/0012] | train_loss:0.464 val_acc:77.4272 val_loss:0.5067
model is saved at epoch 12!![01/0013] | train_loss:0.4436 val_acc:77.6699 val_loss:0.495
model is saved at epoch 13!![01/0014] | train_loss:0.4338 val_acc:76.4563 val_loss:0.4884
[01/0015] | train_loss:0.4212 val_acc:76.2136 val_loss:0.4925
[01/0016] | train_loss:0.4136 val_acc:76.4563 val_loss:0.4855
[01/0017] | train_loss:0.4041 val_acc:76.2136 val_loss:0.4885
[01/0018] | train_loss:0.3997 val_acc:76.2136 val_loss:0.4871
[01/0019] | train_loss:0.3851 val_acc:79.1262 val_loss:0.4806
model is saved at epoch 19!![01/0020] | train_loss:0.3793 val_acc:79.6117 val_loss:0.4912
model is saved at epoch 20!![01/0021] | train_loss:0.3696 val_acc:78.6408 val_loss:0.4815
[01/0022] | train_loss:0.355 val_acc:78.6408 val_loss:0.483
[01/0023] | train_loss:0.3494 val_acc:77.9126 val_loss:0.493
[01/0024] | train_loss:0.335 val_acc:77.9126 val_loss:0.48
[01/0025] | train_loss:0.3226 val_acc:78.1553 val_loss:0.487
[01/0026] | train_loss:0.3039 val_acc:81.7961 val_loss:0.4865
model is saved at epoch 26!![01/0027] | train_loss:0.297 val_acc:80.5825 val_loss:0.4638
[01/0028] | train_loss:0.2952 val_acc:81.3107 val_loss:0.4745
[01/0029] | train_loss:0.2971 val_acc:79.1262 val_loss:0.5032
[01/0030] | train_loss:0.2975 val_acc:79.3689 val_loss:0.4784
[01/0031] | train_loss:0.2821 val_acc:78.1553 val_loss:0.4887
[01/0032] | train_loss:0.2586 val_acc:80.5825 val_loss:0.5056
[01/0033] | train_loss:0.2679 val_acc:78.8835 val_loss:0.4752
[01/0034] | train_loss:0.2447 val_acc:80.0971 val_loss:0.469
[01/0035] | train_loss:0.2337 val_acc:81.068 val_loss:0.5052
[01/0036] | train_loss:0.2306 val_acc:81.3107 val_loss:0.4988
[01/0037] | train_loss:0.2296 val_acc:79.1262 val_loss:0.4707
[01/0038] | train_loss:0.2232 val_acc:76.4563 val_loss:0.5542
[01/0039] | train_loss:0.2208 val_acc:78.6408 val_loss:0.5181
[01/0040] | train_loss:0.2218 val_acc:76.4563 val_loss:0.5877
[01/0041] | train_loss:0.2041 val_acc:79.1262 val_loss:0.5265
[01/0042] | train_loss:0.1946 val_acc:80.0971 val_loss:0.5259
[01/0043] | train_loss:0.1878 val_acc:77.9126 val_loss:0.5998
[01/0044] | train_loss:0.1972 val_acc:79.3689 val_loss:0.5247
[01/0045] | train_loss:0.1756 val_acc:79.1262 val_loss:0.5737
[01/0046] | train_loss:0.1877 val_acc:79.1262 val_loss:0.5372
[01/0047] | train_loss:0.1682 val_acc:80.5825 val_loss:0.551
[01/0048] | train_loss:0.1517 val_acc:78.3981 val_loss:0.6112
[01/0049] | train_loss:0.144 val_acc:77.4272 val_loss:0.615
[01/0050] | train_loss:0.1527 val_acc:78.6408 val_loss:0.5929
[01/0051] | train_loss:0.1411 val_acc:78.6408 val_loss:0.6376
[01/0052] | train_loss:0.1432 val_acc:77.4272 val_loss:0.5178
[01/0053] | train_loss:0.1302 val_acc:77.9126 val_loss:0.618
[01/0054] | train_loss:0.1489 val_acc:77.1845 val_loss:0.5812
[01/0055] | train_loss:0.1458 val_acc:79.1262 val_loss:0.6303
[01/0056] | train_loss:0.1207 val_acc:79.1262 val_loss:0.6516
[01/0057] | train_loss:0.1416 val_acc:78.3981 val_loss:0.6473
[01/0058] | train_loss:0.1523 val_acc:76.9417 val_loss:0.5917
[01/0059] | train_loss:0.1534 val_acc:77.9126 val_loss:0.6
[01/0060] | train_loss:0.1308 val_acc:79.6117 val_loss:0.6244
[01/0061] | train_loss:0.1365 val_acc:79.3689 val_loss:0.687
[01/0062] | train_loss:0.1357 val_acc:77.6699 val_loss:0.6585
[01/0063] | train_loss:0.1224 val_acc:79.1262 val_loss:0.6351
[01/0064] | train_loss:0.1397 val_acc:79.1262 val_loss:0.6167
[01/0065] | train_loss:0.1175 val_acc:77.9126 val_loss:0.6615
[01/0066] | train_loss:0.1104 val_acc:79.3689 val_loss:0.6528
[01/0067] | train_loss:0.1016 val_acc:78.8835 val_loss:0.6863
[01/0068] | train_loss:0.0924 val_acc:79.8544 val_loss:0.6454
[01/0069] | train_loss:0.1001 val_acc:78.3981 val_loss:0.6695
[01/0070] | train_loss:0.1201 val_acc:78.8835 val_loss:0.5805
[01/0071] | train_loss:0.1119 val_acc:79.3689 val_loss:0.6491
[01/0072] | train_loss:0.134 val_acc:78.1553 val_loss:0.5949
[01/0073] | train_loss:0.1216 val_acc:80.3398 val_loss:0.6229
[01/0074] | train_loss:0.0938 val_acc:78.1553 val_loss:0.6865
[01/0075] | train_loss:0.0839 val_acc:78.3981 val_loss:0.6512
[01/0076] | train_loss:0.0845 val_acc:77.6699 val_loss:0.6951
[01/0077] | train_loss:0.0903 val_acc:76.699 val_loss:0.672
Fold: [1/10] Test is finish !! 
 Test Metrics are: test_acc:73.8499 test_loss:0.5716fold [1/10] is start!!
[02/0001] | train_loss:0.6699 val_acc:44.7942 val_loss:0.6951
model is saved at epoch 1!![02/0002] | train_loss:0.6366 val_acc:44.7942 val_loss:0.7085
[02/0003] | train_loss:0.6081 val_acc:44.7942 val_loss:0.7432
[02/0004] | train_loss:0.5762 val_acc:45.0363 val_loss:0.7378
model is saved at epoch 4!![02/0005] | train_loss:0.5489 val_acc:53.0266 val_loss:0.6838
model is saved at epoch 5!![02/0006] | train_loss:0.5385 val_acc:61.7433 val_loss:0.6225
model is saved at epoch 6!![02/0007] | train_loss:0.5184 val_acc:67.0702 val_loss:0.5864
model is saved at epoch 7!![02/0008] | train_loss:0.498 val_acc:71.6707 val_loss:0.5621
model is saved at epoch 8!![02/0009] | train_loss:0.4932 val_acc:72.3971 val_loss:0.5701
model is saved at epoch 9!![02/0010] | train_loss:0.4655 val_acc:70.4601 val_loss:0.5809
[02/0011] | train_loss:0.4578 val_acc:72.3971 val_loss:0.5563
[02/0012] | train_loss:0.4494 val_acc:73.6077 val_loss:0.5694
model is saved at epoch 12!![02/0013] | train_loss:0.4359 val_acc:72.155 val_loss:0.5688
[02/0014] | train_loss:0.4363 val_acc:73.6077 val_loss:0.5934
[02/0015] | train_loss:0.4123 val_acc:74.8184 val_loss:0.6014
model is saved at epoch 15!![02/0016] | train_loss:0.4084 val_acc:71.4286 val_loss:0.5762
[02/0017] | train_loss:0.3882 val_acc:77.2397 val_loss:0.5565
model is saved at epoch 17!![02/0018] | train_loss:0.3738 val_acc:75.3027 val_loss:0.5769
[02/0019] | train_loss:0.368 val_acc:71.9128 val_loss:0.5762
[02/0020] | train_loss:0.3405 val_acc:75.3027 val_loss:0.5984
[02/0021] | train_loss:0.3584 val_acc:73.8499 val_loss:0.5715
[02/0022] | train_loss:0.3371 val_acc:71.6707 val_loss:0.5843
[02/0023] | train_loss:0.3327 val_acc:74.8184 val_loss:0.6083
[02/0024] | train_loss:0.3221 val_acc:73.6077 val_loss:0.6157
[02/0025] | train_loss:0.3164 val_acc:74.092 val_loss:0.6136
[02/0026] | train_loss:0.294 val_acc:74.5763 val_loss:0.5969
[02/0027] | train_loss:0.2878 val_acc:70.4601 val_loss:0.6283
[02/0028] | train_loss:0.2875 val_acc:70.2179 val_loss:0.6254
[02/0029] | train_loss:0.2701 val_acc:75.3027 val_loss:0.6202
[02/0030] | train_loss:0.2663 val_acc:75.3027 val_loss:0.629
[02/0031] | train_loss:0.273 val_acc:69.9758 val_loss:0.6336
[02/0032] | train_loss:0.2535 val_acc:75.7869 val_loss:0.5962
[02/0033] | train_loss:0.2471 val_acc:75.7869 val_loss:0.626
[02/0034] | train_loss:0.2306 val_acc:78.9346 val_loss:0.6873
model is saved at epoch 34!![02/0035] | train_loss:0.2354 val_acc:76.0291 val_loss:0.6418
[02/0036] | train_loss:0.2293 val_acc:74.5763 val_loss:0.6682
[02/0037] | train_loss:0.22 val_acc:75.0605 val_loss:0.6594
[02/0038] | train_loss:0.2051 val_acc:74.5763 val_loss:0.6669
[02/0039] | train_loss:0.2056 val_acc:77.9661 val_loss:0.6747
[02/0040] | train_loss:0.1987 val_acc:73.3656 val_loss:0.6884
[02/0041] | train_loss:0.1993 val_acc:75.0605 val_loss:0.6577
[02/0042] | train_loss:0.1969 val_acc:74.3341 val_loss:0.7237
[02/0043] | train_loss:0.1987 val_acc:74.3341 val_loss:0.6737
[02/0044] | train_loss:0.1942 val_acc:77.724 val_loss:0.7357
[02/0045] | train_loss:0.1773 val_acc:77.2397 val_loss:0.7215
[02/0046] | train_loss:0.1632 val_acc:77.9661 val_loss:0.7341
[02/0047] | train_loss:0.1752 val_acc:77.724 val_loss:0.6982
[02/0048] | train_loss:0.1648 val_acc:77.2397 val_loss:0.8061
[02/0049] | train_loss:0.1796 val_acc:73.1235 val_loss:0.7417
[02/0050] | train_loss:0.1621 val_acc:76.5133 val_loss:0.7572
[02/0051] | train_loss:0.1418 val_acc:75.0605 val_loss:0.7516
[02/0052] | train_loss:0.1414 val_acc:74.5763 val_loss:0.7639
[02/0053] | train_loss:0.1361 val_acc:73.1235 val_loss:0.81
[02/0054] | train_loss:0.1388 val_acc:76.0291 val_loss:0.7621
[02/0055] | train_loss:0.1274 val_acc:75.0605 val_loss:0.7886
[02/0056] | train_loss:0.1379 val_acc:77.2397 val_loss:0.8052
[02/0057] | train_loss:0.1344 val_acc:75.0605 val_loss:0.8323
[02/0058] | train_loss:0.1358 val_acc:75.0605 val_loss:0.8254
[02/0059] | train_loss:0.1138 val_acc:74.8184 val_loss:0.8317
[02/0060] | train_loss:0.1301 val_acc:77.4818 val_loss:0.8436
[02/0061] | train_loss:0.1192 val_acc:75.3027 val_loss:0.8022
[02/0062] | train_loss:0.13 val_acc:77.2397 val_loss:0.8412
[02/0063] | train_loss:0.1196 val_acc:73.1235 val_loss:0.8838
[02/0064] | train_loss:0.1162 val_acc:75.0605 val_loss:0.8215
[02/0065] | train_loss:0.119 val_acc:79.1768 val_loss:0.858
model is saved at epoch 65!![02/0066] | train_loss:0.1108 val_acc:78.6925 val_loss:0.8234
[02/0067] | train_loss:0.1185 val_acc:76.5133 val_loss:0.8718
[02/0068] | train_loss:0.1094 val_acc:76.5133 val_loss:0.8377
[02/0069] | train_loss:0.0984 val_acc:79.1768 val_loss:0.9034
[02/0070] | train_loss:0.1177 val_acc:77.724 val_loss:0.8451
[02/0071] | train_loss:0.1132 val_acc:75.3027 val_loss:0.814
[02/0072] | train_loss:0.1 val_acc:77.4818 val_loss:0.8533
[02/0073] | train_loss:0.0981 val_acc:76.5133 val_loss:0.8344
[02/0074] | train_loss:0.1017 val_acc:76.0291 val_loss:0.8549
[02/0075] | train_loss:0.101 val_acc:78.2082 val_loss:0.9092
[02/0076] | train_loss:0.1253 val_acc:76.0291 val_loss:0.8281
[02/0077] | train_loss:0.0945 val_acc:79.1768 val_loss:0.9259
[02/0078] | train_loss:0.1134 val_acc:75.5448 val_loss:0.8943
[02/0079] | train_loss:0.1046 val_acc:77.724 val_loss:0.9234
[02/0080] | train_loss:0.0937 val_acc:76.5133 val_loss:0.8573
[02/0081] | train_loss:0.09 val_acc:78.4504 val_loss:1.0082
[02/0082] | train_loss:0.0918 val_acc:76.9976 val_loss:0.9502
[02/0083] | train_loss:0.0986 val_acc:76.0291 val_loss:0.8631
[02/0084] | train_loss:0.0795 val_acc:76.7554 val_loss:0.9726
[02/0085] | train_loss:0.0908 val_acc:76.2712 val_loss:0.8561
[02/0086] | train_loss:0.0829 val_acc:78.4504 val_loss:0.9401
[02/0087] | train_loss:0.0855 val_acc:76.5133 val_loss:0.9685
[02/0088] | train_loss:0.0802 val_acc:77.2397 val_loss:0.9389
[02/0089] | train_loss:0.0819 val_acc:75.0605 val_loss:1.015
[02/0090] | train_loss:0.0758 val_acc:76.5133 val_loss:1.0293
[02/0091] | train_loss:0.082 val_acc:76.5133 val_loss:0.9293
[02/0092] | train_loss:0.0704 val_acc:77.2397 val_loss:1.0112
[02/0093] | train_loss:0.0724 val_acc:77.2397 val_loss:1.0129
[02/0094] | train_loss:0.0877 val_acc:76.5133 val_loss:0.9589
[02/0095] | train_loss:0.0708 val_acc:77.9661 val_loss:1.0619
[02/0096] | train_loss:0.0855 val_acc:77.4818 val_loss:1.0436
[02/0097] | train_loss:0.0902 val_acc:76.0291 val_loss:1.0146
[02/0098] | train_loss:0.0986 val_acc:76.9976 val_loss:0.9867
[02/0099] | train_loss:0.0851 val_acc:77.2397 val_loss:1.0258
[02/0100] | train_loss:0.0637 val_acc:78.6925 val_loss:1.0123
[02/0101] | train_loss:0.0659 val_acc:76.0291 val_loss:0.9852
[02/0102] | train_loss:0.0726 val_acc:75.7869 val_loss:1.0608
[02/0103] | train_loss:0.0616 val_acc:76.5133 val_loss:1.1721
[02/0104] | train_loss:0.0638 val_acc:76.2712 val_loss:1.0723
[02/0105] | train_loss:0.061 val_acc:74.5763 val_loss:1.0466
[02/0106] | train_loss:0.0604 val_acc:79.1768 val_loss:1.1186
[02/0107] | train_loss:0.0652 val_acc:77.4818 val_loss:1.1107
[02/0108] | train_loss:0.0574 val_acc:76.5133 val_loss:1.0477
[02/0109] | train_loss:0.0613 val_acc:76.2712 val_loss:1.0477
[02/0110] | train_loss:0.0599 val_acc:76.0291 val_loss:1.0666
[02/0111] | train_loss:0.0505 val_acc:77.724 val_loss:1.0762
[02/0112] | train_loss:0.0478 val_acc:76.5133 val_loss:1.0601
[02/0113] | train_loss:0.0587 val_acc:73.8499 val_loss:1.1273
[02/0114] | train_loss:0.0705 val_acc:75.7869 val_loss:0.9984
[02/0115] | train_loss:0.0524 val_acc:79.661 val_loss:1.1928
model is saved at epoch 115!![02/0116] | train_loss:0.0711 val_acc:76.5133 val_loss:1.0227
[02/0117] | train_loss:0.0558 val_acc:77.2397 val_loss:1.0723
[02/0118] | train_loss:0.0571 val_acc:80.3874 val_loss:1.0471
model is saved at epoch 118!![02/0119] | train_loss:0.0548 val_acc:77.2397 val_loss:1.065
[02/0120] | train_loss:0.0708 val_acc:76.0291 val_loss:1.0595
[02/0121] | train_loss:0.0474 val_acc:75.5448 val_loss:1.1376
[02/0122] | train_loss:0.0448 val_acc:78.9346 val_loss:1.197
[02/0123] | train_loss:0.081 val_acc:72.8814 val_loss:0.9789
[02/0124] | train_loss:0.0732 val_acc:76.9976 val_loss:1.1236
[02/0125] | train_loss:0.0595 val_acc:77.4818 val_loss:1.0834
[02/0126] | train_loss:0.0684 val_acc:73.3656 val_loss:1.1727
[02/0127] | train_loss:0.0702 val_acc:77.724 val_loss:1.1211
[02/0128] | train_loss:0.0712 val_acc:74.092 val_loss:1.1473
[02/0129] | train_loss:0.0587 val_acc:75.7869 val_loss:1.2054
[02/0130] | train_loss:0.0458 val_acc:74.8184 val_loss:1.1711
[02/0131] | train_loss:0.0418 val_acc:76.2712 val_loss:1.1835
[02/0132] | train_loss:0.0597 val_acc:76.5133 val_loss:1.1195
[02/0133] | train_loss:0.0512 val_acc:77.4818 val_loss:1.1645
[02/0134] | train_loss:0.0461 val_acc:74.8184 val_loss:1.2175
[02/0135] | train_loss:0.0575 val_acc:75.5448 val_loss:1.0961
[02/0136] | train_loss:0.0555 val_acc:78.6925 val_loss:1.188
[02/0137] | train_loss:0.049 val_acc:78.4504 val_loss:1.1551
[02/0138] | train_loss:0.0496 val_acc:76.5133 val_loss:1.2039
[02/0139] | train_loss:0.0554 val_acc:74.8184 val_loss:1.1995
[02/0140] | train_loss:0.0658 val_acc:78.2082 val_loss:1.2433
[02/0141] | train_loss:0.0987 val_acc:77.9661 val_loss:0.9955
[02/0142] | train_loss:0.067 val_acc:77.9661 val_loss:1.0708
[02/0143] | train_loss:0.0512 val_acc:79.661 val_loss:1.1054
[02/0144] | train_loss:0.044 val_acc:76.7554 val_loss:1.1412
[02/0145] | train_loss:0.048 val_acc:76.9976 val_loss:1.2229
[02/0146] | train_loss:0.0479 val_acc:76.7554 val_loss:1.1808
[02/0147] | train_loss:0.0397 val_acc:76.7554 val_loss:1.1357
[02/0148] | train_loss:0.0337 val_acc:75.7869 val_loss:1.1505
[02/0149] | train_loss:0.0366 val_acc:78.6925 val_loss:1.209
[02/0150] | train_loss:0.0458 val_acc:77.4818 val_loss:1.1893
[02/0151] | train_loss:0.0571 val_acc:75.3027 val_loss:1.0822
[02/0152] | train_loss:0.0653 val_acc:75.7869 val_loss:1.1844
[02/0153] | train_loss:0.0672 val_acc:76.5133 val_loss:1.0923
[02/0154] | train_loss:0.0476 val_acc:73.3656 val_loss:1.29
[02/0155] | train_loss:0.0487 val_acc:78.6925 val_loss:1.2267
[02/0156] | train_loss:0.0415 val_acc:78.2082 val_loss:1.2077
[02/0157] | train_loss:0.0406 val_acc:78.2082 val_loss:1.1707
[02/0158] | train_loss:0.05 val_acc:77.2397 val_loss:1.1055
[02/0159] | train_loss:0.0389 val_acc:77.9661 val_loss:1.205
[02/0160] | train_loss:0.035 val_acc:77.4818 val_loss:1.2722
[02/0161] | train_loss:0.0477 val_acc:74.8184 val_loss:1.1912
[02/0162] | train_loss:0.0403 val_acc:73.6077 val_loss:1.2172
[02/0163] | train_loss:0.0324 val_acc:77.2397 val_loss:1.2813
[02/0164] | train_loss:0.0504 val_acc:76.9976 val_loss:1.232
[02/0165] | train_loss:0.0565 val_acc:76.5133 val_loss:1.196
[02/0166] | train_loss:0.0567 val_acc:78.9346 val_loss:1.1634
[02/0167] | train_loss:0.0411 val_acc:77.4818 val_loss:1.2648
[02/0168] | train_loss:0.0286 val_acc:75.5448 val_loss:1.3179
[02/0169] | train_loss:0.0371 val_acc:76.9976 val_loss:1.2995
Fold: [2/10] Test is finish !! 
 Test Metrics are: test_acc:79.661 test_loss:1.0191fold [2/10] is start!!
[03/0001] | train_loss:0.6734 val_acc:51.5738 val_loss:0.6885
model is saved at epoch 1!![03/0002] | train_loss:0.6321 val_acc:51.5738 val_loss:0.6836
[03/0003] | train_loss:0.6018 val_acc:51.5738 val_loss:0.6864
[03/0004] | train_loss:0.5707 val_acc:55.9322 val_loss:0.6699
model is saved at epoch 4!![03/0005] | train_loss:0.5435 val_acc:60.2906 val_loss:0.662
model is saved at epoch 5!![03/0006] | train_loss:0.5295 val_acc:67.0702 val_loss:0.6205
model is saved at epoch 6!![03/0007] | train_loss:0.5021 val_acc:71.4286 val_loss:0.5745
model is saved at epoch 7!![03/0008] | train_loss:0.4829 val_acc:69.4915 val_loss:0.6044
[03/0009] | train_loss:0.4761 val_acc:71.6707 val_loss:0.5835
model is saved at epoch 9!![03/0010] | train_loss:0.4585 val_acc:72.155 val_loss:0.5445
model is saved at epoch 10!![03/0011] | train_loss:0.4451 val_acc:72.3971 val_loss:0.5461
model is saved at epoch 11!![03/0012] | train_loss:0.4361 val_acc:71.9128 val_loss:0.5771
[03/0013] | train_loss:0.4202 val_acc:74.3341 val_loss:0.5512
model is saved at epoch 13!![03/0014] | train_loss:0.4162 val_acc:74.5763 val_loss:0.5449
model is saved at epoch 14!![03/0015] | train_loss:0.3954 val_acc:75.3027 val_loss:0.5292
model is saved at epoch 15!![03/0016] | train_loss:0.3908 val_acc:72.8814 val_loss:0.5295
[03/0017] | train_loss:0.3727 val_acc:73.6077 val_loss:0.5497
[03/0018] | train_loss:0.3568 val_acc:75.0605 val_loss:0.5324
[03/0019] | train_loss:0.341 val_acc:74.3341 val_loss:0.5792
[03/0020] | train_loss:0.3408 val_acc:75.0605 val_loss:0.5403
[03/0021] | train_loss:0.3428 val_acc:77.724 val_loss:0.5332
model is saved at epoch 21!![03/0022] | train_loss:0.3137 val_acc:75.0605 val_loss:0.553
[03/0023] | train_loss:0.3068 val_acc:74.8184 val_loss:0.5399
[03/0024] | train_loss:0.3068 val_acc:76.5133 val_loss:0.5287
[03/0025] | train_loss:0.2809 val_acc:75.7869 val_loss:0.5513
[03/0026] | train_loss:0.2856 val_acc:75.3027 val_loss:0.5956
[03/0027] | train_loss:0.2768 val_acc:73.3656 val_loss:0.6099
[03/0028] | train_loss:0.2662 val_acc:77.2397 val_loss:0.5653
[03/0029] | train_loss:0.2599 val_acc:75.0605 val_loss:0.629
[03/0030] | train_loss:0.2578 val_acc:75.3027 val_loss:0.5678
[03/0031] | train_loss:0.2332 val_acc:76.5133 val_loss:0.5711
[03/0032] | train_loss:0.2437 val_acc:76.0291 val_loss:0.5623
[03/0033] | train_loss:0.2327 val_acc:75.7869 val_loss:0.6415
[03/0034] | train_loss:0.2168 val_acc:78.4504 val_loss:0.6076
model is saved at epoch 34!![03/0035] | train_loss:0.2028 val_acc:76.9976 val_loss:0.6251
[03/0036] | train_loss:0.1914 val_acc:77.2397 val_loss:0.6396
[03/0037] | train_loss:0.189 val_acc:76.5133 val_loss:0.612
[03/0038] | train_loss:0.1841 val_acc:76.0291 val_loss:0.6373
[03/0039] | train_loss:0.1991 val_acc:78.4504 val_loss:0.5993
[03/0040] | train_loss:0.1851 val_acc:76.5133 val_loss:0.6612
[03/0041] | train_loss:0.1792 val_acc:76.9976 val_loss:0.6201
[03/0042] | train_loss:0.1647 val_acc:78.2082 val_loss:0.6449
[03/0043] | train_loss:0.151 val_acc:78.6925 val_loss:0.6436
model is saved at epoch 43!![03/0044] | train_loss:0.147 val_acc:75.7869 val_loss:0.6882
[03/0045] | train_loss:0.1677 val_acc:77.2397 val_loss:0.6159
[03/0046] | train_loss:0.1548 val_acc:77.724 val_loss:0.6623
[03/0047] | train_loss:0.1529 val_acc:75.3027 val_loss:0.7404
[03/0048] | train_loss:0.1314 val_acc:77.9661 val_loss:0.692
[03/0049] | train_loss:0.1115 val_acc:77.724 val_loss:0.6525
[03/0050] | train_loss:0.1131 val_acc:76.7554 val_loss:0.7144
[03/0051] | train_loss:0.1118 val_acc:77.724 val_loss:0.7464
[03/0052] | train_loss:0.1228 val_acc:76.2712 val_loss:0.7121
[03/0053] | train_loss:0.1375 val_acc:75.7869 val_loss:0.7527
[03/0054] | train_loss:0.1292 val_acc:76.7554 val_loss:0.7153
[03/0055] | train_loss:0.1303 val_acc:75.0605 val_loss:0.7536
[03/0056] | train_loss:0.1096 val_acc:77.724 val_loss:0.7448
[03/0057] | train_loss:0.1069 val_acc:77.4818 val_loss:0.7209
[03/0058] | train_loss:0.1007 val_acc:75.3027 val_loss:0.8529
[03/0059] | train_loss:0.11 val_acc:78.2082 val_loss:0.7674
[03/0060] | train_loss:0.1079 val_acc:75.3027 val_loss:0.8183
[03/0061] | train_loss:0.0918 val_acc:76.9976 val_loss:0.8406
[03/0062] | train_loss:0.1061 val_acc:75.7869 val_loss:0.7848
[03/0063] | train_loss:0.0924 val_acc:78.9346 val_loss:0.8117
model is saved at epoch 63!![03/0064] | train_loss:0.0856 val_acc:78.9346 val_loss:0.7888
[03/0065] | train_loss:0.1063 val_acc:75.5448 val_loss:0.8486
[03/0066] | train_loss:0.1083 val_acc:76.0291 val_loss:0.798
[03/0067] | train_loss:0.0948 val_acc:77.724 val_loss:0.8187
[03/0068] | train_loss:0.0819 val_acc:77.724 val_loss:0.7893
[03/0069] | train_loss:0.0852 val_acc:77.4818 val_loss:0.8042
[03/0070] | train_loss:0.0749 val_acc:77.724 val_loss:0.9269
[03/0071] | train_loss:0.074 val_acc:76.2712 val_loss:0.8648
[03/0072] | train_loss:0.0716 val_acc:78.2082 val_loss:0.8725
[03/0073] | train_loss:0.0739 val_acc:78.6925 val_loss:0.7993
[03/0074] | train_loss:0.0683 val_acc:79.1768 val_loss:0.8729
model is saved at epoch 74!![03/0075] | train_loss:0.0618 val_acc:76.9976 val_loss:0.9764
[03/0076] | train_loss:0.0591 val_acc:76.2712 val_loss:0.9918
[03/0077] | train_loss:0.0571 val_acc:78.9346 val_loss:0.9527
[03/0078] | train_loss:0.0555 val_acc:76.7554 val_loss:1.0083
[03/0079] | train_loss:0.068 val_acc:75.3027 val_loss:0.9632
[03/0080] | train_loss:0.0693 val_acc:75.3027 val_loss:0.9306
[03/0081] | train_loss:0.0676 val_acc:76.7554 val_loss:0.9216
[03/0082] | train_loss:0.0607 val_acc:76.7554 val_loss:0.9351
[03/0083] | train_loss:0.0715 val_acc:77.2397 val_loss:0.9102
[03/0084] | train_loss:0.0874 val_acc:75.7869 val_loss:0.8881
[03/0085] | train_loss:0.0837 val_acc:76.0291 val_loss:0.9103
[03/0086] | train_loss:0.0688 val_acc:75.7869 val_loss:0.9388
[03/0087] | train_loss:0.0737 val_acc:76.2712 val_loss:0.9743
[03/0088] | train_loss:0.1108 val_acc:74.8184 val_loss:0.8939
[03/0089] | train_loss:0.0938 val_acc:77.2397 val_loss:0.9032
[03/0090] | train_loss:0.0605 val_acc:77.2397 val_loss:0.9428
[03/0091] | train_loss:0.0529 val_acc:78.2082 val_loss:0.9804
[03/0092] | train_loss:0.0507 val_acc:76.9976 val_loss:0.9494
[03/0093] | train_loss:0.0489 val_acc:79.1768 val_loss:0.9754
[03/0094] | train_loss:0.05 val_acc:78.9346 val_loss:0.9737
[03/0095] | train_loss:0.0533 val_acc:78.2082 val_loss:1.0378
[03/0096] | train_loss:0.0582 val_acc:78.2082 val_loss:0.9261
[03/0097] | train_loss:0.0544 val_acc:76.9976 val_loss:0.9833
[03/0098] | train_loss:0.0518 val_acc:77.9661 val_loss:1.0112
[03/0099] | train_loss:0.0582 val_acc:76.5133 val_loss:0.894
[03/0100] | train_loss:0.0488 val_acc:76.2712 val_loss:1.0438
[03/0101] | train_loss:0.0462 val_acc:77.9661 val_loss:1.027
[03/0102] | train_loss:0.0464 val_acc:74.8184 val_loss:1.1477
[03/0103] | train_loss:0.0629 val_acc:78.4504 val_loss:0.9399
[03/0104] | train_loss:0.073 val_acc:79.4189 val_loss:0.9036
model is saved at epoch 104!![03/0105] | train_loss:0.0626 val_acc:77.724 val_loss:0.9384
[03/0106] | train_loss:0.0744 val_acc:77.724 val_loss:0.9815
[03/0107] | train_loss:0.0739 val_acc:76.2712 val_loss:0.9733
[03/0108] | train_loss:0.0758 val_acc:75.7869 val_loss:1.0197
[03/0109] | train_loss:0.0676 val_acc:76.0291 val_loss:1.0424
[03/0110] | train_loss:0.0591 val_acc:76.2712 val_loss:1.1451
[03/0111] | train_loss:0.0555 val_acc:78.9346 val_loss:0.9309
[03/0112] | train_loss:0.0579 val_acc:78.9346 val_loss:0.9783
[03/0113] | train_loss:0.0469 val_acc:76.5133 val_loss:1.092
[03/0114] | train_loss:0.0514 val_acc:76.7554 val_loss:1.0942
[03/0115] | train_loss:0.0725 val_acc:76.0291 val_loss:1.015
[03/0116] | train_loss:0.0538 val_acc:76.9976 val_loss:1.1017
[03/0117] | train_loss:0.0507 val_acc:76.2712 val_loss:1.0744
[03/0118] | train_loss:0.0446 val_acc:76.0291 val_loss:1.1612
[03/0119] | train_loss:0.0451 val_acc:78.6925 val_loss:1.1717
[03/0120] | train_loss:0.0361 val_acc:77.4818 val_loss:1.2217
[03/0121] | train_loss:0.0509 val_acc:77.4818 val_loss:1.0141
[03/0122] | train_loss:0.0505 val_acc:74.3341 val_loss:1.1453
[03/0123] | train_loss:0.0491 val_acc:78.2082 val_loss:1.0897
[03/0124] | train_loss:0.0416 val_acc:75.7869 val_loss:1.119
[03/0125] | train_loss:0.0482 val_acc:73.8499 val_loss:1.2489
[03/0126] | train_loss:0.0688 val_acc:75.5448 val_loss:0.985
[03/0127] | train_loss:0.0692 val_acc:77.4818 val_loss:0.9678
[03/0128] | train_loss:0.0365 val_acc:76.2712 val_loss:1.2928
[03/0129] | train_loss:0.049 val_acc:75.3027 val_loss:1.221
[03/0130] | train_loss:0.0682 val_acc:77.4818 val_loss:0.9367
[03/0131] | train_loss:0.0421 val_acc:79.1768 val_loss:1.1596
[03/0132] | train_loss:0.0308 val_acc:78.4504 val_loss:1.1532
[03/0133] | train_loss:0.0408 val_acc:78.2082 val_loss:1.0732
[03/0134] | train_loss:0.0299 val_acc:76.7554 val_loss:1.1706
[03/0135] | train_loss:0.0389 val_acc:74.092 val_loss:1.1858
[03/0136] | train_loss:0.0344 val_acc:76.0291 val_loss:1.1492
[03/0137] | train_loss:0.0332 val_acc:75.3027 val_loss:1.3384
[03/0138] | train_loss:0.04 val_acc:76.5133 val_loss:1.1843
[03/0139] | train_loss:0.0412 val_acc:77.2397 val_loss:1.15
[03/0140] | train_loss:0.0559 val_acc:76.5133 val_loss:1.0797
[03/0141] | train_loss:0.054 val_acc:76.9976 val_loss:1.1821
[03/0142] | train_loss:0.0593 val_acc:74.8184 val_loss:1.2003
[03/0143] | train_loss:0.0496 val_acc:76.5133 val_loss:1.1176
[03/0144] | train_loss:0.0715 val_acc:76.7554 val_loss:1.0175
[03/0145] | train_loss:0.0399 val_acc:77.4818 val_loss:1.2117
[03/0146] | train_loss:0.0432 val_acc:77.2397 val_loss:1.1362
[03/0147] | train_loss:0.0448 val_acc:75.5448 val_loss:1.2213
[03/0148] | train_loss:0.0456 val_acc:75.5448 val_loss:1.1907
[03/0149] | train_loss:0.0413 val_acc:76.9976 val_loss:1.1638
[03/0150] | train_loss:0.0394 val_acc:74.8184 val_loss:1.201
[03/0151] | train_loss:0.0349 val_acc:76.7554 val_loss:1.2064
[03/0152] | train_loss:0.0365 val_acc:76.7554 val_loss:1.224
[03/0153] | train_loss:0.0369 val_acc:75.7869 val_loss:1.2529
[03/0154] | train_loss:0.0346 val_acc:76.5133 val_loss:1.3298
[03/0155] | train_loss:0.0327 val_acc:76.5133 val_loss:1.2797
Fold: [3/10] Test is finish !! 
 Test Metrics are: test_acc:77.2397 test_loss:0.9305fold [3/10] is start!!
[04/0001] | train_loss:0.671 val_acc:50.1211 val_loss:0.6896
model is saved at epoch 1!![04/0002] | train_loss:0.6295 val_acc:49.8789 val_loss:0.6864
[04/0003] | train_loss:0.5932 val_acc:49.8789 val_loss:0.6857
[04/0004] | train_loss:0.5666 val_acc:56.1743 val_loss:0.6669
model is saved at epoch 4!![04/0005] | train_loss:0.5523 val_acc:53.0266 val_loss:0.6861
[04/0006] | train_loss:0.5381 val_acc:68.523 val_loss:0.5868
model is saved at epoch 6!![04/0007] | train_loss:0.5112 val_acc:72.8814 val_loss:0.567
model is saved at epoch 7!![04/0008] | train_loss:0.495 val_acc:74.5763 val_loss:0.5441
model is saved at epoch 8!![04/0009] | train_loss:0.4809 val_acc:72.6392 val_loss:0.5535
[04/0010] | train_loss:0.4647 val_acc:73.6077 val_loss:0.5459
[04/0011] | train_loss:0.4531 val_acc:74.092 val_loss:0.5649
[04/0012] | train_loss:0.4408 val_acc:75.7869 val_loss:0.549
model is saved at epoch 12!![04/0013] | train_loss:0.4274 val_acc:75.7869 val_loss:0.5282
[04/0014] | train_loss:0.4132 val_acc:77.4818 val_loss:0.5192
model is saved at epoch 14!![04/0015] | train_loss:0.3955 val_acc:77.2397 val_loss:0.5382
[04/0016] | train_loss:0.3987 val_acc:71.4286 val_loss:0.5661
[04/0017] | train_loss:0.3768 val_acc:76.9976 val_loss:0.5135
[04/0018] | train_loss:0.3649 val_acc:77.2397 val_loss:0.5225
[04/0019] | train_loss:0.3515 val_acc:72.155 val_loss:0.5827
[04/0020] | train_loss:0.3425 val_acc:74.5763 val_loss:0.5566
[04/0021] | train_loss:0.3254 val_acc:76.7554 val_loss:0.5636
[04/0022] | train_loss:0.3143 val_acc:76.2712 val_loss:0.541
[04/0023] | train_loss:0.3137 val_acc:72.3971 val_loss:0.5969
[04/0024] | train_loss:0.2972 val_acc:78.2082 val_loss:0.5196
model is saved at epoch 24!![04/0025] | train_loss:0.2855 val_acc:77.4818 val_loss:0.5764
[04/0026] | train_loss:0.2899 val_acc:79.9031 val_loss:0.5436
model is saved at epoch 26!![04/0027] | train_loss:0.2768 val_acc:76.7554 val_loss:0.5745
[04/0028] | train_loss:0.2541 val_acc:74.8184 val_loss:0.6032
[04/0029] | train_loss:0.2505 val_acc:74.3341 val_loss:0.613
[04/0030] | train_loss:0.2669 val_acc:74.5763 val_loss:0.611
[04/0031] | train_loss:0.2563 val_acc:75.7869 val_loss:0.5858
[04/0032] | train_loss:0.2291 val_acc:77.724 val_loss:0.6164
[04/0033] | train_loss:0.2241 val_acc:77.724 val_loss:0.5716
[04/0034] | train_loss:0.2136 val_acc:76.2712 val_loss:0.6055
[04/0035] | train_loss:0.2276 val_acc:75.0605 val_loss:0.6041
[04/0036] | train_loss:0.1947 val_acc:76.0291 val_loss:0.6073
[04/0037] | train_loss:0.192 val_acc:76.5133 val_loss:0.6106
[04/0038] | train_loss:0.1959 val_acc:75.3027 val_loss:0.6211
[04/0039] | train_loss:0.1881 val_acc:73.3656 val_loss:0.6978
[04/0040] | train_loss:0.1765 val_acc:75.7869 val_loss:0.6517
[04/0041] | train_loss:0.1586 val_acc:75.7869 val_loss:0.7053
[04/0042] | train_loss:0.1505 val_acc:77.2397 val_loss:0.6425
[04/0043] | train_loss:0.1524 val_acc:73.8499 val_loss:0.7593
[04/0044] | train_loss:0.152 val_acc:75.5448 val_loss:0.6532
[04/0045] | train_loss:0.1386 val_acc:75.3027 val_loss:0.7729
[04/0046] | train_loss:0.1343 val_acc:76.7554 val_loss:0.6966
[04/0047] | train_loss:0.1405 val_acc:76.7554 val_loss:0.7494
[04/0048] | train_loss:0.1266 val_acc:75.3027 val_loss:0.7703
[04/0049] | train_loss:0.1293 val_acc:76.2712 val_loss:0.7397
[04/0050] | train_loss:0.1141 val_acc:75.0605 val_loss:0.7031
[04/0051] | train_loss:0.1322 val_acc:73.8499 val_loss:0.7759
[04/0052] | train_loss:0.1323 val_acc:77.4818 val_loss:0.7162
[04/0053] | train_loss:0.1072 val_acc:75.7869 val_loss:0.8084
[04/0054] | train_loss:0.1067 val_acc:76.9976 val_loss:0.8142
[04/0055] | train_loss:0.1173 val_acc:74.092 val_loss:0.8298
[04/0056] | train_loss:0.108 val_acc:78.6925 val_loss:0.7867
[04/0057] | train_loss:0.1198 val_acc:75.3027 val_loss:0.8525
[04/0058] | train_loss:0.1094 val_acc:76.7554 val_loss:0.7738
[04/0059] | train_loss:0.1214 val_acc:76.0291 val_loss:0.7988
[04/0060] | train_loss:0.1076 val_acc:74.8184 val_loss:0.8503
[04/0061] | train_loss:0.1043 val_acc:78.2082 val_loss:0.785
[04/0062] | train_loss:0.1291 val_acc:72.155 val_loss:0.8901
[04/0063] | train_loss:0.1242 val_acc:75.3027 val_loss:0.8436
[04/0064] | train_loss:0.1219 val_acc:75.3027 val_loss:0.8217
[04/0065] | train_loss:0.1165 val_acc:76.0291 val_loss:0.8378
[04/0066] | train_loss:0.0963 val_acc:74.092 val_loss:0.9281
[04/0067] | train_loss:0.094 val_acc:74.092 val_loss:0.9638
[04/0068] | train_loss:0.0839 val_acc:75.3027 val_loss:0.8733
[04/0069] | train_loss:0.0715 val_acc:73.6077 val_loss:0.9785
[04/0070] | train_loss:0.0926 val_acc:76.2712 val_loss:0.8081
[04/0071] | train_loss:0.0762 val_acc:76.5133 val_loss:0.9193
[04/0072] | train_loss:0.0685 val_acc:78.4504 val_loss:0.8979
[04/0073] | train_loss:0.084 val_acc:73.8499 val_loss:0.9376
[04/0074] | train_loss:0.0928 val_acc:78.6925 val_loss:0.7904
[04/0075] | train_loss:0.0843 val_acc:76.2712 val_loss:0.9047
[04/0076] | train_loss:0.0746 val_acc:74.5763 val_loss:0.9783
[04/0077] | train_loss:0.0718 val_acc:75.7869 val_loss:0.9433
Fold: [4/10] Test is finish !! 
 Test Metrics are: test_acc:77.4818 test_loss:0.5686fold [4/10] is start!!
[05/0001] | train_loss:0.6777 val_acc:50.6053 val_loss:0.6953
model is saved at epoch 1!![05/0002] | train_loss:0.6344 val_acc:48.9104 val_loss:0.69
[05/0003] | train_loss:0.612 val_acc:49.1525 val_loss:0.6883
[05/0004] | train_loss:0.5839 val_acc:49.3947 val_loss:0.68
[05/0005] | train_loss:0.568 val_acc:62.7119 val_loss:0.6594
model is saved at epoch 5!![05/0006] | train_loss:0.5379 val_acc:65.8596 val_loss:0.6091
model is saved at epoch 6!![05/0007] | train_loss:0.5168 val_acc:71.9128 val_loss:0.5656
model is saved at epoch 7!![05/0008] | train_loss:0.5198 val_acc:72.6392 val_loss:0.552
model is saved at epoch 8!![05/0009] | train_loss:0.4925 val_acc:72.3971 val_loss:0.5461
[05/0010] | train_loss:0.4786 val_acc:75.5448 val_loss:0.5399
model is saved at epoch 10!![05/0011] | train_loss:0.4686 val_acc:76.0291 val_loss:0.5226
model is saved at epoch 11!![05/0012] | train_loss:0.4526 val_acc:76.7554 val_loss:0.5196
model is saved at epoch 12!![05/0013] | train_loss:0.4363 val_acc:75.0605 val_loss:0.522
[05/0014] | train_loss:0.4127 val_acc:76.2712 val_loss:0.5268
[05/0015] | train_loss:0.4133 val_acc:75.5448 val_loss:0.5329
[05/0016] | train_loss:0.3898 val_acc:75.3027 val_loss:0.5284
[05/0017] | train_loss:0.3973 val_acc:76.7554 val_loss:0.5201
[05/0018] | train_loss:0.3844 val_acc:75.5448 val_loss:0.5188
[05/0019] | train_loss:0.3679 val_acc:72.3971 val_loss:0.564
[05/0020] | train_loss:0.3526 val_acc:76.2712 val_loss:0.5472
[05/0021] | train_loss:0.3391 val_acc:71.1864 val_loss:0.6189
[05/0022] | train_loss:0.3374 val_acc:75.0605 val_loss:0.537
[05/0023] | train_loss:0.3191 val_acc:76.9976 val_loss:0.5465
model is saved at epoch 23!![05/0024] | train_loss:0.3089 val_acc:75.7869 val_loss:0.5357
[05/0025] | train_loss:0.2987 val_acc:76.9976 val_loss:0.5716
[05/0026] | train_loss:0.2891 val_acc:75.0605 val_loss:0.5722
[05/0027] | train_loss:0.2715 val_acc:75.3027 val_loss:0.5586
[05/0028] | train_loss:0.2616 val_acc:73.3656 val_loss:0.6369
[05/0029] | train_loss:0.2739 val_acc:73.3656 val_loss:0.5766
[05/0030] | train_loss:0.2559 val_acc:75.3027 val_loss:0.6142
[05/0031] | train_loss:0.2487 val_acc:74.092 val_loss:0.5968
[05/0032] | train_loss:0.2377 val_acc:73.8499 val_loss:0.6268
[05/0033] | train_loss:0.2295 val_acc:74.5763 val_loss:0.619
[05/0034] | train_loss:0.2267 val_acc:76.7554 val_loss:0.6029
[05/0035] | train_loss:0.2039 val_acc:75.5448 val_loss:0.6051
[05/0036] | train_loss:0.1963 val_acc:76.2712 val_loss:0.6052
[05/0037] | train_loss:0.1923 val_acc:77.9661 val_loss:0.6177
model is saved at epoch 37!![05/0038] | train_loss:0.1929 val_acc:75.0605 val_loss:0.6763
[05/0039] | train_loss:0.1915 val_acc:74.8184 val_loss:0.6611
[05/0040] | train_loss:0.1791 val_acc:77.2397 val_loss:0.6394
[05/0041] | train_loss:0.1603 val_acc:77.9661 val_loss:0.6336
[05/0042] | train_loss:0.1627 val_acc:76.7554 val_loss:0.6572
[05/0043] | train_loss:0.1738 val_acc:75.3027 val_loss:0.6795
[05/0044] | train_loss:0.1525 val_acc:75.0605 val_loss:0.6805
[05/0045] | train_loss:0.151 val_acc:76.2712 val_loss:0.6551
[05/0046] | train_loss:0.1537 val_acc:77.2397 val_loss:0.7127
[05/0047] | train_loss:0.1488 val_acc:76.5133 val_loss:0.6885
[05/0048] | train_loss:0.1396 val_acc:75.5448 val_loss:0.7001
[05/0049] | train_loss:0.1348 val_acc:77.4818 val_loss:0.7161
[05/0050] | train_loss:0.1216 val_acc:73.8499 val_loss:0.7431
[05/0051] | train_loss:0.1261 val_acc:73.3656 val_loss:0.7299
[05/0052] | train_loss:0.1315 val_acc:75.5448 val_loss:0.8172
[05/0053] | train_loss:0.1252 val_acc:77.4818 val_loss:0.7843
[05/0054] | train_loss:0.1176 val_acc:76.2712 val_loss:0.7585
[05/0055] | train_loss:0.1174 val_acc:76.5133 val_loss:0.7395
[05/0056] | train_loss:0.132 val_acc:76.5133 val_loss:0.7451
[05/0057] | train_loss:0.1019 val_acc:77.4818 val_loss:0.7786
[05/0058] | train_loss:0.0996 val_acc:77.2397 val_loss:0.7803
[05/0059] | train_loss:0.104 val_acc:77.724 val_loss:0.7481
[05/0060] | train_loss:0.0974 val_acc:77.4818 val_loss:0.7864
[05/0061] | train_loss:0.0986 val_acc:74.092 val_loss:0.8759
[05/0062] | train_loss:0.1067 val_acc:76.0291 val_loss:0.7756
[05/0063] | train_loss:0.1039 val_acc:79.1768 val_loss:0.8675
model is saved at epoch 63!![05/0064] | train_loss:0.0845 val_acc:74.3341 val_loss:0.8689
[05/0065] | train_loss:0.0794 val_acc:76.5133 val_loss:0.8419
[05/0066] | train_loss:0.0849 val_acc:75.3027 val_loss:0.8346
[05/0067] | train_loss:0.0838 val_acc:74.092 val_loss:0.9083
[05/0068] | train_loss:0.0834 val_acc:76.5133 val_loss:0.862
[05/0069] | train_loss:0.0798 val_acc:76.5133 val_loss:0.9481
[05/0070] | train_loss:0.0765 val_acc:76.0291 val_loss:0.8728
[05/0071] | train_loss:0.079 val_acc:76.7554 val_loss:0.8244
[05/0072] | train_loss:0.0629 val_acc:77.2397 val_loss:0.9285
[05/0073] | train_loss:0.0797 val_acc:75.3027 val_loss:0.8961
[05/0074] | train_loss:0.073 val_acc:74.5763 val_loss:0.9709
[05/0075] | train_loss:0.0652 val_acc:75.0605 val_loss:0.9783
[05/0076] | train_loss:0.0749 val_acc:75.7869 val_loss:0.9799
[05/0077] | train_loss:0.0911 val_acc:75.0605 val_loss:0.9909
[05/0078] | train_loss:0.0964 val_acc:75.5448 val_loss:0.9012
[05/0079] | train_loss:0.0757 val_acc:76.5133 val_loss:0.9671
[05/0080] | train_loss:0.0639 val_acc:74.092 val_loss:0.9878
[05/0081] | train_loss:0.0699 val_acc:79.661 val_loss:0.9487
model is saved at epoch 81!![05/0082] | train_loss:0.0734 val_acc:76.5133 val_loss:0.9377
[05/0083] | train_loss:0.0757 val_acc:77.724 val_loss:0.9696
[05/0084] | train_loss:0.0734 val_acc:76.9976 val_loss:0.922
[05/0085] | train_loss:0.0684 val_acc:77.2397 val_loss:0.9389
[05/0086] | train_loss:0.0681 val_acc:75.7869 val_loss:0.9933
[05/0087] | train_loss:0.0703 val_acc:75.7869 val_loss:0.9045
[05/0088] | train_loss:0.0686 val_acc:77.2397 val_loss:0.9299
[05/0089] | train_loss:0.0504 val_acc:76.9976 val_loss:1.0195
[05/0090] | train_loss:0.064 val_acc:76.7554 val_loss:1.0566
[05/0091] | train_loss:0.0684 val_acc:76.7554 val_loss:0.8945
[05/0092] | train_loss:0.0577 val_acc:76.9976 val_loss:1.0206
[05/0093] | train_loss:0.0472 val_acc:76.0291 val_loss:0.9933
[05/0094] | train_loss:0.045 val_acc:78.2082 val_loss:0.9551
[05/0095] | train_loss:0.0471 val_acc:77.724 val_loss:0.9579
[05/0096] | train_loss:0.0634 val_acc:77.4818 val_loss:1.0097
[05/0097] | train_loss:0.0609 val_acc:77.4818 val_loss:1.0057
[05/0098] | train_loss:0.0488 val_acc:76.2712 val_loss:1.0495
[05/0099] | train_loss:0.0636 val_acc:75.7869 val_loss:0.9771
[05/0100] | train_loss:0.051 val_acc:77.4818 val_loss:1.0692
[05/0101] | train_loss:0.0497 val_acc:78.9346 val_loss:1.0012
[05/0102] | train_loss:0.0623 val_acc:76.7554 val_loss:1.0219
[05/0103] | train_loss:0.0659 val_acc:75.3027 val_loss:1.042
[05/0104] | train_loss:0.054 val_acc:76.5133 val_loss:1.0612
[05/0105] | train_loss:0.0571 val_acc:76.9976 val_loss:1.0592
[05/0106] | train_loss:0.059 val_acc:74.8184 val_loss:1.05
[05/0107] | train_loss:0.0483 val_acc:76.9976 val_loss:1.0517
[05/0108] | train_loss:0.0578 val_acc:79.661 val_loss:0.9488
[05/0109] | train_loss:0.047 val_acc:76.9976 val_loss:1.0862
[05/0110] | train_loss:0.0407 val_acc:76.2712 val_loss:1.1638
[05/0111] | train_loss:0.0433 val_acc:74.3341 val_loss:1.1516
[05/0112] | train_loss:0.0422 val_acc:75.5448 val_loss:1.138
[05/0113] | train_loss:0.0522 val_acc:75.3027 val_loss:1.0596
[05/0114] | train_loss:0.0438 val_acc:73.8499 val_loss:1.2996
[05/0115] | train_loss:0.0432 val_acc:77.2397 val_loss:1.1569
[05/0116] | train_loss:0.0525 val_acc:76.9976 val_loss:0.9729
[05/0117] | train_loss:0.0633 val_acc:76.9976 val_loss:1.0865
[05/0118] | train_loss:0.0494 val_acc:74.092 val_loss:1.1906
[05/0119] | train_loss:0.0486 val_acc:75.3027 val_loss:1.1197
[05/0120] | train_loss:0.0371 val_acc:75.0605 val_loss:1.218
[05/0121] | train_loss:0.044 val_acc:76.0291 val_loss:1.1003
[05/0122] | train_loss:0.0418 val_acc:77.9661 val_loss:1.1376
[05/0123] | train_loss:0.0438 val_acc:76.5133 val_loss:1.1807
[05/0124] | train_loss:0.04 val_acc:76.0291 val_loss:1.2911
[05/0125] | train_loss:0.0377 val_acc:78.2082 val_loss:1.1324
[05/0126] | train_loss:0.0332 val_acc:77.9661 val_loss:1.212
[05/0127] | train_loss:0.0422 val_acc:76.2712 val_loss:1.1294
[05/0128] | train_loss:0.0454 val_acc:74.8184 val_loss:1.1471
[05/0129] | train_loss:0.0372 val_acc:76.2712 val_loss:1.1579
[05/0130] | train_loss:0.0512 val_acc:78.4504 val_loss:1.0828
[05/0131] | train_loss:0.0432 val_acc:75.5448 val_loss:1.2334
[05/0132] | train_loss:0.0488 val_acc:74.5763 val_loss:1.2129
Fold: [5/10] Test is finish !! 
 Test Metrics are: test_acc:82.5666 test_loss:0.7346fold [5/10] is start!!
[06/0001] | train_loss:0.674 val_acc:53.9952 val_loss:0.6924
model is saved at epoch 1!![06/0002] | train_loss:0.6359 val_acc:50.3632 val_loss:0.6803
[06/0003] | train_loss:0.6076 val_acc:50.6053 val_loss:0.6744
[06/0004] | train_loss:0.5819 val_acc:57.8692 val_loss:0.6487
model is saved at epoch 4!![06/0005] | train_loss:0.567 val_acc:55.9322 val_loss:0.6375
[06/0006] | train_loss:0.5585 val_acc:70.4601 val_loss:0.5726
model is saved at epoch 6!![06/0007] | train_loss:0.5335 val_acc:73.1235 val_loss:0.5274
model is saved at epoch 7!![06/0008] | train_loss:0.5221 val_acc:75.5448 val_loss:0.5046
model is saved at epoch 8!![06/0009] | train_loss:0.5109 val_acc:77.9661 val_loss:0.4947
model is saved at epoch 9!![06/0010] | train_loss:0.489 val_acc:78.2082 val_loss:0.4869
model is saved at epoch 10!![06/0011] | train_loss:0.4772 val_acc:80.3874 val_loss:0.477
model is saved at epoch 11!![06/0012] | train_loss:0.4814 val_acc:79.661 val_loss:0.4803
[06/0013] | train_loss:0.4545 val_acc:79.661 val_loss:0.5068
[06/0014] | train_loss:0.4344 val_acc:79.1768 val_loss:0.4725
[06/0015] | train_loss:0.4259 val_acc:77.4818 val_loss:0.4801
[06/0016] | train_loss:0.4144 val_acc:79.9031 val_loss:0.4641
[06/0017] | train_loss:0.4096 val_acc:77.2397 val_loss:0.4704
[06/0018] | train_loss:0.3893 val_acc:80.1453 val_loss:0.4736
[06/0019] | train_loss:0.3846 val_acc:74.3341 val_loss:0.4936
[06/0020] | train_loss:0.3698 val_acc:80.8717 val_loss:0.4611
model is saved at epoch 20!![06/0021] | train_loss:0.3522 val_acc:81.5981 val_loss:0.4648
model is saved at epoch 21!![06/0022] | train_loss:0.3471 val_acc:77.724 val_loss:0.4625
[06/0023] | train_loss:0.3393 val_acc:80.3874 val_loss:0.4706
[06/0024] | train_loss:0.3339 val_acc:79.9031 val_loss:0.4451
[06/0025] | train_loss:0.3269 val_acc:80.1453 val_loss:0.4601
[06/0026] | train_loss:0.3163 val_acc:79.1768 val_loss:0.4682
[06/0027] | train_loss:0.2947 val_acc:80.1453 val_loss:0.4661
[06/0028] | train_loss:0.2865 val_acc:77.4818 val_loss:0.4775
[06/0029] | train_loss:0.2735 val_acc:78.9346 val_loss:0.5135
[06/0030] | train_loss:0.2507 val_acc:79.4189 val_loss:0.4944
[06/0031] | train_loss:0.2561 val_acc:79.4189 val_loss:0.4847
[06/0032] | train_loss:0.2492 val_acc:79.4189 val_loss:0.5102
[06/0033] | train_loss:0.2378 val_acc:78.6925 val_loss:0.498
[06/0034] | train_loss:0.2363 val_acc:78.9346 val_loss:0.4958
[06/0035] | train_loss:0.2391 val_acc:79.661 val_loss:0.4769
[06/0036] | train_loss:0.2212 val_acc:79.4189 val_loss:0.4951
[06/0037] | train_loss:0.2023 val_acc:78.6925 val_loss:0.5327
[06/0038] | train_loss:0.2015 val_acc:77.724 val_loss:0.5384
[06/0039] | train_loss:0.2017 val_acc:78.9346 val_loss:0.5321
[06/0040] | train_loss:0.1792 val_acc:79.661 val_loss:0.5326
[06/0041] | train_loss:0.1731 val_acc:79.1768 val_loss:0.5483
[06/0042] | train_loss:0.1839 val_acc:79.1768 val_loss:0.5638
[06/0043] | train_loss:0.1733 val_acc:78.2082 val_loss:0.5426
[06/0044] | train_loss:0.1611 val_acc:77.724 val_loss:0.601
[06/0045] | train_loss:0.1727 val_acc:77.9661 val_loss:0.5597
[06/0046] | train_loss:0.1798 val_acc:79.9031 val_loss:0.5734
[06/0047] | train_loss:0.157 val_acc:80.6295 val_loss:0.5515
[06/0048] | train_loss:0.1448 val_acc:81.8402 val_loss:0.5506
model is saved at epoch 48!![06/0049] | train_loss:0.1419 val_acc:79.4189 val_loss:0.5921
[06/0050] | train_loss:0.1566 val_acc:79.4189 val_loss:0.5908
[06/0051] | train_loss:0.1479 val_acc:80.3874 val_loss:0.5724
[06/0052] | train_loss:0.1309 val_acc:79.661 val_loss:0.5788
[06/0053] | train_loss:0.1191 val_acc:80.1453 val_loss:0.577
[06/0054] | train_loss:0.1176 val_acc:77.724 val_loss:0.6666
[06/0055] | train_loss:0.1258 val_acc:78.9346 val_loss:0.642
[06/0056] | train_loss:0.1185 val_acc:76.9976 val_loss:0.6311
[06/0057] | train_loss:0.1304 val_acc:79.1768 val_loss:0.6525
[06/0058] | train_loss:0.1336 val_acc:80.1453 val_loss:0.585
[06/0059] | train_loss:0.1154 val_acc:80.1453 val_loss:0.6712
[06/0060] | train_loss:0.0975 val_acc:81.1138 val_loss:0.6567
[06/0061] | train_loss:0.0982 val_acc:80.3874 val_loss:0.6318
[06/0062] | train_loss:0.0995 val_acc:78.4504 val_loss:0.6745
[06/0063] | train_loss:0.098 val_acc:78.9346 val_loss:0.7131
[06/0064] | train_loss:0.1118 val_acc:76.2712 val_loss:0.6843
[06/0065] | train_loss:0.112 val_acc:78.9346 val_loss:0.694
[06/0066] | train_loss:0.0893 val_acc:79.1768 val_loss:0.7259
[06/0067] | train_loss:0.0881 val_acc:81.3559 val_loss:0.7177
[06/0068] | train_loss:0.0871 val_acc:78.4504 val_loss:0.7293
[06/0069] | train_loss:0.1131 val_acc:77.724 val_loss:0.6649
[06/0070] | train_loss:0.1168 val_acc:79.1768 val_loss:0.6762
[06/0071] | train_loss:0.1049 val_acc:78.6925 val_loss:0.6577
[06/0072] | train_loss:0.0976 val_acc:78.9346 val_loss:0.6802
[06/0073] | train_loss:0.0765 val_acc:78.4504 val_loss:0.7513
[06/0074] | train_loss:0.0917 val_acc:80.1453 val_loss:0.7187
[06/0075] | train_loss:0.0786 val_acc:78.9346 val_loss:0.7695
[06/0076] | train_loss:0.0669 val_acc:79.661 val_loss:0.7222
[06/0077] | train_loss:0.0812 val_acc:79.4189 val_loss:0.7559
[06/0078] | train_loss:0.0774 val_acc:78.2082 val_loss:0.7739
[06/0079] | train_loss:0.0691 val_acc:79.9031 val_loss:0.747
[06/0080] | train_loss:0.0754 val_acc:78.9346 val_loss:0.7316
[06/0081] | train_loss:0.0662 val_acc:77.9661 val_loss:0.7619
[06/0082] | train_loss:0.0616 val_acc:78.4504 val_loss:0.802
[06/0083] | train_loss:0.0604 val_acc:80.1453 val_loss:0.7907
[06/0084] | train_loss:0.071 val_acc:81.3559 val_loss:0.6949
[06/0085] | train_loss:0.0736 val_acc:78.4504 val_loss:0.7536
[06/0086] | train_loss:0.0726 val_acc:80.6295 val_loss:0.6791
[06/0087] | train_loss:0.0673 val_acc:79.661 val_loss:0.7653
[06/0088] | train_loss:0.0677 val_acc:80.3874 val_loss:0.784
[06/0089] | train_loss:0.0759 val_acc:80.6295 val_loss:0.7517
[06/0090] | train_loss:0.0716 val_acc:78.6925 val_loss:0.7887
[06/0091] | train_loss:0.0639 val_acc:80.1453 val_loss:0.8555
[06/0092] | train_loss:0.0721 val_acc:77.724 val_loss:0.7677
[06/0093] | train_loss:0.0588 val_acc:78.9346 val_loss:0.8835
[06/0094] | train_loss:0.0686 val_acc:79.661 val_loss:0.8027
[06/0095] | train_loss:0.0601 val_acc:80.1453 val_loss:0.8204
[06/0096] | train_loss:0.0692 val_acc:76.7554 val_loss:0.7633
[06/0097] | train_loss:0.0624 val_acc:78.9346 val_loss:0.8609
[06/0098] | train_loss:0.0654 val_acc:78.4504 val_loss:0.8762
[06/0099] | train_loss:0.0693 val_acc:78.9346 val_loss:0.843
Fold: [6/10] Test is finish !! 
 Test Metrics are: test_acc:81.1138 test_loss:0.5231fold [6/10] is start!!
[07/0001] | train_loss:0.6701 val_acc:53.2688 val_loss:0.6855
model is saved at epoch 1!![07/0002] | train_loss:0.6334 val_acc:53.2688 val_loss:0.6789
[07/0003] | train_loss:0.6012 val_acc:53.2688 val_loss:0.68
[07/0004] | train_loss:0.5771 val_acc:54.4794 val_loss:0.6666
model is saved at epoch 4!![07/0005] | train_loss:0.5447 val_acc:59.5642 val_loss:0.6281
model is saved at epoch 5!![07/0006] | train_loss:0.5337 val_acc:66.1017 val_loss:0.5684
model is saved at epoch 6!![07/0007] | train_loss:0.5157 val_acc:73.6077 val_loss:0.5075
model is saved at epoch 7!![07/0008] | train_loss:0.498 val_acc:73.6077 val_loss:0.4887
[07/0009] | train_loss:0.4944 val_acc:76.9976 val_loss:0.4806
model is saved at epoch 9!![07/0010] | train_loss:0.4738 val_acc:78.2082 val_loss:0.4788
model is saved at epoch 10!![07/0011] | train_loss:0.4637 val_acc:77.2397 val_loss:0.4635
[07/0012] | train_loss:0.4413 val_acc:79.661 val_loss:0.4738
model is saved at epoch 12!![07/0013] | train_loss:0.434 val_acc:80.3874 val_loss:0.4395
model is saved at epoch 13!![07/0014] | train_loss:0.4156 val_acc:79.1768 val_loss:0.4555
[07/0015] | train_loss:0.4084 val_acc:77.9661 val_loss:0.4478
[07/0016] | train_loss:0.3941 val_acc:79.1768 val_loss:0.4617
[07/0017] | train_loss:0.4002 val_acc:78.2082 val_loss:0.4726
[07/0018] | train_loss:0.3716 val_acc:78.9346 val_loss:0.4829
[07/0019] | train_loss:0.3644 val_acc:79.1768 val_loss:0.4856
[07/0020] | train_loss:0.3481 val_acc:80.1453 val_loss:0.4416
[07/0021] | train_loss:0.3466 val_acc:79.1768 val_loss:0.4407
[07/0022] | train_loss:0.3263 val_acc:80.6295 val_loss:0.4444
model is saved at epoch 22!![07/0023] | train_loss:0.3291 val_acc:80.1453 val_loss:0.4562
[07/0024] | train_loss:0.3023 val_acc:79.1768 val_loss:0.463
[07/0025] | train_loss:0.3047 val_acc:79.1768 val_loss:0.4435
[07/0026] | train_loss:0.297 val_acc:77.4818 val_loss:0.5243
[07/0027] | train_loss:0.3064 val_acc:78.9346 val_loss:0.442
[07/0028] | train_loss:0.2754 val_acc:80.3874 val_loss:0.4467
[07/0029] | train_loss:0.2561 val_acc:80.3874 val_loss:0.5417
[07/0030] | train_loss:0.2474 val_acc:82.0823 val_loss:0.4614
model is saved at epoch 30!![07/0031] | train_loss:0.2397 val_acc:80.6295 val_loss:0.453
[07/0032] | train_loss:0.2289 val_acc:80.3874 val_loss:0.4684
[07/0033] | train_loss:0.2125 val_acc:79.1768 val_loss:0.5085
[07/0034] | train_loss:0.2226 val_acc:79.661 val_loss:0.5077
[07/0035] | train_loss:0.2106 val_acc:80.6295 val_loss:0.5178
[07/0036] | train_loss:0.2156 val_acc:80.6295 val_loss:0.532
[07/0037] | train_loss:0.203 val_acc:78.9346 val_loss:0.5981
[07/0038] | train_loss:0.1914 val_acc:79.9031 val_loss:0.5079
[07/0039] | train_loss:0.1819 val_acc:79.4189 val_loss:0.5596
[07/0040] | train_loss:0.1727 val_acc:80.8717 val_loss:0.547
[07/0041] | train_loss:0.1563 val_acc:80.3874 val_loss:0.526
[07/0042] | train_loss:0.1575 val_acc:80.1453 val_loss:0.5506
[07/0043] | train_loss:0.1677 val_acc:79.9031 val_loss:0.5659
[07/0044] | train_loss:0.1522 val_acc:79.4189 val_loss:0.5549
[07/0045] | train_loss:0.1534 val_acc:80.6295 val_loss:0.5593
[07/0046] | train_loss:0.1407 val_acc:79.661 val_loss:0.5751
[07/0047] | train_loss:0.1412 val_acc:79.4189 val_loss:0.6243
[07/0048] | train_loss:0.1518 val_acc:80.1453 val_loss:0.5497
[07/0049] | train_loss:0.1377 val_acc:78.9346 val_loss:0.5909
[07/0050] | train_loss:0.1407 val_acc:79.9031 val_loss:0.5529
[07/0051] | train_loss:0.1267 val_acc:80.6295 val_loss:0.6242
[07/0052] | train_loss:0.12 val_acc:80.3874 val_loss:0.6078
[07/0053] | train_loss:0.1087 val_acc:77.4818 val_loss:0.6594
[07/0054] | train_loss:0.1164 val_acc:80.6295 val_loss:0.624
[07/0055] | train_loss:0.1149 val_acc:79.4189 val_loss:0.6172
[07/0056] | train_loss:0.1068 val_acc:80.3874 val_loss:0.6389
[07/0057] | train_loss:0.1046 val_acc:78.6925 val_loss:0.6272
[07/0058] | train_loss:0.107 val_acc:79.661 val_loss:0.6312
[07/0059] | train_loss:0.1012 val_acc:78.6925 val_loss:0.6551
[07/0060] | train_loss:0.0894 val_acc:80.6295 val_loss:0.6093
[07/0061] | train_loss:0.0944 val_acc:80.8717 val_loss:0.6243
[07/0062] | train_loss:0.0961 val_acc:80.3874 val_loss:0.6766
[07/0063] | train_loss:0.0861 val_acc:79.9031 val_loss:0.7622
[07/0064] | train_loss:0.0915 val_acc:80.3874 val_loss:0.637
[07/0065] | train_loss:0.104 val_acc:79.9031 val_loss:0.6315
[07/0066] | train_loss:0.0864 val_acc:79.9031 val_loss:0.739
[07/0067] | train_loss:0.1041 val_acc:79.4189 val_loss:0.6605
[07/0068] | train_loss:0.0897 val_acc:78.6925 val_loss:0.8026
[07/0069] | train_loss:0.0837 val_acc:78.6925 val_loss:0.7637
[07/0070] | train_loss:0.084 val_acc:79.661 val_loss:0.7088
[07/0071] | train_loss:0.081 val_acc:78.6925 val_loss:0.6911
[07/0072] | train_loss:0.0913 val_acc:80.3874 val_loss:0.6949
[07/0073] | train_loss:0.0774 val_acc:80.6295 val_loss:0.6998
[07/0074] | train_loss:0.089 val_acc:81.1138 val_loss:0.7343
[07/0075] | train_loss:0.0929 val_acc:79.1768 val_loss:0.6297
[07/0076] | train_loss:0.075 val_acc:80.6295 val_loss:0.7619
[07/0077] | train_loss:0.0635 val_acc:81.8402 val_loss:0.7638
[07/0078] | train_loss:0.0845 val_acc:80.1453 val_loss:0.7411
[07/0079] | train_loss:0.0644 val_acc:80.6295 val_loss:0.7985
[07/0080] | train_loss:0.0858 val_acc:80.3874 val_loss:0.7023
[07/0081] | train_loss:0.0665 val_acc:79.1768 val_loss:0.7464
Fold: [7/10] Test is finish !! 
 Test Metrics are: test_acc:82.3245 test_loss:0.4434fold [7/10] is start!!
[08/0001] | train_loss:0.6736 val_acc:50.6053 val_loss:0.6899
model is saved at epoch 1!![08/0002] | train_loss:0.6293 val_acc:50.6053 val_loss:0.698
[08/0003] | train_loss:0.601 val_acc:50.6053 val_loss:0.7521
[08/0004] | train_loss:0.5683 val_acc:50.8475 val_loss:0.7475
model is saved at epoch 4!![08/0005] | train_loss:0.5502 val_acc:53.753 val_loss:0.6915
model is saved at epoch 5!![08/0006] | train_loss:0.5263 val_acc:63.9225 val_loss:0.6155
model is saved at epoch 6!![08/0007] | train_loss:0.5247 val_acc:71.9128 val_loss:0.5322
model is saved at epoch 7!![08/0008] | train_loss:0.5131 val_acc:74.092 val_loss:0.5115
model is saved at epoch 8!![08/0009] | train_loss:0.4924 val_acc:74.3341 val_loss:0.515
model is saved at epoch 9!![08/0010] | train_loss:0.4825 val_acc:76.7554 val_loss:0.4947
model is saved at epoch 10!![08/0011] | train_loss:0.4666 val_acc:76.5133 val_loss:0.481
[08/0012] | train_loss:0.45 val_acc:76.0291 val_loss:0.483
[08/0013] | train_loss:0.4399 val_acc:78.6925 val_loss:0.4786
model is saved at epoch 13!![08/0014] | train_loss:0.4308 val_acc:77.4818 val_loss:0.4767
[08/0015] | train_loss:0.4245 val_acc:76.7554 val_loss:0.4818
[08/0016] | train_loss:0.4198 val_acc:78.9346 val_loss:0.4667
model is saved at epoch 16!![08/0017] | train_loss:0.403 val_acc:77.9661 val_loss:0.4747
[08/0018] | train_loss:0.3947 val_acc:78.4504 val_loss:0.4673
[08/0019] | train_loss:0.3854 val_acc:78.9346 val_loss:0.476
[08/0020] | train_loss:0.3724 val_acc:76.5133 val_loss:0.4768
[08/0021] | train_loss:0.3663 val_acc:78.2082 val_loss:0.494
[08/0022] | train_loss:0.3586 val_acc:76.2712 val_loss:0.4894
[08/0023] | train_loss:0.3474 val_acc:79.9031 val_loss:0.5123
model is saved at epoch 23!![08/0024] | train_loss:0.36 val_acc:78.9346 val_loss:0.4633
[08/0025] | train_loss:0.3283 val_acc:80.3874 val_loss:0.4834
model is saved at epoch 25!![08/0026] | train_loss:0.3222 val_acc:77.9661 val_loss:0.4811
[08/0027] | train_loss:0.3079 val_acc:76.9976 val_loss:0.4839
[08/0028] | train_loss:0.2985 val_acc:77.724 val_loss:0.5081
[08/0029] | train_loss:0.3061 val_acc:78.2082 val_loss:0.4794
[08/0030] | train_loss:0.2817 val_acc:77.9661 val_loss:0.5173
[08/0031] | train_loss:0.2792 val_acc:78.2082 val_loss:0.4696
[08/0032] | train_loss:0.2675 val_acc:78.6925 val_loss:0.5286
[08/0033] | train_loss:0.2623 val_acc:78.6925 val_loss:0.5035
[08/0034] | train_loss:0.2405 val_acc:78.9346 val_loss:0.4827
[08/0035] | train_loss:0.2363 val_acc:79.9031 val_loss:0.5036
[08/0036] | train_loss:0.2227 val_acc:78.9346 val_loss:0.5327
[08/0037] | train_loss:0.2211 val_acc:77.9661 val_loss:0.5211
[08/0038] | train_loss:0.2287 val_acc:78.9346 val_loss:0.5166
[08/0039] | train_loss:0.213 val_acc:79.9031 val_loss:0.5103
[08/0040] | train_loss:0.2132 val_acc:81.3559 val_loss:0.5231
model is saved at epoch 40!![08/0041] | train_loss:0.2193 val_acc:77.2397 val_loss:0.5222
[08/0042] | train_loss:0.2022 val_acc:77.9661 val_loss:0.5369
[08/0043] | train_loss:0.1998 val_acc:79.9031 val_loss:0.5155
[08/0044] | train_loss:0.1838 val_acc:79.661 val_loss:0.5371
[08/0045] | train_loss:0.1749 val_acc:80.3874 val_loss:0.5595
[08/0046] | train_loss:0.1729 val_acc:79.661 val_loss:0.5616
[08/0047] | train_loss:0.1913 val_acc:79.1768 val_loss:0.5207
[08/0048] | train_loss:0.1865 val_acc:77.9661 val_loss:0.6086
[08/0049] | train_loss:0.1673 val_acc:78.4504 val_loss:0.5415
[08/0050] | train_loss:0.1746 val_acc:80.3874 val_loss:0.5507
[08/0051] | train_loss:0.1626 val_acc:80.3874 val_loss:0.5553
[08/0052] | train_loss:0.1443 val_acc:80.1453 val_loss:0.5836
[08/0053] | train_loss:0.137 val_acc:79.9031 val_loss:0.5525
[08/0054] | train_loss:0.1392 val_acc:81.5981 val_loss:0.5671
model is saved at epoch 54!![08/0055] | train_loss:0.1502 val_acc:80.1453 val_loss:0.5647
[08/0056] | train_loss:0.1521 val_acc:79.1768 val_loss:0.5727
[08/0057] | train_loss:0.1384 val_acc:80.3874 val_loss:0.6196
[08/0058] | train_loss:0.1287 val_acc:80.3874 val_loss:0.6075
[08/0059] | train_loss:0.1149 val_acc:78.4504 val_loss:0.6418
[08/0060] | train_loss:0.1177 val_acc:78.4504 val_loss:0.6038
[08/0061] | train_loss:0.1201 val_acc:81.3559 val_loss:0.6398
[08/0062] | train_loss:0.1223 val_acc:81.3559 val_loss:0.6251
[08/0063] | train_loss:0.1328 val_acc:79.9031 val_loss:0.6323
[08/0064] | train_loss:0.12 val_acc:78.4504 val_loss:0.6479
[08/0065] | train_loss:0.1157 val_acc:79.9031 val_loss:0.6177
[08/0066] | train_loss:0.0975 val_acc:78.4504 val_loss:0.6747
[08/0067] | train_loss:0.0934 val_acc:79.1768 val_loss:0.6799
[08/0068] | train_loss:0.0935 val_acc:78.9346 val_loss:0.6279
[08/0069] | train_loss:0.0901 val_acc:79.4189 val_loss:0.7306
[08/0070] | train_loss:0.1101 val_acc:80.1453 val_loss:0.7592
[08/0071] | train_loss:0.1139 val_acc:79.661 val_loss:0.6319
[08/0072] | train_loss:0.1005 val_acc:78.2082 val_loss:0.6838
[08/0073] | train_loss:0.117 val_acc:79.661 val_loss:0.6897
[08/0074] | train_loss:0.1083 val_acc:80.1453 val_loss:0.6588
[08/0075] | train_loss:0.1176 val_acc:78.4504 val_loss:0.704
[08/0076] | train_loss:0.1004 val_acc:77.9661 val_loss:0.6975
[08/0077] | train_loss:0.0829 val_acc:80.6295 val_loss:0.6855
[08/0078] | train_loss:0.0872 val_acc:79.9031 val_loss:0.6859
[08/0079] | train_loss:0.0769 val_acc:79.4189 val_loss:0.7741
[08/0080] | train_loss:0.0868 val_acc:79.1768 val_loss:0.7221
[08/0081] | train_loss:0.1034 val_acc:79.4189 val_loss:0.7312
[08/0082] | train_loss:0.1105 val_acc:79.661 val_loss:0.6536
[08/0083] | train_loss:0.1101 val_acc:80.8717 val_loss:0.6502
[08/0084] | train_loss:0.0883 val_acc:79.1768 val_loss:0.704
[08/0085] | train_loss:0.0764 val_acc:77.9661 val_loss:0.7338
[08/0086] | train_loss:0.0789 val_acc:79.661 val_loss:0.7255
[08/0087] | train_loss:0.0891 val_acc:78.9346 val_loss:0.703
[08/0088] | train_loss:0.0983 val_acc:78.9346 val_loss:0.6832
[08/0089] | train_loss:0.0791 val_acc:81.1138 val_loss:0.7089
[08/0090] | train_loss:0.0833 val_acc:80.8717 val_loss:0.7102
[08/0091] | train_loss:0.0769 val_acc:79.9031 val_loss:0.7391
[08/0092] | train_loss:0.0694 val_acc:78.6925 val_loss:0.7483
[08/0093] | train_loss:0.0633 val_acc:79.4189 val_loss:0.7518
[08/0094] | train_loss:0.0775 val_acc:80.6295 val_loss:0.712
[08/0095] | train_loss:0.065 val_acc:79.4189 val_loss:0.7691
[08/0096] | train_loss:0.0544 val_acc:77.4818 val_loss:0.8142
[08/0097] | train_loss:0.0578 val_acc:76.2712 val_loss:0.8073
[08/0098] | train_loss:0.0767 val_acc:79.9031 val_loss:0.7838
[08/0099] | train_loss:0.0717 val_acc:79.9031 val_loss:0.7654
[08/0100] | train_loss:0.0709 val_acc:79.9031 val_loss:0.7675
[08/0101] | train_loss:0.0573 val_acc:79.9031 val_loss:0.7781
[08/0102] | train_loss:0.0626 val_acc:78.2082 val_loss:0.8497
[08/0103] | train_loss:0.0583 val_acc:78.2082 val_loss:0.8665
[08/0104] | train_loss:0.0517 val_acc:79.4189 val_loss:0.8288
[08/0105] | train_loss:0.044 val_acc:80.6295 val_loss:0.8355
Fold: [8/10] Test is finish !! 
 Test Metrics are: test_acc:74.2718 test_loss:0.7779fold [8/10] is start!!
[09/0001] | train_loss:0.667 val_acc:54.3689 val_loss:0.687
model is saved at epoch 1!![09/0002] | train_loss:0.6261 val_acc:54.3689 val_loss:0.677
[09/0003] | train_loss:0.5965 val_acc:54.3689 val_loss:0.6866
[09/0004] | train_loss:0.5687 val_acc:55.3398 val_loss:0.6667
model is saved at epoch 4!![09/0005] | train_loss:0.5448 val_acc:60.1942 val_loss:0.6389
model is saved at epoch 5!![09/0006] | train_loss:0.5174 val_acc:66.9903 val_loss:0.5897
model is saved at epoch 6!![09/0007] | train_loss:0.5072 val_acc:67.7184 val_loss:0.5929
model is saved at epoch 7!![09/0008] | train_loss:0.4831 val_acc:71.6019 val_loss:0.5445
model is saved at epoch 8!![09/0009] | train_loss:0.4823 val_acc:72.5728 val_loss:0.5405
model is saved at epoch 9!![09/0010] | train_loss:0.4715 val_acc:73.0583 val_loss:0.5705
model is saved at epoch 10!![09/0011] | train_loss:0.449 val_acc:72.0874 val_loss:0.5759
[09/0012] | train_loss:0.4491 val_acc:75.4854 val_loss:0.517
model is saved at epoch 12!![09/0013] | train_loss:0.4264 val_acc:73.7864 val_loss:0.5823
[09/0014] | train_loss:0.4131 val_acc:74.0291 val_loss:0.5562
[09/0015] | train_loss:0.4027 val_acc:76.2136 val_loss:0.5352
model is saved at epoch 15!![09/0016] | train_loss:0.3894 val_acc:76.699 val_loss:0.5412
model is saved at epoch 16!![09/0017] | train_loss:0.3797 val_acc:74.5146 val_loss:0.6188
[09/0018] | train_loss:0.3695 val_acc:75.0 val_loss:0.5751
[09/0019] | train_loss:0.3603 val_acc:77.6699 val_loss:0.5599
model is saved at epoch 19!![09/0020] | train_loss:0.3627 val_acc:72.8155 val_loss:0.542
[09/0021] | train_loss:0.3513 val_acc:76.9417 val_loss:0.5722
[09/0022] | train_loss:0.3276 val_acc:75.4854 val_loss:0.5446
[09/0023] | train_loss:0.3345 val_acc:76.699 val_loss:0.5619
[09/0024] | train_loss:0.3219 val_acc:75.7282 val_loss:0.59
[09/0025] | train_loss:0.305 val_acc:75.4854 val_loss:0.5779
[09/0026] | train_loss:0.2965 val_acc:77.4272 val_loss:0.5481
[09/0027] | train_loss:0.2825 val_acc:74.0291 val_loss:0.5694
[09/0028] | train_loss:0.2733 val_acc:76.699 val_loss:0.6336
[09/0029] | train_loss:0.296 val_acc:75.7282 val_loss:0.5564
[09/0030] | train_loss:0.2808 val_acc:74.2718 val_loss:0.689
[09/0031] | train_loss:0.2469 val_acc:75.4854 val_loss:0.6168
[09/0032] | train_loss:0.2338 val_acc:75.0 val_loss:0.6328
[09/0033] | train_loss:0.2294 val_acc:76.699 val_loss:0.645
[09/0034] | train_loss:0.2243 val_acc:76.2136 val_loss:0.6547
[09/0035] | train_loss:0.2217 val_acc:77.1845 val_loss:0.6594
[09/0036] | train_loss:0.2123 val_acc:75.9709 val_loss:0.656
[09/0037] | train_loss:0.1958 val_acc:77.6699 val_loss:0.6706
[09/0038] | train_loss:0.2021 val_acc:75.9709 val_loss:0.622
[09/0039] | train_loss:0.1987 val_acc:76.4563 val_loss:0.682
[09/0040] | train_loss:0.1893 val_acc:74.7573 val_loss:0.7046
[09/0041] | train_loss:0.1793 val_acc:75.9709 val_loss:0.7309
[09/0042] | train_loss:0.17 val_acc:75.9709 val_loss:0.6488
[09/0043] | train_loss:0.1638 val_acc:74.5146 val_loss:0.7564
[09/0044] | train_loss:0.158 val_acc:75.9709 val_loss:0.7646
[09/0045] | train_loss:0.1535 val_acc:75.2427 val_loss:0.7458
[09/0046] | train_loss:0.1439 val_acc:76.9417 val_loss:0.7633
[09/0047] | train_loss:0.1607 val_acc:77.1845 val_loss:0.7627
[09/0048] | train_loss:0.1423 val_acc:77.9126 val_loss:0.7962
model is saved at epoch 48!![09/0049] | train_loss:0.1488 val_acc:77.9126 val_loss:0.7203
[09/0050] | train_loss:0.1246 val_acc:76.9417 val_loss:0.9215
[09/0051] | train_loss:0.1389 val_acc:75.0 val_loss:0.8797
[09/0052] | train_loss:0.1369 val_acc:75.2427 val_loss:0.8159
[09/0053] | train_loss:0.1146 val_acc:75.9709 val_loss:0.9288
[09/0054] | train_loss:0.1214 val_acc:73.7864 val_loss:0.8541
[09/0055] | train_loss:0.1296 val_acc:75.0 val_loss:0.9313
[09/0056] | train_loss:0.1146 val_acc:76.9417 val_loss:0.8367
[09/0057] | train_loss:0.1159 val_acc:75.4854 val_loss:0.9253
[09/0058] | train_loss:0.115 val_acc:75.0 val_loss:0.8629
[09/0059] | train_loss:0.1159 val_acc:74.2718 val_loss:0.9332
[09/0060] | train_loss:0.1065 val_acc:75.9709 val_loss:0.8463
[09/0061] | train_loss:0.1122 val_acc:74.2718 val_loss:0.9691
[09/0062] | train_loss:0.0921 val_acc:75.0 val_loss:0.9956
[09/0063] | train_loss:0.1006 val_acc:76.699 val_loss:0.8874
[09/0064] | train_loss:0.1039 val_acc:74.5146 val_loss:0.9459
[09/0065] | train_loss:0.0846 val_acc:75.9709 val_loss:1.0298
[09/0066] | train_loss:0.081 val_acc:75.9709 val_loss:0.9451
[09/0067] | train_loss:0.0911 val_acc:74.5146 val_loss:1.0501
[09/0068] | train_loss:0.1004 val_acc:76.699 val_loss:0.9111
[09/0069] | train_loss:0.1055 val_acc:75.7282 val_loss:1.034
[09/0070] | train_loss:0.1191 val_acc:75.7282 val_loss:0.9552
[09/0071] | train_loss:0.1002 val_acc:76.2136 val_loss:0.9408
[09/0072] | train_loss:0.1038 val_acc:76.699 val_loss:0.9721
[09/0073] | train_loss:0.0875 val_acc:75.0 val_loss:1.0517
[09/0074] | train_loss:0.0729 val_acc:76.4563 val_loss:0.9752
[09/0075] | train_loss:0.0895 val_acc:75.9709 val_loss:1.0062
[09/0076] | train_loss:0.0977 val_acc:76.699 val_loss:0.9855
[09/0077] | train_loss:0.088 val_acc:76.2136 val_loss:0.9615
[09/0078] | train_loss:0.082 val_acc:74.5146 val_loss:1.0653
[09/0079] | train_loss:0.0862 val_acc:74.7573 val_loss:1.0964
[09/0080] | train_loss:0.0727 val_acc:74.7573 val_loss:0.971
[09/0081] | train_loss:0.0727 val_acc:75.2427 val_loss:1.1479
[09/0082] | train_loss:0.0873 val_acc:76.9417 val_loss:0.9762
[09/0083] | train_loss:0.0752 val_acc:74.0291 val_loss:1.1155
[09/0084] | train_loss:0.0708 val_acc:76.9417 val_loss:1.067
[09/0085] | train_loss:0.0614 val_acc:74.5146 val_loss:1.2137
[09/0086] | train_loss:0.0767 val_acc:75.0 val_loss:1.0276
[09/0087] | train_loss:0.0666 val_acc:76.4563 val_loss:1.0814
[09/0088] | train_loss:0.0597 val_acc:76.2136 val_loss:1.1201
[09/0089] | train_loss:0.054 val_acc:77.1845 val_loss:1.1246
[09/0090] | train_loss:0.0577 val_acc:74.7573 val_loss:1.1725
[09/0091] | train_loss:0.0576 val_acc:75.0 val_loss:1.2234
[09/0092] | train_loss:0.0754 val_acc:78.3981 val_loss:1.1305
model is saved at epoch 92!![09/0093] | train_loss:0.0585 val_acc:77.6699 val_loss:1.1081
[09/0094] | train_loss:0.0607 val_acc:76.2136 val_loss:1.2027
[09/0095] | train_loss:0.0591 val_acc:75.4854 val_loss:1.228
[09/0096] | train_loss:0.0551 val_acc:76.9417 val_loss:1.1744
[09/0097] | train_loss:0.0584 val_acc:75.0 val_loss:1.1333
[09/0098] | train_loss:0.0569 val_acc:76.2136 val_loss:1.2377
[09/0099] | train_loss:0.074 val_acc:77.9126 val_loss:1.0158
[09/0100] | train_loss:0.0597 val_acc:75.0 val_loss:1.0967
[09/0101] | train_loss:0.0541 val_acc:76.9417 val_loss:1.1399
[09/0102] | train_loss:0.061 val_acc:74.0291 val_loss:1.1583
[09/0103] | train_loss:0.0638 val_acc:77.1845 val_loss:1.1672
[09/0104] | train_loss:0.0531 val_acc:77.6699 val_loss:1.1579
[09/0105] | train_loss:0.0672 val_acc:77.1845 val_loss:1.1098
[09/0106] | train_loss:0.0614 val_acc:75.7282 val_loss:1.1919
[09/0107] | train_loss:0.0643 val_acc:76.2136 val_loss:1.1435
[09/0108] | train_loss:0.0617 val_acc:76.9417 val_loss:1.1686
[09/0109] | train_loss:0.066 val_acc:77.4272 val_loss:1.1473
[09/0110] | train_loss:0.0564 val_acc:75.9709 val_loss:1.1941
[09/0111] | train_loss:0.0805 val_acc:75.9709 val_loss:1.1363
[09/0112] | train_loss:0.0655 val_acc:75.4854 val_loss:1.0872
[09/0113] | train_loss:0.0652 val_acc:74.7573 val_loss:1.3052
[09/0114] | train_loss:0.0768 val_acc:75.2427 val_loss:1.133
[09/0115] | train_loss:0.0604 val_acc:75.7282 val_loss:1.2634
[09/0116] | train_loss:0.0643 val_acc:76.2136 val_loss:1.1669
[09/0117] | train_loss:0.0615 val_acc:76.699 val_loss:1.0497
[09/0118] | train_loss:0.051 val_acc:75.4854 val_loss:1.2792
[09/0119] | train_loss:0.0448 val_acc:75.2427 val_loss:1.2272
[09/0120] | train_loss:0.0634 val_acc:75.0 val_loss:1.2253
[09/0121] | train_loss:0.054 val_acc:74.0291 val_loss:1.4297
[09/0122] | train_loss:0.0643 val_acc:74.7573 val_loss:1.2607
[09/0123] | train_loss:0.0617 val_acc:76.9417 val_loss:1.1396
[09/0124] | train_loss:0.0408 val_acc:77.6699 val_loss:1.2499
[09/0125] | train_loss:0.0333 val_acc:74.0291 val_loss:1.2942
[09/0126] | train_loss:0.0442 val_acc:75.4854 val_loss:1.2626
[09/0127] | train_loss:0.0451 val_acc:74.2718 val_loss:1.3238
[09/0128] | train_loss:0.0408 val_acc:74.5146 val_loss:1.3225
[09/0129] | train_loss:0.0386 val_acc:76.699 val_loss:1.3337
[09/0130] | train_loss:0.0437 val_acc:76.4563 val_loss:1.3824
[09/0131] | train_loss:0.0497 val_acc:75.2427 val_loss:1.3788
[09/0132] | train_loss:0.0439 val_acc:77.4272 val_loss:1.31
[09/0133] | train_loss:0.0406 val_acc:77.6699 val_loss:1.2668
[09/0134] | train_loss:0.0397 val_acc:75.7282 val_loss:1.5045
[09/0135] | train_loss:0.0395 val_acc:77.4272 val_loss:1.2881
[09/0136] | train_loss:0.0377 val_acc:77.4272 val_loss:1.3971
[09/0137] | train_loss:0.0395 val_acc:74.7573 val_loss:1.3257
[09/0138] | train_loss:0.0416 val_acc:75.2427 val_loss:1.4164
[09/0139] | train_loss:0.0388 val_acc:75.4854 val_loss:1.3847
[09/0140] | train_loss:0.0421 val_acc:75.7282 val_loss:1.3825
[09/0141] | train_loss:0.0461 val_acc:77.1845 val_loss:1.3408
[09/0142] | train_loss:0.0367 val_acc:73.7864 val_loss:1.368
[09/0143] | train_loss:0.0386 val_acc:79.3689 val_loss:1.3368
model is saved at epoch 143!![09/0144] | train_loss:0.0515 val_acc:76.4563 val_loss:1.248
[09/0145] | train_loss:0.0505 val_acc:74.5146 val_loss:1.4572
[09/0146] | train_loss:0.0403 val_acc:76.4563 val_loss:1.3279
[09/0147] | train_loss:0.0496 val_acc:76.9417 val_loss:1.3696
[09/0148] | train_loss:0.0415 val_acc:76.9417 val_loss:1.4214
[09/0149] | train_loss:0.042 val_acc:76.2136 val_loss:1.3289
[09/0150] | train_loss:0.0451 val_acc:75.2427 val_loss:1.3426
[09/0151] | train_loss:0.0553 val_acc:75.2427 val_loss:1.2439
[09/0152] | train_loss:0.0338 val_acc:75.2427 val_loss:1.4384
[09/0153] | train_loss:0.0323 val_acc:75.9709 val_loss:1.4463
[09/0154] | train_loss:0.0348 val_acc:75.9709 val_loss:1.323
[09/0155] | train_loss:0.0385 val_acc:77.4272 val_loss:1.3102
[09/0156] | train_loss:0.0315 val_acc:76.2136 val_loss:1.4821
[09/0157] | train_loss:0.0271 val_acc:75.0 val_loss:1.3976
[09/0158] | train_loss:0.0384 val_acc:75.9709 val_loss:1.4121
[09/0159] | train_loss:0.0393 val_acc:74.2718 val_loss:1.57
[09/0160] | train_loss:0.0667 val_acc:76.2136 val_loss:1.2178
[09/0161] | train_loss:0.0538 val_acc:77.1845 val_loss:1.2937
[09/0162] | train_loss:0.0406 val_acc:75.7282 val_loss:1.5358
[09/0163] | train_loss:0.055 val_acc:75.2427 val_loss:1.3077
[09/0164] | train_loss:0.0351 val_acc:76.2136 val_loss:1.4831
[09/0165] | train_loss:0.0369 val_acc:76.2136 val_loss:1.5575
[09/0166] | train_loss:0.047 val_acc:76.9417 val_loss:1.3437
[09/0167] | train_loss:0.0366 val_acc:77.4272 val_loss:1.3018
[09/0168] | train_loss:0.0536 val_acc:76.9417 val_loss:1.3248
[09/0169] | train_loss:0.0422 val_acc:75.4854 val_loss:1.2662
[09/0170] | train_loss:0.0334 val_acc:77.9126 val_loss:1.4351
[09/0171] | train_loss:0.0419 val_acc:75.2427 val_loss:1.4606
[09/0172] | train_loss:0.0446 val_acc:76.4563 val_loss:1.4351
[09/0173] | train_loss:0.0339 val_acc:77.6699 val_loss:1.3923
[09/0174] | train_loss:0.03 val_acc:76.4563 val_loss:1.7378
[09/0175] | train_loss:0.0404 val_acc:77.1845 val_loss:1.4828
[09/0176] | train_loss:0.0301 val_acc:77.4272 val_loss:1.4736
[09/0177] | train_loss:0.0279 val_acc:76.4563 val_loss:1.533
[09/0178] | train_loss:0.0292 val_acc:76.4563 val_loss:1.4086
[09/0179] | train_loss:0.0292 val_acc:77.1845 val_loss:1.5399
[09/0180] | train_loss:0.0506 val_acc:76.2136 val_loss:1.3268
[09/0181] | train_loss:0.0463 val_acc:76.9417 val_loss:1.3991
[09/0182] | train_loss:0.0315 val_acc:75.7282 val_loss:1.558
[09/0183] | train_loss:0.0341 val_acc:75.2427 val_loss:1.5104
[09/0184] | train_loss:0.0351 val_acc:76.699 val_loss:1.456
[09/0185] | train_loss:0.0273 val_acc:74.2718 val_loss:1.4532
[09/0186] | train_loss:0.0258 val_acc:75.4854 val_loss:1.5113
[09/0187] | train_loss:0.0257 val_acc:76.2136 val_loss:1.6249
[09/0188] | train_loss:0.0407 val_acc:75.4854 val_loss:1.5619
[09/0189] | train_loss:0.0537 val_acc:75.4854 val_loss:1.4221
[09/0190] | train_loss:0.046 val_acc:75.9709 val_loss:1.5308
[09/0191] | train_loss:0.039 val_acc:75.4854 val_loss:1.6602
[09/0192] | train_loss:0.0383 val_acc:76.2136 val_loss:1.5029
[09/0193] | train_loss:0.0384 val_acc:75.4854 val_loss:1.4931
[09/0194] | train_loss:0.0283 val_acc:75.2427 val_loss:1.6184
Fold: [9/10] Test is finish !! 
 Test Metrics are: test_acc:81.068 test_loss:1.1591fold [9/10] is start!!
[10/0001] | train_loss:0.6813 val_acc:50.7282 val_loss:0.7022
model is saved at epoch 1!![10/0002] | train_loss:0.6492 val_acc:48.7864 val_loss:0.6873
[10/0003] | train_loss:0.6172 val_acc:49.2718 val_loss:0.6873
[10/0004] | train_loss:0.5844 val_acc:49.7573 val_loss:0.7182
[10/0005] | train_loss:0.5717 val_acc:54.3689 val_loss:0.6769
model is saved at epoch 5!![10/0006] | train_loss:0.5559 val_acc:61.4078 val_loss:0.6238
model is saved at epoch 6!![10/0007] | train_loss:0.5375 val_acc:72.0874 val_loss:0.5278
model is saved at epoch 7!![10/0008] | train_loss:0.5211 val_acc:75.0 val_loss:0.5158
model is saved at epoch 8!![10/0009] | train_loss:0.5057 val_acc:74.2718 val_loss:0.5158
[10/0010] | train_loss:0.4955 val_acc:77.6699 val_loss:0.4952
model is saved at epoch 10!![10/0011] | train_loss:0.4819 val_acc:78.8835 val_loss:0.4979
model is saved at epoch 11!![10/0012] | train_loss:0.4647 val_acc:74.5146 val_loss:0.5379
[10/0013] | train_loss:0.4644 val_acc:75.9709 val_loss:0.4867
[10/0014] | train_loss:0.4442 val_acc:77.1845 val_loss:0.5088
[10/0015] | train_loss:0.4353 val_acc:77.9126 val_loss:0.4717
[10/0016] | train_loss:0.4184 val_acc:78.6408 val_loss:0.4768
[10/0017] | train_loss:0.4183 val_acc:78.1553 val_loss:0.4666
[10/0018] | train_loss:0.4084 val_acc:79.3689 val_loss:0.5028
model is saved at epoch 18!![10/0019] | train_loss:0.3897 val_acc:75.9709 val_loss:0.4932
[10/0020] | train_loss:0.3987 val_acc:77.9126 val_loss:0.4845
[10/0021] | train_loss:0.3793 val_acc:77.9126 val_loss:0.4917
[10/0022] | train_loss:0.3665 val_acc:79.3689 val_loss:0.4889
[10/0023] | train_loss:0.3585 val_acc:78.1553 val_loss:0.478
[10/0024] | train_loss:0.3483 val_acc:79.3689 val_loss:0.4785
[10/0025] | train_loss:0.3408 val_acc:75.2427 val_loss:0.5015
[10/0026] | train_loss:0.3284 val_acc:78.6408 val_loss:0.5102
[10/0027] | train_loss:0.318 val_acc:75.2427 val_loss:0.5097
[10/0028] | train_loss:0.3088 val_acc:78.8835 val_loss:0.5085
[10/0029] | train_loss:0.2895 val_acc:79.1262 val_loss:0.555
[10/0030] | train_loss:0.2954 val_acc:78.3981 val_loss:0.481
[10/0031] | train_loss:0.2807 val_acc:78.3981 val_loss:0.5548
[10/0032] | train_loss:0.2878 val_acc:79.8544 val_loss:0.4882
model is saved at epoch 32!![10/0033] | train_loss:0.2658 val_acc:79.6117 val_loss:0.5107
[10/0034] | train_loss:0.2575 val_acc:79.1262 val_loss:0.5092
[10/0035] | train_loss:0.2409 val_acc:79.3689 val_loss:0.5298
[10/0036] | train_loss:0.2466 val_acc:76.9417 val_loss:0.5127
[10/0037] | train_loss:0.2419 val_acc:79.8544 val_loss:0.5391
[10/0038] | train_loss:0.2312 val_acc:79.8544 val_loss:0.5199
[10/0039] | train_loss:0.223 val_acc:78.8835 val_loss:0.5401
[10/0040] | train_loss:0.2191 val_acc:77.6699 val_loss:0.5699
[10/0041] | train_loss:0.206 val_acc:76.699 val_loss:0.5438
[10/0042] | train_loss:0.1848 val_acc:79.1262 val_loss:0.5502
[10/0043] | train_loss:0.1924 val_acc:79.1262 val_loss:0.5485
[10/0044] | train_loss:0.191 val_acc:79.3689 val_loss:0.5782
[10/0045] | train_loss:0.181 val_acc:77.6699 val_loss:0.593
[10/0046] | train_loss:0.181 val_acc:79.3689 val_loss:0.5565
[10/0047] | train_loss:0.1788 val_acc:80.3398 val_loss:0.583
model is saved at epoch 47!![10/0048] | train_loss:0.1671 val_acc:80.3398 val_loss:0.6028
[10/0049] | train_loss:0.1603 val_acc:76.9417 val_loss:0.6096
[10/0050] | train_loss:0.159 val_acc:77.6699 val_loss:0.5748
[10/0051] | train_loss:0.1629 val_acc:78.1553 val_loss:0.6373
[10/0052] | train_loss:0.1657 val_acc:78.6408 val_loss:0.5816
[10/0053] | train_loss:0.1489 val_acc:78.3981 val_loss:0.6386
[10/0054] | train_loss:0.1368 val_acc:80.3398 val_loss:0.7241
[10/0055] | train_loss:0.1658 val_acc:79.6117 val_loss:0.612
[10/0056] | train_loss:0.1545 val_acc:78.3981 val_loss:0.5948
[10/0057] | train_loss:0.1376 val_acc:79.1262 val_loss:0.6292
[10/0058] | train_loss:0.1175 val_acc:79.1262 val_loss:0.6822
[10/0059] | train_loss:0.1281 val_acc:77.9126 val_loss:0.6735
[10/0060] | train_loss:0.1191 val_acc:78.8835 val_loss:0.6979
[10/0061] | train_loss:0.101 val_acc:79.3689 val_loss:0.7175
[10/0062] | train_loss:0.1016 val_acc:79.3689 val_loss:0.6828
[10/0063] | train_loss:0.111 val_acc:78.3981 val_loss:0.6824
[10/0064] | train_loss:0.1097 val_acc:76.699 val_loss:0.7733
[10/0065] | train_loss:0.1083 val_acc:79.1262 val_loss:0.7083
[10/0066] | train_loss:0.1172 val_acc:78.3981 val_loss:0.6848
[10/0067] | train_loss:0.1226 val_acc:77.4272 val_loss:0.7511
[10/0068] | train_loss:0.1022 val_acc:79.6117 val_loss:0.7067
[10/0069] | train_loss:0.1035 val_acc:77.9126 val_loss:0.7485
[10/0070] | train_loss:0.1042 val_acc:79.3689 val_loss:0.7472
[10/0071] | train_loss:0.1047 val_acc:78.8835 val_loss:0.7782
[10/0072] | train_loss:0.1055 val_acc:78.3981 val_loss:0.6987
[10/0073] | train_loss:0.0959 val_acc:77.9126 val_loss:0.7456
[10/0074] | train_loss:0.091 val_acc:78.1553 val_loss:0.771
[10/0075] | train_loss:0.0973 val_acc:78.8835 val_loss:0.7761
[10/0076] | train_loss:0.1021 val_acc:78.6408 val_loss:0.8009
[10/0077] | train_loss:0.1226 val_acc:77.1845 val_loss:0.7235
[10/0078] | train_loss:0.1224 val_acc:78.1553 val_loss:0.8976
[10/0079] | train_loss:0.1071 val_acc:78.1553 val_loss:0.7727
[10/0080] | train_loss:0.0806 val_acc:77.4272 val_loss:0.7644
[10/0081] | train_loss:0.0761 val_acc:77.4272 val_loss:0.8389
[10/0082] | train_loss:0.073 val_acc:77.4272 val_loss:0.8284
[10/0083] | train_loss:0.083 val_acc:78.1553 val_loss:0.7987
[10/0084] | train_loss:0.0757 val_acc:78.3981 val_loss:0.855
[10/0085] | train_loss:0.0628 val_acc:78.6408 val_loss:0.8782
[10/0086] | train_loss:0.0617 val_acc:78.3981 val_loss:0.8955
[10/0087] | train_loss:0.0723 val_acc:78.1553 val_loss:0.8811
[10/0088] | train_loss:0.0653 val_acc:78.6408 val_loss:0.8499
[10/0089] | train_loss:0.065 val_acc:79.8544 val_loss:0.8711
[10/0090] | train_loss:0.064 val_acc:77.1845 val_loss:0.8863
[10/0091] | train_loss:0.0603 val_acc:78.8835 val_loss:0.894
[10/0092] | train_loss:0.0769 val_acc:78.8835 val_loss:0.9351
[10/0093] | train_loss:0.0699 val_acc:79.3689 val_loss:0.8423
[10/0094] | train_loss:0.0661 val_acc:78.1553 val_loss:0.9112
[10/0095] | train_loss:0.0607 val_acc:76.9417 val_loss:1.0208
[10/0096] | train_loss:0.0707 val_acc:78.3981 val_loss:0.8651
[10/0097] | train_loss:0.0564 val_acc:79.1262 val_loss:0.9444
[10/0098] | train_loss:0.051 val_acc:77.4272 val_loss:0.9294
Fold: [10/10] Test is finish !! 
 Test Metrics are: test_acc:79.6117 test_loss:0.6154
all fold acc is: 
[73.84988069534302, 79.6610176563263, 77.23971009254456, 77.48184204101562, 82.5665831565857, 81.11380338668823, 82.3244571685791, 74.27184581756592, 81.06796145439148, 79.61165308952332] 
Test is finish !! 
 Test Metrics are: acc_mean:78.9189 acc_std:2.9588